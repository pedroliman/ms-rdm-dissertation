The recent interest in big data has led many companies to develop big data analytics capability (BDAC) in order to enhance firm performance (FPER). However, BDAC pays off for some companies but not for others. It appears that very few have achieved a big impact through big data. To address this challenge, this study proposes a BDAC model drawing on the resource-based theory (RBT) and the entanglement view of sociomaterialism. The findings show BDAC as a hierarchical model, which consists of three primary dimensions (i.e., management, technology, and talent capability) and 11 subdimensions (i.e., planning, investment, coordination, control, connectivity, compatibility, modularity, technology management knowledge, technical knowledge, business knowledge and relational knowledge). The findings from two Delphi studies and 152 online surveys of business analysts in the U.S. confirm the value of the entanglement conceptualization of the higher-order BDAC model and its impact on FPER. The results also illuminate the significant moderating impact of analytics capability–business strategy alignment on the BDAC–FPER relationship. © 2016
Grey literature can be a valuable source of information about organizations’ activities and, to a certain extent, about their identity. Some of the major problems that hinder their full exploitation are the heterogeneity of formats, the lack of structure, the unpredictability of their content, the size of the document bases, which can quickly become huge. The collection and mining of grey literature can be applied to individual organizations or classes of organizations, thus enabling the analysis of the trends in particular fields. To this end, some techniques can be inherited from the best practices for the management of structured documents belonging to well identified categories, but something more is needed in our case. Obvious steps are: identifying sources, collecting items, cleansing and de-duplicating contents, assigning unique and persistent identifiers, adding metadata and augmenting the information using other sources. These phases are common to all digital libraries but further steps are required, in our opinion, in the case of grey literature in order to build document bases of value. In particular, we think that an iterative approach would be the most suitable in this context, one including an assessment of what has been collected in order to identify possible gaps and start over with the collection phase. We think that big data technologies, together with information retrieval and data and text mining techniques, will play a key role in this sector. This “bag of tools” will certainly facilitate the management, browsing and exploitation of large document bases that belong not only to a single organisation but also, for example, to a large number of organizations working in a particular sector. This on the one hand opens new scenarios regarding the type of information that can be extracted, but on the other hand introduces new problems regarding the homogenization of contents, formats and metadata, and additional issues related to quality control and confidentiality protection. We believe that in this context the incremental-iterative approach would help address, gradually and based on real cases, the problems mentioned above. In this paper we describe the process that, in our opinion, should be put in place and a high level ICT architecture for its dematerialization, along with the technologies that could be leveraged for its implementation. © 2016, GreyNet. All Rights reserved.
The era of Big Data has arrived along with large volume, complex and growing data generated by many distinct sources. Nowadays, nearly every aspect of the modern society is impacted by Big Data, involving medical, health care, business, management and government. It has been receiving growing attention of researches from many disciplines including natural sciences, life sciences, engineering and even art & humanities. It also leads to new research paradigms and ways of thinking on the path of development. Lots of developed and under-developing tools improve our ability to make more felicitous decisions than what we have made ever before. This paper presents an overview on Big Data including four issues, namely: (i) concepts, characteristics and processing paradigms of Big Data
Data-driven bottleneck identification has received an increasing interest during the recent years. This approach locates the throughput bottleneck of manufacturing systems based on indicators derived from measured machine performance metrics. However, the variability in manufacturing systems may affect the quality of bottleneck indicators, leading to possible inaccurate detection results. This paper presents a statistical framework (SF) to decrease the data-driven detection inaccuracy caused by system variability. Using several statistical tools as building blocks, the proposed SF is able to analyse the logical conditions under which a machine is detected as the bottleneck, and rejects the proposal of bottleneck when no sufficient statistical evidence is collected. A full factorial design experiment is used to study the parameter effects of the SF, and to calibrate the SF. The proposed SF was numerically verified to be effective in decreasing the wrong bottleneck detection rate in serial production lines. © 2016 Informa UK Limited, trading as Taylor & Francis Group.
Three Gameplay Action-Decision (GAD) profiles: Explorer, Fulfiller, and Quitter, have been identified based on individual's decision-making actions and navigational behaviors in situ serious games. The ability to profile trainees using serious games can yield new analytics and insights towards training and learning performance improvement, including the identification of weaknesses or potential training needs in the players towards adaptive training, and the creation of new diagnostics for prescriptive training, retraining, and remediation. Similarity measures of players' in-game course of actions (COAs) have been shown to be a viable approach in differentiating novices from experts in serious games. In this study, we examined and compared several popular similarity measures to see if any measure, or combination of measures, would be viable in differentiating players based on their GAD profiles in serious games. Our findings revealed that similarity measures, while significant in their predicting abilities individually, could gain more strength from one another in combination. More research is needed to create or develop new metrics and methods for players’ action and behavioral profiling in Serious Games Analytics. © 2016
Aiming at the restoration of degraded social network services media, this paper proposed a novel multi-prior models collaboration framework for image restoration with big data. Different from the traditional non-reference media restoration strategies, a big reference image set is adopted to provide the references and predictions of different popular prior models and accordingly guide the further prior collaboration. With these cues, the collaboration of multi-prior models is mathematically formulated as a ridge regression problem in this paper. Due to the computation complexity of dealing big reference data, scatter-matrix-based KRR is proposed which can achieve high accuracy and low complexity in big data related decision making task. Specifically, an iterative pursuit is proposed to obtain further refined and robust estimation. Five popular prior methods are applied to evaluate the effectiveness of the proposed multi-prior models collaboration. Compared with the traditional restoration strategies, the proposed framework improves the restoration performance significantly and provides a reasonable method for the relative exploration of big data driven decision making. © 2014, Springer Science+Business Media New York.
Both sustainable manufacturing and manufacturing service are the trends in industry because they are regarded as ways to reduce the resource cost and energy consumption in manufacturing process, to improve the flexibility and responding speed to customers’ demand, and to improve the production efficiency. In order to improve the sustainability of manufacturing equipment services in job shop, this paper presents a multi-objective joint model of energy consumption and production efficiency. The model is related to multi-conditions of manufacturing equipment services. The conditions are monitored in real-time to drive a multi-objective dynamic optimized scheduling of manufacturing services. In order to solve the multi-objective problem, an enhanced Pareto-based bees algorithm (EPBA) is proposed. In order to ensure the variety of population, to prevent the premature convergence, and to improve the searching speed, several key technologies are utilized such as variable neighborhood searching, mutation and crossover operation, fast non-dominated ranking, critical path local search, archive Pareto set, critical path taboo set, etc. Finally, the proposed method is evaluated and shows better performance in static and dynamic scenarios compared with the existing optimization algorithms. © 2016 The Society of Manufacturing Engineers
Following a multidisciplinary perspective (that combines the literature from management, information systems, marketing and engineering telecommunications perspectives), the purpose of this article is to create and analyze a conceptual framework and to propose a new methodology that encompasses different techniques for pervasive information gathering in hotels and for identifying clients’ habits. Focusing on the future of hotels, this work presents new technologies for hotels suitable for correlating the customers’ on-site activities with online activities including passive location tracking using Wi-Fi devices’ connectivity, customer satisfaction evaluated via facial or voice recognition using inbuilt cameras/microphones altogether with data mining analysis. Moreover, this article explains how multidisciplinary consumer behavior can be analyzed by data mining to include this information in the vacation marketing approach for efficient business administration. © 2015, © The Author(s) 2015.
The explosion of data sources, accompanied by the evolution of technology and analytical techniques, has created considerable challenges and opportunities for drug development and healthcare resource utilization. We present a systematic overview these phenomena, and suggest measures to be taken for effective integration of the new developments in the traditional medical research paradigm and health policy decision making. Special attention is paid to pertinent issues in emerging areas, including rare disease drug development, personalized medicine, Comparative Effectiveness Research, and privacy and confidentiality concerns. © 2016, The Author(s).
This paper proposes a concept for a prescriptive control of business processes by using event-based process predictions. In this regard, it explores new potentials through the application of predictive analytics to big data while focusing on production planning and control in the context of the process manufacturing industry. This type of industry is an adequate application domain for the conceived concept, since it features several characteristics that are opposed to conventional industries such as assembling ones. These specifics include divergent and cyclic material flows, high diversity in end products’ qualities, as well as non-linear production processes that are not fully controllable. Based on a case study of a German steel producing company – a typical example of the process industry – the work at hand outlines which data becomes available when using state-of-the-art sensor technology and thus providing the required basis to realize the proposed concept. However, a consideration of the data size reveals that dedicated methods of big data analytics are required to tap the full potential of this data. Consequently, the paper derives seven requirements that need to be addressed for a successful implementation of the concept. Additionally, the paper proposes a generic architecture of prescriptive enterprise systems. This architecture comprises five building blocks of a system that is capable to detect complex event patterns within a multi-sensor environment, to correlate them with historical data and to calculate predictions that are finally used to recommend the best course of action during process execution in order to minimize or maximize certain key performance indicators. © 2015, Springer Fachmedien Wiesbaden.
The implementation of business intelligence (BI) and big data analytic (BDA) in managing organizational performance especially in public sector is important and critical. Weakness in managing the implementation strategy and the performance can result a massive impact to people and nation. Therefore, integration of BI and BDA are necessity to assist decision makers to increase efficiency in public services. However, preliminary study had identified limited implementation of BI and business analytics with organisation performance management (OPM) that led to inefficient performance in management practice. At the same time, large amount of data from various resources have headed to the emergent of big data analytics. Therefore, this research is proposed with the aim to develop an integrated framework of business intelligence and big data analytics (BI-BDA) for OPM. To achieve this goal, elements and sub elements of integrated BI-BDA and OPM implementation will be identified which focuses on big data analytics. Main outcome from this research is the new integrated framework of BI and BDA (BI-BDA) for OPM in public sector. The proposed framework is valuable for the practitioners as well as the stakeholders to ensure the OPM system to be more effective and dynamic. © 2016 American Scientific Publishers. All rights reserved.
With the increasing connectivity of devices, the amount of data that is recorded and ready for analysis is growing correspondingly. This is also the case for shop floors in flexible sheet metal handling and production. With the growing need for flexibility in production, the availability of machine tools is imminent. This paper shows different approaches that a classical manufacturing systems company such as TRUMPF takes in applying data mining techniques to address the new challenges which come with the Internet of things. In addition to classical methods, a new approach is introduced that does not need any alteration of the machine or its interfaces. © 2016 Walter de Gruyter Berlin/Boston.
Flexible manufacturing systems are the conceptual basis for highly flexible and efficient high volume manufacturing. The inherent complexity of these systems impedes the identification of bottlenecks and therefore their efficient operation. This paper describes a method which detects shifting bottlenecks in a flexible manufacturing system by using a Smart-Data-model. Hereby the complexity of these systems can be managed and an efficient operation can be reached. © 2016 Walter de Gruyter Berlin/Boston.
The adoption of Internet, new media and information technology tools has attracted the attention of many academicians and practitioners. The founders and the managers of nonprofits look forward to information and communication technologies (ICTs) not only to aid in organizational effectiveness, but also to aid in societal missions. This paper analyzes the attitude of nonprofit organizations toward the new ICTs such as social media, mobility, analytics and cloud computing (SMAC). We followed a mixed methods sequential explanatory strategy, in which two data collection phases were employed. The quantitative phase following the qualitative phase investigated the SMAC adoption and how the adoption of SMAC helped the organizations meet their missions. Overall, the adoption of SMAC showed immediate positive outcomes such as organizational effectiveness and bridging digital divide. © 2015 Commonwealth Secretariat.
Although recognized as increasingly relevant, the relationships between analytical orientation and process innovation are still unclear, specifically the role of analytical resources and capabilities in this association. This paper presents the results of empirical research conducted to investigate these complex links. The empirical data was drawn from 81 companies of different sectors in Brazil. Data analysis was mainly based on path analysis with structural equation modeling. The findings of this study indicate that analytical orientation can leverage process innovation. Moreover, information quality is somehow a more valuable resource to analytical orientation than information technology, and that analytical capabilities, especially analytical leadership, were proven to be more important to sustain analytical orientation than companies' analytical resources. Another important result, also with considerable repercussion in future research on the theme, was the predictive relevance of analytical capabilities for analytical orientation. This was much higher than the predictive relevance shown by analytical resources. Copyright © 2016, IGI Global.
Abstract: Many governmental agencies are currently engaged in activities to improve access to information which are encouraging recent Big Data (BD) trends. However, determining and demonstrating the benefit to emergency management information systems (EMIS) users has yet been realised. EMIS are critical to navigating a complex network of disparate IS used for real time, coordinated decision-making among agencies. Two significant challenges to incorporating BD analytical, visual, and predictive capabilities are mutual acceptance and system integration. This paper looks at the underlying principles of organisational mindfulness (OM) demonstrated in the convergence of EM processes, systems, and organisations in decision-making. The investigation proposes a design science research in information systems (DSRIS) approach to build an IS artefact for EMIS evaluation. Critical immediacy of information and decisions require adherence to processes which build capabilities to support EMIS users. This adds a layer of complexity to ISDT for EMIS, yet may equally lead to broader BD acceptance and integration. © 2016 Informa UK Limited, trading as Taylor & Francis Group.
Abstract: This research-in-progress paper describes a study of data mining using historical manufacturing test data at an electronic circuit board assembly firm. The purpose of this data mining is to determine, if in a multi-step manufacturing process at small and medium manufacturing firm downstream production problems can be identified and prevented by evaluating up stream data. While quantitative analysis and data mining have been used in manufacturing, the scope of their application is generally limited to a specific step or process that is a first order analysis of the data. © 2016 Informa UK Limited, trading as Taylor & Francis Group.
Abstract: This paper reports on the use of business process review workshops to facilitate collaborative engagement for requirements gathering as part of a business transformation project. The workshops produced a visual output of the design of a ‘to-be’ data driven business process for application processing of international applicants in a university environment. The resulting visual artefacts defining the ‘to-be’ data driven business process would suggest that business stakeholders can contribute more to business transformation projects than they themselves may realise. © 2016 Informa UK Limited, trading as Taylor & Francis Group.
Big data has captured the interests of scholars across many disciplines over the last half a decade. Business scholars have increasingly turned their attention to the impact of this emerging phenomenon. Despite the rise in attention, our understanding of what big data is and what it means for organizations and institutional actors remains uncertain. In this study, we conduct a systematic review on big data across business scholarship over the past six years (2009-2014). We analyzed 219 peer-reviewed academic papers from 152 journals from the most comprehensive business literature database. We conducted the systematic review both quantitatively and qualitatively using the data analysis software NVivo10. Our results reveal several key insights about the scholarly investigation of big data, including its top benefits and challenges. Overall, we found that big data remains a fragmented, early-stage domain of research in terms of theoretical grounding, methodological diversity and empirically oriented work. These challenges serve to improve our understanding of the state of big data in contemporary research, and to further prompt scholars and decision-makers to advance future research in the most productive manner. © 2016 Elsevier Ltd. All rights reserved.

This paper introduces National Institute of Standards and Technology (NIST)’s Sustainable Process Analytics Formalism (SPAF) to facilitate the use of simulation and optimization technologies for decision support in sustainable manufacturing. SPAF allows formal modeling of modular, extensible, and reusable process components and enables sustainability performance prediction, what-if analysis, and decision optimization based on mathematical programming. SPAF models describe (1) process structure and resource flow, (2) process data, (3) control variables, and (4) computation of sustainability metrics, constraints, and objectives. This paper presents the SPAF syntax and formal semantics, provides a sound and complete algorithm to translate SPAF models into formal mathematical programming models, and illustrates the use of SPAF through a manufacturing process example. © 2014, Springer Science+Business Media New York (outside the USA).

The need to connect deeply with the consumer has led many brands to replace traditional product-focused business models with a customer-centric approach. Insights2020-the largest global marketing research initiative to date-has provided the strategic framework and practical guidelines to “help leaders in the fields of marketing and insights and analytics drive customer-centric growth.” The 2015 project, led by Kantar Vermeer (renamed in 2016 from Millward Brown Vermeer), included interviews with 337 business, marketing, and insights and analytics leaders, and an online survey of 10,495 practitioners from 60 countries. In the pages that follow, Kantar Vermeer authors offer an in-depth look at the Insights2020 findings, data, and methodology that inform their roadmap for companies to develop a customer-centric business model in which insights and analytics plays a pivotal role. Such a model, according to the authors, is what has led to strong revenue growth at “overperformers” (i.e., brands experiencing higher revenue growth than their peers across the category). © 2016, World Advertising Research Center. All rights reserved.
Aiming at the big-data characteristics of intelligent workshop data, operational analysis and decision-making method driven by big-data were investigated. Based on the shortage of traditional methods, a new correlation, forecast and regulation decision-making pattern was proposed. The related methodology driven by big data was put forward, which included big data processing and analysis method, the workshop operation prediction method and the workshop operation decision method. The technical system to realize the methodology was designed. The key technologies used in big data processing method, big data temporal analysis method, data network modelling and analysis method, workshop operational analysis and forecast method and quantitative-control-based workshop operation decision method were discussed. The proposed methodology and technical system could provide important referential value to realize big data driven intelligent plants. © 2016, CIMS. All right reserved.
Purpose – There are various risks that can derail the adoption of business analytics (BA) practice in a telecom service provider (TSP) thereby jeopardising the possibility to increase profitability and improved customer experience. The purpose of this paper is to analyse different associated risks using situation-actor-process, learnings-actions-performance (SAP-LAP) model and build mitigation strategies for the adoption. Also the risks are ranked using the interactive ranking process (IRP) methodology and the dominating matrix provides insight to the actions and actors that need attention to improve the processes and performance. Design/methodology/approach – A case study of a TSP (X1) was analysed through close interactions with experts within the company and externals involved in setting up the BA practice in X1. Using the SAP-LAP framework risks were identified and then the IRP was used to rank the actors w.r.t performance and actions w.r.t processes. Findings – X1 has taken initiatives for setting up the BA practice in order to improve the profitability and customer experience through data insights. The suggested conceptual SAP-LAP model helps to address risk mitigation strategies for its adoption and the IRP frameworks helps in understanding the prioritisation matrix (using the ranking) to be considered to mitigate the risks. Research limitations/implications – The IRP framework is limited to certain relationships between actors, w.r.t processes and actions w.r.t performance for the prioritisation matrix of identified risks. This has scope to be further expanded to other relationships and therefore refining the findings. Also this approach could be used to study other industries too. Practical implications – SAP-LAP model identifies the risks in adopting the BA practice in a TSP. The synthesis of SAP leads to LAP, which bridges the gap by suggesting improvement actions based on the learning from the present situation, actors and processes. IRP provides the prioritisation matrix for mitigating the risks by identifying the dominating factors. Originality/value – BA practice plays a dominant role in a TSP. An approach to study the risks of its adoption using the SAP-LAP and IRP framework bridges the gap between the academic and corporate world. This paper is very relevant to managers involved in setting up a BA practice. For the academic, use of research model validates the identification of risks that are recognised in the corporate world and prioritising the risks that need to be addressed. © 2016, © Emerald Group Publishing Limited.
Enhancing efficiency of pharmaceutical batch production processes is an important challenge in times of increasing public pressure on healthcare costs and decreasing research productivity. This study presents a data-based procedure for systematic yield enhancements in drug product manufacturing, based on four steps. On the first step, production is reviewed to select relevant loss causes, which are assessed on the second step deductively with the goal of assigning measurable parameters. Descriptive Statistical Modelling of loss causes is then performed on the third step, enabling model-based enhancements of processes on the fourth step or, if necessary, a loop-back review of a given loss cause. An industrial case study was performed on production data of 88 batches and demonstrated the applicability of the procedure by prioritizing relevant loss causes, reducing required sample quantities by up to 8% and a cosmetic defect by about 70% by a process change. © 2016 Elsevier Ltd.
Although of high relevance to political science, the interaction between technological change and political change in the era of Big Data remains somewhat of a neglected topic. Most studies focus on the concept of e-government and e-governance, and on how already existing government activities performed through the bureaucratic body of public administration could be improved by technology. This article attempts to build a bridge between the field of e-governance and theories of public administration that goes beyond the service delivery approach that dominates a large part of e-government research. Using the policy cycle as a generic model for policy processes and policy development, a new look on how policy decision making could be conducted on the basis of ICT and Big Data is presented in this article. © Johann Höchtl, Peter Parycek, and Ralph Schöllhammer.
ABSTRACT: This article addresses the use and impact of the so-called Big Data on arts organisations. The article argues that arts bodies have been slower than some business sectors to enter the data sphere and to contribute to the key debates even though many arts organisations have been developing data systems as a means of responding to the demands of funding bodies for more concrete measurement of impact. The article tests this hypothesis by examining the outcomes of a major NESTA/ACE/AHRC-funded project which applied ethnographic method to analyse the use of data by three major arts bodies operating in London, a project which built on the arguments forwarded in a previous NESTA document, Counting what counts (NESTA 2013). The project, managed by the Audience Agency, on which the present author was principal researcher, was delivered over 12 months from August 2014 to August 2015. The article outlines the methodology, and highlights the emergence of a new ethnographic form, “thick data” grounded in data theory and practice. This form is then used to illuminate some of the practices used by large arts organisations to handle audience data, and to develop inclusive and comprehensive future programming strategies. A range of embedded practices are discussed and examined illustrating the various strategies used by the partner organisations to address key data issues and problems. From this detailed analysis, the article then outlines a number of models for the organisation of data within arts institutions. © 2016 Informa UK Limited, trading as Taylor & Francis Group.
To improve the efficiency and productivity of modern manufacturing, the requirements for enterprises are discussed. The new emerged technologies such as cloud computing and internet of things are analyzed and the bottlenecks faced by enterprises in manufacturing big data analytics are investigated. Scientific workflow technology as a method to solve the problems is introduced and an architecture of scientific workflow management system based on cloud manufacturing service platform is proposed. The functions of each layer in the architecture are described in detail and implemented with an existing workflow system as a case study. The workflow scheduling algorithm is the key issue of management system, and the related work is reviewed. This paper takes the general problems of existing algorithms as the motivation to propose a novel scheduling algorithm called MP (max percentages) algorithm. The simulation results indicate that the proposed algorithm has performed better than the other five classic algorithms with respect to both the total completion time and load balancing level. © 2015, Springer-Verlag London.
Big data (BD) has attracted increasing attention from both academics and practitioners. This paper aims at illustrating the role of big data analytics in supporting world-class sustainable manufacturing (WCSM). Using an extensive literature review to identify different factors that enable the achievement of WCSM through BD and 405 usable responses from senior managers gathered through social networking sites (SNS), we propose a conceptual framework using constructs obtained using reduction of gathered data that summarizes this role
Cloud Manufacturing twining with Internet of Things (IoT) has been waked up to achieve final intelligent manufacturing. With the IoT technologies such as radio frequency identification (RFID) implemented in manufacturing sites, enormous data will be generated. Such data are so complex, abstract, and variable so that it is difficult to make full use of the data which carry great myriad of useful information and knowledge. This paper presents a visualization approach for the RFID-enabled shopfloor logistics Big Data from Cloud Manufacturing. An innovative RFID-Cuboid model is used for reconstructing the RFID raw data given the production logic and time series. Several contributions are highlighted. Firstly, a possible approach to integrate IoT and Cloud Manufacturing is introduced to upgrade and transform the traditional industry for an intelligent future. Secondly, an RFID-Cuboid model is proposed by using the production logic and time stamps to chain the RFID data so that the data could be interpreted. Thirdly, a real-life case is reported to show the feasibility and practicality of the proposed visualization approach to help different end-users to ease their daily operations. Lessons and insights from this case are meaningful for the implementation of IoT-enabled Cloud Manufacturing and Big Data analytics in industry field. © 2015, Springer-Verlag London.
Hydrostatic systems are considered as essential supporting structures in heavy machine tools. The calculations and analyses for hydrostatic bearings are always laborious because of the involvement of several disciplines such as elastic mechanics, hydromechanics, thermodynamics, and other factors in the design. It is well known that large data and cloud technology are capable of processing and transmission of hydrostatic system studies. In this work, a cloud manufacture model is presented to provide big data storage, transmission, and processing platform for designers, manufacturers, and users of hydrostatic bearings. All participants involved in the manufacturing were linked together by timely information communication in order to ensure that the customized products met with the actual conditions in the cloud server. Based on the actual requirements of the user, the carrying capacity of hydrostatic system was analyzed by a designer using finite difference method and the results were sent to the manufacturer for machining of components. The monitored data from the consumer could be fed back to the designer for performance evaluation through the cloud server. It was found that customized manufacturing tasks could be finished more reliably and efficiently if all participants exchanged the data through cloud platform in real time. © 2015, Springer-Verlag London.
Overview This article introduces NTT Group initiatives towards advanced application of the Internet of Things (IoT) and big data. The contents of the article are based on the keynote lecture presented by NTT President and Chief Executive Officer Hiroo Unoura at NTT R&D Forum 2016 held February 18-19, 2016.
As the Internet of Things (IoT) and machine-to-machine (M2M) communications become prevalent, the number of devices that connect to networks is rapidly increasing. Data traffic via networks has thus grown to a significant volume. The connectivity to a network, and the quality of the network itself, are becoming prerequisites as opposed to added value that generates profits. Given this as a background, the service-oriented business models based on requests by upper layers, which have been around for some time, are expected to develop further and faster. To support such business models, Fujitsu has developed a real-time full capture system compatible with the 200 Gbps bandwidth, leveraging the Company's networking and big data analysis technologies. The system is designed to capture and analyze a large volume of highspeed data using virtual platforms. This paper briefly looks at user expectations for virtual platform services and challenges with regards to providing such services. Then it describes the characteristics of the developed system.
Objectives: A large amount of informative data is being captured and processed by today's organizations and is continuing to increase exponentially. It becomes computationally inaccurate to analyze such big data for decision making systems. Methods/Analysis: Hadoop, which is a working model based on the Map-Reduce framework with efficient computation and processing of Big Data. Findings: Most of the traditional classification algorithms have issues such as class imbalance and dimension reduction on Big Data. However, a large part of the data produced today are incomplete and inaccurate, so large organizations prefer relational databases to store their information, but the user query processing speed is very low. Unlike existing solutions that require a prior knowledge of classification accuracy for various types of data characteristics, which is impossible to obtain in practice. Applications/Improvement: In this paper, we have given a compared proposed model to different big data feature selection and classification models along with advantages and limitations.
Overview: While initiatives that use big data to improve productivity have become commonplace in the industrial sector, the quantitative assessment of the relationship between worker actions and organizational results and its use to make improvements has proved difficult in service businesses and knowledge work. Hitachi has devised a technique that uses wearable sensors and AI to identify action characteristics that influence organizational KPIs. This article describes a demonstration project at The Bank of Tokyo-Mitsubishi UFJ, Ltd. that involved a comprehensive study of the relationship between organizational KPIs and action characteristics to identify those action characteristics that were effective at improving KPIs for each attribute and situation. This succeeded in identifying the people and actions that contribute to the organization. Hitachi intends to use this technique to build management support systems that enable teams to perform to their full potential.
Intelli-Stat introduces a product that is able to discover previously unknown patterns from electronics test data to help electronics manufacturing enterprises to produce products faster. The product offered by Intelli-Stat is software that can provide the intelligence to engineers on how to optimize the test time in manufacturing. The software is installed in the test machine itself so that it can analyze the pattern and provide on the spot recommendation to engineers. A study was conducted to review the state of the art of data mining applications in manufacturing. A decision tree based data mining technique for electronics product was selected to form the Intelli-Stat data mining framework that is able to meet the requirement of Intelli-Stat. An experiment was conducted to validate the concept of the framework and a visual prototype was built to demonstrate the capability of the software. © Research India Publications.
It is well known that professional sports teams have made extensive use of analytics to improve their on-field performance. However, it is not as apparent that these same organizations use analytics to improve performance on the business side. The North American professional sports industry is unique in that the teams operate as legal monopolies/duopolies, with very few cities having more than one major-league team in a particular sport. Thus, our intent is to explore the adoption and assessment of business analytics in professional sports organizations. An empirical analysis is conducted concerning management's perception of the effects of business analytics as well as the actual change in financial performance since the adoption of analytics by the organizations. © 2015 Elsevier Ltd.
In their study of the state-level party organizations in the US, Cotter et al. (1984) introduce a theory of party organizational strength, arguing that the strongest parties exhibit both organizational complexity and programmatic capacity. In the present era, organizational complexity and programmatic capacity still emerge as the primary dimensions of party organizational strength, but this research demonstrates the importance of new indicators of party technological capacity to our understanding of party effectiveness in the early 21st century. In particular, parties that hire technologically-oriented staff are best equipped to carry out party programmatic activities, as they have the capacity to conduct voter mobilization programs and sophisticated data analytics in an online environment. Using data from a 2011 survey of the state party organizations in the US, this research shows that party organizational strength and party technological capacity are interrelated and overlapping concepts and are essential to understanding the parties’ programmatic and voter outreach capabilities. © 2015, The Author(s) 2015.
As a rapidly expanding market, carsharing presents a possible remedy for traffic congestion in urban centers. Especially free-floating carsharing, which allows customers to leave their car anywhere within the operator's business area, provides users with flexibility, and complements public transportation. We present a novel method that provides strategic and operational decision support to companies maneuvering this competitive and constantly changing market environment. Using an extensive set of customer data in a zero-inflated regression model, we explain spatial variation in carsharing activity through the proximity of particular points of interests, such as movie theaters and airports. As an application case, as well as a validation of the model, we use the resulting indicators to predict the number of rentals before an expansion of the business area and compare it to the actual demand post-expansion. We find that our approach correctly identifies areas with a high carsharing activity and can be easily adapted to other cities. © 2015 Elsevier Ltd.
Data-driven decision making continues to be a growing educational reform initiative across the globe. The effective use of data requires that teachers develop the knowledge and skills to analyze and use data to improve instruction. The purpose of this article is to examine teachers’ capacity for and beliefs about data use. These issues are examined through a review of research in the past decade. We find that teachers’ beliefs about and capacity for data use are often not connected within the literature or in practice, but we argue they are the heart of the connection between data and instructional change. Teachers’ capacity to use data and their beliefs about data use are shaped within their professional communities, in training sessions, and in their interactions with coaches, consultants, and principals. However, efforts to develop teachers’ capacity for data use often fall short of their goals. Correspondingly, teachers have varied beliefs about data use, and some feel they lack the ability to use data to inform instruction. In order to be more successful, capacity building should directly address teachers’ beliefs, and data use must be decoupled from external accountability demands and involve a variety of information on student learning. © 2015, Springer Science+Business Media Dordrecht.
Purpose – The purpose of this paper is to describe the current environment for libraries to consider the value of using data to support decision-making. Design/methodology/approach – This paper contains literature review and commentary on this topic that has been addressed by professionals, researchers and practitioners. Findings – In developing a library’s strategic direction, it is essential that evidentiary data be referenced to supplement the organization’s rationale for decision-making. There is an expectation by stakeholders that libraries are able to generate reports and decisions based on aggregated data for in-demand reporting. Therefore, capturing, analyzing and reporting decisions based on data are indispensable in today’s libraries. Originality/value – The value in addressing this topic is to examine the option by libraries to use data to support data-driven decision-making. © 2016, © Emerald Group Publishing Limited.
Grey Oaks, a premier country club and gated residential community established in Naples, Florida, developed and used a collaborative and highly feedback-driven system to build adaptive capacity and foster proactive change. Special attention has been given to the unique use and refinement of its business plan and planning process through the feedback management received from club members. This highly adaptive business plan helped Grey Oaks to endure the economic downturn that began in 2008, and the company’s ability to thrive in the midst of economic instability has demonstrated the effectiveness of the plan as a model capable of sustaining the cultural and financial health of the club and community. The Grey Oaks planning model has been an innovation in its industry, and other organizations can greatly benefit by studying the implementation and utilization of this evidence-driven model. © 2016 Taylor & Francis.
Purpose: The purpose of this paper is to derive a taxonomy of business models used by start-up firms that rely on data as a key resource for business, namely data-driven business models (DDBMs). By providing a framework to systematically analyse DDBMs, the study provides an introduction to DDBM as a field of study. Design/methodology/approach: To develop the taxonomy of DDBMs, business model descriptions of 100 randomly chosen start-up firms were coded using a DDBM framework derived from literature, comprising six dimensions with 35 features. Subsequent application of clustering algorithms produced six different types of DDBM, validated by case studies from the study’s sample. Findings: The taxonomy derived from the research consists of six different types of DDBM among start-ups. These types are characterised by a subset of six of nine clustering variables from the DDBM framework. Practical implications: A major contribution of the paper is the designed framework, which stimulates thinking about the nature and future of DDBMs. The proposed taxonomy will help organisations to position their activities in the current DDBM landscape. Moreover, framework and taxonomy may lead to a DDBM design toolbox. Originality/value: This paper develops a basis for understanding how start-ups build business models capture value from data as a key resource, adding a business perspective to the discussion of big data. By offering the scientific community a specific framework of business model features and a subsequent taxonomy, the paper provides reference points and serves as a foundation for future studies of DDBMs. © 2016, © Emerald Group Publishing Limited.
Globalization has forced manufacturing organizations to take cost reduction and productivity improvement initiatives in order to gain competitive advantage. In this drive integration of business processes and systems and improvements in information flow across organization becomes essential which is obtained by adopting Enterprise Resource Planning (ERP) systems which acts as enabler. Effective integration of ERP with Supply Chain Management (SCM) enables sharing of information on order and shipment with multiple organizations and to better manage information, cash and product flows. Changing business environment has turned supply chains (SC) more complex as they are moving into the digital world through multiple initiatives, embracing new technologies such as big data, social media, mobile, Internet of things simultaneously. The convergence of these new technologies has finally resulted into phenomenal growth in the underlying data set not only in volume, but also in variety, veracity, velocity and value. Organizations are trying to use new forms of data to improve supply chain excellence. Big data analytics (BDA) allows to quickly combine structured data such as that in ERP, and combine them with unstructured data for analysis that could lead to increased efficiency and profitability in the SC. Purpose of this paper is to examine the potential of ERP and SCM integrated with BDA contributing to improvement in the performance of manufacturing organizations. The analysis will be beneficial to practitioners and enterprise to evaluate performance. © International Science Press.
In a Big Data environment, in order to study the decision-making problem of Big Data information investment and the effects of using Big Data information to improve industry cost on supply chain coordination, firstly the importance of Data Company in supply chain was analyzed, and the original supply chain model was built. Meanwhile, some changes of consumer behavior were analyzed in a Big Data environment. Based on these, the market demand function and the benefit model of stakeholder were built and analyzed. Findings: (1) The first finding is whether an enterprise was suitable for gaining Big Data to improve its costs, which was determined by the cost improvement coefficient
Health care and educational legislation and policy require that clinicians demonstrate, using measurement and report of outcomes, accountability for services rendered. Clinical algorithms have been developed and are used by various health care professionals to assist with hypothesis generation and systematic clinical reasoning
Health care and educational legislation and policy require that clinicians demonstrate, using measurement and report of outcomes, accountability for services rendered. Clinical algorithms have been developed and are used by various health care professionals to assist with hypothesis generation and systematic clinical reasoning
The drinking water quality is important for residents' daily life, so a multiple criteria decision-making model based on data driven and information gain ratio is set up to evaluate drinking water quality. Firstly, the initial weights of every index in multiple indices system are determined by the variation coefficient method. Secondly, the initial evaluation results are obtained by the comprehensive weighted indices evaluation method. Finally, information gain method is applied to adjust the weights of indices according to the contribution degree and evaluate the system again. The model can not only comprehensively evaluate the drinking water quality, but also find the important indices playing a key role. Water quality distribution is shown intuitively by GIS to illustrate the validity of the decision-making model. © 2016, Editorial Office of Journal of Dalian University of Technology. All right reserved.
To ensure a series of missions can be completed with only finite breaks, many systems are required to guarantee system safety and mission success. Of these, maintenance decision support is vital. One widely used maintenance strategy has been selective maintenance. Most traditional selective maintenance optimisation research has focused on binary state systems, which are subject to distribution deterioration or failure. However, a majority of systems used in aerospace or industrial applications are multi-state systems with more than two states deteriorating at the same time, meaning that real-time state distribution is needed to provide more timely and effective maintenance. This paper presents a novel integrated system health management-oriented maintenance decision support methodology and framework for a multi-state system based on data mining. An aero-engine system numerical example is given to illustrate the methodology, the results of which demonstrate the significant advantages of using data mining to efficiently obtain state distribution information, and the benefits of using a robust optimal model to choose suitable strategies. This methodology, which is applicable to multi-state systems of varying sizes, has the ability to solve maintenance problems when imperfect maintenance quality is considered. © 2015 Taylor & Francis.
Over the last few years, the volume and the variety of data that we deal has grown up enormously from various sources such as social media networks, machine generated data, mobile networks, CRM data etc. The exponential growth of data creates various challenges to the traditional business intelligence (BI) and analytic solutions. Newer approaches to handle Big data and extract better business value are the topmost priorities for both IT management and business users. In the recent years, organisations have made strategic decisions using Big data analytics, which have resulted in better business processes, minimise risks, reduce costs and increase business value in broad-spectrum. In this paper, we epitomise Big data analytics and the promising technologies to analyse Big data. We have also experimented predictive analytics to evaluate the sales forecasting trend and enrich customer relationship management (CRM) data with all the obtainable insights. This experiment has been executed for a pharmaceutical company using SFDC (SalesForce.Com) in cloud environment. Copyright © 2016 Inderscience Enterprises Ltd.
Background: In the competitive data driven business world, business Intelligence (BI) team converts the raw operational data to information for decision making. Operational system captures the day-to-day operations and BI database refreshes operational data periodically. Methods: A component to create the metadata repository which maintains the current BI database summary by logical data partitioning using range based partition for frequently changing parameter which are critical to business. During different time frequency, using metadata repository component identifies the latest data victim between BI vs operational data and refreshes the modified victim to BI database. Findings: In traditional data loading approach from Operational system to BI database, huge volume of data gets refreshed periodically irrespective of modifications, which leads to higher processing time and cost. To overcome this limitation, this proposed methodology helps to identify the latest data victims present in operational systems instead of bulk data replacement which can minimize the processing time and enables faster data transformation to achieve Time to Decision and Quick to Market implementation for business enhancements. Also component can be scheduled for data refresh with different time frequency for multiple critical to business as well as frequently changing parameters. Applications: In financial, traffic, weather, e-business, Logistics & stock management transactions, data changes frequently and process big data periodically to gain real-time knowledge discovery for time sensitive decision making.
Real-time access to business performance information is critical for corporations to run a competitive business and respond to a continuously changing business environment with ever-higher levels of competition. The timely analysis and monitoring of business processes are essential to identify non-compliant situations and react immediately to those inconsistencies in order to respond quickly to competitors. In this regard, the integration of business intelligence (BI) systems with Process Aware Information Systems (PAIS) can become a key tool for business users in decision making. However, current BI systems are not suitable for optimising and improving end-to-end processes since these are normally business domain specific and are not sufficiently process-aware to support the needs of process improvement type activities. In addition, highly transactional business environments may produce vast amounts of event data that cannot be efficiently managed by the use of traditional storage systems which are not designed to manage vast amounts of event data. We introduce a cloud-based architecture that leverages big-data technology to support performance analysis on any business domain, in a timely manner and regardless of the underlying concerns of the operational systems. Likewise, we demonstrate the ability of the solution to provide real-time business activity monitoring on big-data environments with low hardware costs. © 2015 Elsevier Ltd. All rights reserved.
The huge amount of data are being generated by rapid development of internet and social networks with the wide spread of smart devices. Many data has started to be produced more and more and become complex as the IoT, M2M is active. A lot of research has been progressed in business and government in order to create new business model using this. The application value of these big data is expanding in various fields. The demand of technique and professional manpower as a source of future competitive advantage is expected to jump. In case of Korea it has been estimated that there is a shortage of professional manpower and technology gap with the advanced counties. Therefore, we analyzed the characteristics and paradigm of the Big data business in this paper. We also analyzed applied technology for development model of Big data business through it. In particular, we divided the development model for Big data business step-by-step and analyzed consideration about this systematically. The companies and public institutions based on the analyzed result in this paper will provide advantages to production of big data systems. © 2016 SERSC.
Purpose - Many power producers are looking for ways to develop smarter energy capabilities to tackle challenges in the sophisticated, non-linear dynamic processes due to the complicated operating conditions. One prominent strategy is to deploy advanced intelligence systems and analytics to monitor key performance indicators, capture insights about the behavior of the electricity generation processes, and identify factors affecting combustion efficiency. Thus, the purpose of this paper is to outline a way to incorporate a business intelligence framework into existing coal-fired power plant data to transform the data into insights and deliver analytical solutions to power producers. Design/methodology/approach - The proposed ten-step business intelligence framework combines the architectures of database management, business analytics, business performance management, and data visualization to manage existing enterprise data in a coal-fired power plant. Findings - The results of this study provide plant-wide signals of any unusual operational and coal-quality factors that impact the level of NOx and consequently explain and predict the leading causes of variation in the emission of NOx in the combustion process. Research limitations/implications - Once the framework is integrated into the power generation process, it is important to ensure that the top management and the data analysts at the plants have the same perceptions of the benefits of big data and analytics in the long run and continue to provide support and awareness of the use of business intelligence technology and infrastructure in operational decision making. Practical implications - The key finding of this study helps the power plant prioritize the important factors associated with the emission of NOx
The entrepreneurship required for a creative economy is one that promotes job creation through the creativity of CEOs who have entrepreneurial spirit as well as excellent knowledge and technology. This entrepreneurship plays a critical role in the construction of a venture business ecosystem. Meanwhile, big data, until now, have consisted only of numbers and texts that are specified and standardized by certain structured rules. Nowadays, however, big data analysis methods are being developed to gain information and business opportunities from new aspects through the use of nonstandard data. The big data technology is becoming a core element in the new digital age rather than just a trend, and the big data strategies are progressing from the testing stage to the implementation stage. In particular, as the importance of nonstandard data is increasing, limitations of the conventional system analysis appeared, and the analysis methods of advanced analytics are being highlighted. The scope of big data is anticipated to expand in enterprises with the emergence of many application cases, such as the real-time use of the data. The business model for the formation of the entrepreneurial ecosystem of the creative economy can assist the construction of an entrepreneurial platform as a catalyst for the stimulation of innovative business start-ups. This entrepreneurial platform can produce such effects as fast product development, commercialization of technology, risk reduction, and job creation. In this study, the operating model formation according to the characteristics of the industry or the business model in operation, the profit creation sources, the scope of value provision, and the priorities with regard to the required data were examined. Furthermore, cases related to the performance of big data business models of advanced corporations were examined, and a big data business model for the formation of the entrepreneurial ecosystem of the creative economy in the 21st century was derived. © Springer Science+Business Media Singapore 2016.
Purpose - While it is commonly recognised that Big Data have an immense potential to generate value for business organisations, appropriating value from Big Data and, in particular, Big Dataenabled analytics is still an open issue for many organisations. The purpose of this paper is to develop a maturity model to support organisations in the realisation of the value created by Big Data. Design/methodology/approach - The maturity model is developed following a qualitative approach based on literature analysis and semi-structured interviews with domain experts. The completeness and usefulness of the model is evaluated qualitatively by practitioners, whereas the applicability of the model is evaluated by Big Data maturity assessments in three real-world organisations. Findings - The proposed maturity model is considered exhaustive by domain experts and has helped the three assessed organisations to develop a more critical understanding of the next steps to take. Originality/value - The maturity model integrates existing industry-developed maturity models into one single coherent Big Data maturity model. The proposed model answers the call for research on Big Data to abstract from technical issues to focus on the business implications of Big Data initiatives. © Emerald Group Publishing Limited.
Over recent years, there has been tremendous growth of interest in business intelligence (BI) for higher education. BI analysis solutions are operated to extract useful information from a multi-dimensional datasets. However, higher education-based business intelligence is complex to build, maintain and it faces the knowledge constraints. Therefore, data mining techniques provide an effective computational methods for higher educationbased business intelligence. The main purpose of using data mining approaches in business intelligence is to provide decision making solution to higher education management. This paper presents the implementation of data mining approaches in business intelligence using a total of 13508 postgraduates (PG) data. These PG data are to allow the research to identify the postgraduates who Graduate On Time (GOT) via business intelligence process integrating data mining approaches. There are four layers will be discussed in this paper: data source layer (Layer 1), data integration layer (Layer 2), logic layer (Layer 3), and reporting layer (Layer 4). The main scope of this paper is to identify suitable data mining which is to allow decision making on GOT so as to an appropriate analysis to education management on GOT. The results show that Support Vector Machine (SVM) classifier is with better accuracy of 99%. Hence, the contribution of data mining in business intelligence allows an accurate decision making in higher education. © 2016 Penerbit UTM Press. All rights reserved.
Genetic Algorithms, provides continuous and discrete functions to optimization and search to be carried across various data sets. Genetic Programming, are evolved to program computer systems as an approximate problem solving methods. Genetic algorithms is take an important role, to find complex spacing handling and in many other fields namely financial, marketing, engineering, automated systems, AI etc. This paper deals with the specific utilization of genetic algorithm across data mining domain. The data mining techniques like Prediction, clustering and micro clustering are effectively done by implementing the successful methods of mutation and crossover of the Genetic Algorithm. It is based on the genital process, henceforth it can be used to design and brilliant business intelligent system which works on the words population principles. © International Science Press.
In recent years, data centric business process is becoming a new trend in the development of BPM. It is very valuable for enterprises to make full use of these resources and knowledge, to dig out the new process model or to find the deficiency in the existing process model. In this paper, the author analyzes the computer-aided business process management based on data mining and artifact. Process model mining technology is an important way to implement process reengineering and optimization. Through the construction of computer aided business process model. The results show that the improved data mining algorithm has significant advantages in query time and execution efficiency, shows higher recall and precision rate.
The “performance movement” has been a subject of enthusiasm and frustration for evaluators. Performance measurement, data analytics, and program evaluation have been treated as different tasks, and those addressing them speak their own languages in their own circles. We suggest that situating performance measurement and data analytics within the broader field of evaluation would be theoretically parsimonious and fruitful. Scholars and practitioners of performance measurement and analytics may profitably use an evaluation mind-set and frame their tasks within the multidisciplinary field of evaluation practice. With this change in mind-set, we discuss some implications of viewing measurement, analytics, and other evaluation-related capacities within public organizations as part of an integrated, evaluation mission-support function. Working with other mission-support functions, evaluation capacity could be used by operating units to improve learning, strategy, and performance and better accomplish the mission. We outline steps that could be considered to help forge a more strategic and comprehensive approach to evaluation in public and nonprofit organizations. © 2015, SAGE Publications. All rights reserved.
Micro, large share of enterprises in most countries and are cornerstone of economies. They are major source of employment and income in most countries. However, they need to be competitive to survive and grow in the present era of liberalized and digital economy, making national boundaries almost irrelevant. Most of the SMEs are inward looking and not very competitive. They merely survive and sooner than later, seize to exist. One of the reasons for this lackluster performance is their inability to access and analyze right kind of information for decision making. Large corporates can afford to hire world class consultancy firms like McKinsey, Price Waterhouse Coopers, etc. whereas MSMEs find it almost impossible to afford them. These large firms get extra advantage, as the information generated by analysis of data collected using reliable research methodology, accurately serves the purpose of aiding them in making decisions under intricate situations. Also, data related to visitors of social media sites and navigation patterns on portals are widely used by large firms for predictive analysis. On the other hand, decision making at MSMEs is often based on ad-hoc methods or gut feeling making them more vulnerable to failures. Although, globalization has led to increased competition, it also offers opportunities for expansion of business, for firms with strategic and competitive strengths. Furthermore, the size of firm does not matter. In this era of digital economy, business success is strongly associated with up-to-date information and knowledge of markets, consumers and competitors acquired using scientifically proven research methods. Business research is vital for sustainability of SMEs in global economy. To grab opportunities that globalization offers, MSMEs need information to support their decisions on target markets, product-mix, branding, consumer perception, trends, supply and demand forecasting and quality control. The present research paper Analyse the importance of Business analytics and their effectiveness in SME sector.
It is impossible to deny the significant impact from the emergence of big data and business analytics on the fields of Information Technology, Quantitative Methods, and the Decision Sciences. Both industry and academia seek to hire talent in these areas with the hope of developing organizational competencies. This article describes a multi-method research agenda that was executed to ascertain insights regarding which knowledge, skills, and abilities, (KSAs) are valued by employers seeking to hire entry-level analytics professionals from schools of business. Current undergraduate business analytics programs are first examined to define the research scope. A triangulated mixed-method research approach is then used to determine the knowledge, skills, and abilities that are in demand for entry-level jobs in this area. Finally, the multi-method triangulation of data is combined with experiences in building academic programs in business analytics at two nationally-ranked state universities to offer insights for those seeking to develop academic programs in this area. © 2016 Decision Sciences Institute.
Business analytics is a fast-growing job market for business school graduates. Hence, researchers have made many calls to enhance business analytics training in business schools to meet the growing market demand for analyticssavvy employees. A growing set of business analytics courses have begun to address these calls. In this paper, we examine the maturity of business analytics offerings in business schools in the United States by analyzing current business analytics-related course offerings of the top 104 business schools (363 courses) and 20 unranked business schools (51 courses) in the United States. We analyze these data by examining the types of courses offered and rank the schools based on their maturity levels in terms of business analytics offerings. Our findings indicate that, to the extent that these schools reflect what is happening across the nation, business schools still have a long way to go before they reach higher levels of business analytics maturity and that they are not yet in an ideal position to serve the presumed industry needs. We offer actionable recommendations. © 2016 by the Association for Information Systems.
Health care organizations must develop integrated health information systems to respond to the numerous government mandates driving the movement toward reimbursement models emphasizing value-based and accountable care. Success in this transition requires integrated data analytics, supported by the combination of health informatics, interoperability, business process design, and advanced decision support tools. This case study presents the development of a master's level cross- and multidisciplinary informatics program offered through a business school. The program provides students from diverse backgrounds with the knowledge, leadership, and practical application skills of health informatics, information systems, and data analytics that bridge the interests of clinical and nonclinical professionals. This case presents the actions taken and challenges encountered in navigating intra-university politics, specifying curriculum, recruiting the requisite interdisciplinary faculty, innovating the educational format, managing students with diverse educational and professional backgrounds, and balancing multiple accreditation agencies. © The Author 2016.
Ingram Micro, the world's largest distributor of technology products, operates in a high-volume low-margin environment. The company started its Business Intelligence and Analytics practice in North America in 2009. This group has since built and deployed a scalable, innovative price-optimization engine, a set of analytics applications to identify sales opportunities for Ingram Micro's sales force and an integrated digital marketing platform to run data-driven marketing campaigns for its customers and end-user businesses. Since 2011, these products and analytics programs have generated $1.3 billion of incremental product revenue and $42 million of incremental gross profit. Our next steps are to continue to implement these best practices in regions outside of North America and continue our activities that enable our sales force to generate revenue. © 2016 INFORMS.
Background: Quantitative electroencephalogram (EEG) is one neuroimaging technique that has been shown to differentiate patients with major depressive disorder (MDD) and non-depressed healthy volunteers (HV) at the group-level, but its diagnostic potential for detecting differences at the individual level has yet to be realized. Quantitative EEGs produce complex data sets derived from digitally analyzed electrical activity at different frequency bands, at multiple electrode locations, and under different vigilance (eyes open vs. closed) states, resulting in potential feature patterns which may be diagnostically useful, but detectable only with advanced mathematical models. Methods: This paper uses a data mining methodology for classifying EEGs of 53 MDD patients and 43 HVs. This included: (a) pre-processing the data, including cleaning and normalization, applying Linear Discriminant Analysis (LDA) to map the features into a new feature space

Ranking journals is a longstanding problem and can be addressed quantitatively, qualitatively or using a combination of both approaches. In the last decades, the Impact Factor (i.e., the most known quantitative approach) has been widely questioned, and other indices have thus been developed and become popular. Previous studies have reported strengths and weaknesses of each index, and devised meta-indices to rank journals in a certain field of study. However, the proposed meta-indices exhibit some intrinsic limitations: (1) the indices to be combined are not always chosen according to well-grounded principles
Providing data-centric decision support for organizational decision processes is a crucial but challenging task. Business intelligence and analytics (BI&A) equips analytics experts with the technological capabilities to support decision processes with reliable information and analytic insights, thus potentially raising the quality of managerial decision making. However, the very nature of organizational decision processes imposes conflicting task requirements regarding adaptability and rigor. This research proposes ambidexterity as a theoretical lens to investigate data-centric decision support. Based on an in-depth multiple case study of BI&A-supported decision processes, we identify and discuss tensions that arise from the conflicting task requirements and that pose a challenge for effective BI&A support. We also provide insights into tactics for managing these tensions and thus achieving ambidexterity. Additionally, we shed light on the relationship between ambidexterity and decision quality. Integrating the empirical findings from this research, we propose a theory of ambidexterity in decision support, which explains how such ambidexterity can be facilitated and how it affects decision outcomes. Finally, we discuss the study's implications for theory and practice. © 2015 Elsevier B.V.
This paper proposes two novel algorithms for adaptive crowdsourcing in 60-GHz medical imaging big-data platforms, namely, a max-weight scheduling algorithm for medical cloud platforms and a stochastic decision-making algorithm for distributed power-and-latency-aware dynamic buffer management in medical devices. In the first algorithm, medical cloud platforms perform a joint queue-backlog and rate-aware scheduling decisions for matching deployed access points (APs) and medical users where APs are eventually connected to medical clouds. In the second algorithm, each scheduled medical device computes the amounts of power allocation to upload its own medical data to medical big-data clouds with stochastic decision making considering joint energy-efficiency and buffer stability optimization. Through extensive simulations, the proposed algorithms are shown to achieve the desired results. © 2015 IEEE.
The prevalence of data mining by businesses and government organizations raises concerns among many individuals about the privacy of their personal data. We address this issue by offering a different perspective that reconciles the conflicting desires of businesses and consumers. We describe privacy, data mining, and their interaction in the larger context, identify the costs and benefits of the uses of data mining, and discuss potential stakeholders found at the intersection of the two subjects. To help synthesize our proposed code of ethical conduct, we examine existing codes of conduct and how they relate to the issue of privacy in the context of data mining with people, processes, and technology. Showing that a uniform code of ethical conduct for online privacy is feasible from both a managerial and ethical perspective, we provide an initial philosophical and principle synthesis that businesses and organizations can tailor for their own specific customers and needs. The developed code of ethical conduct respects consumers’ desire for privacy while allowing businesses to use data mining techniques to elicit information that benefits both the business and the consumer. © 2015 by the Association for Information Systems.
Big Data and Business Analytics (BD/BA) have emerged as an important theme for both practitioners and researchers in recent years. The aim of this article is to review the state-of-the-art of BD/BA applications in the supply chain by means of bibliometrics and systematic analysis. In order to achieve this goal, we adopted ProKnow-C, as intervention tool. As main result, we identified key trends, challenges and knowledge gaps for BD/BA. © 2015 IEEE.

Hospitals have invested and continue to invest heavily in building information systems to support operations at various levels of administration. These systems generate a lot of data but fail to effectively convert these data into actionable information for decision makers. Such ineffectiveness often is attributed to a lack of alignment between strategic planning and information technology (IT) initiatives supporting operational goals. We present a case study that illustrates how the use of digital dashboards at St. Joseph Mercy Oakland (SJMO) Hospital in Pontiac, Michigan, was instrumental in supporting such an alignment. Driven by a focus on key performance indicators (KPIs), dashboard applications also led to other tangible and intangible benefits. An ability to track KPIs over time and against established targets, with drill-down capabilities, allowed leadership to hold staff members accountable for achieving their performance targets. By displaying the dashboards in prominent locations (such as operational unit floors, the physicians' cafeteria, and nursing stations), SJMO ushered in transparency in the planning and monitoring processes. The need to develop KPI metrics and drive data collection efforts became ingrained in the work ethos of people at every level of the organization. Although IT-enabled dashboards have been instrumental in supporting this cultural transformation, the focus of investment was the ability of technology to make collective vision and action the responsibility of all stakeholders.
Abstract To reduce the production costs and breakdown risks in industrial manufacturing systems, condition-based maintenance has been actively pursued for prediction of equipment degradation and optimization of maintenance schedules. In this paper, a two-stage maintenance framework using data-driven techniques under two training types will be developed to predict the degradation status in industrial applications. The proposed framework consists of three main blocks, namely, Primary Maintenance Block (PMB), Secondary Maintenance Block (SMB), and degradation status determination block. As the popular methods with deterministic training, back-propagation Neural Network (NN) and evolvable NN are employed in PMB for the degradation prediction. Another two data-driven methods with probabilistic training, namely, restricted Boltzmann machine and deep belief network are applied in SMB as the backup of PMB to model non-stationary processes with the complicated underlying characteristics. Finally, the multiple regression forecasting is adopted in both blocks to check prediction accuracies. The effectiveness of our proposed two-stage maintenance framework is testified with extensive computation and experimental studies on an industrial case of the wafer fabrication plant in semiconductor manufactories, achieving up to 74.1% in testing accuracies for equipment degradation prediction. © 2015 Elsevier Ltd.
This paper describes the development and the actual utilization of fab-wide fault detection and classification (FDC) for the advanced semiconductor manufacturing using big data. In the fab-wide FDC, the collection of equipment's big data for the FDC judgment is required
While business analytics is being increasingly used to gain data-driven insights to support decision making, little research exists regarding the mechanism through which business analytics can be used to improve decision-making effectiveness (DME) at the organizational level. Drawing on the information processing view and contingency theory, this paper develops a research model linking business analytics to organizational DME. The research model is tested using structural equation modeling based on 740 responses collected from U.K. businesses. The key findings demonstrate that business analytics, through the mediation of a data-driven environment, positively influences information processing capability, which in turn has a positive effect on DME. The findings also demonstrate that the paths from business analytics to DME have no statistical differences between large and medium companies, but some differences between manufacturing and professional service industries. Our findings contribute to the business analytics literature by providing useful insights into business analytics applications and the facilitation of data-driven decision making. They also contribute to managers' knowledge and understanding by demonstrating how business analytics should be implemented to improve DME. © 1988-2012 IEEE.
Drastic changes in consumer markets over the last decades have increased the pressure and challenges for the trade exhibition industry. Exhibiting organizations demand higher levels of justification for involvement and expect returns on trade show investments. This study proposes an RFID-enabled track and traceability framework to improve information visibility at the trade site. The identification information can potentially create detailed, accurate, and complete visibility of attendees' movements and purchasing behaviors and consequently lead to considerable analytical benefits. Leveraging the wealth of information made available by RFID is challenging

IBM z Systems™ have been hosting critical business applications across several industries including banking, healthcare, insurance, and retail for over 50 years. With the recent growth in the use of analytics in the business environment, more customers are seeking to integrate analytics into their core operational environments to enable real-time insight on operational data for competitive advantage. IBM z Systems support very high-performance computation for business analytics. The z System processors are typically higher frequency than any general-purpose microprocessor, and with the zEnterprise® 196 (z196) system, huge gains in performance were made through out-of-order execution. The robust cache hierarchy also provides additional performance boosts to high concurrency analytics operations. The IBM z13™ extends these performance improvements with a new SIMD (single-instruction, multiple-data) engine in the processor for targeted acceleration of business analytics workloads. This SIMD engine supports new instructions for three data types: integer, string, and floating-point. We provide an overview of the z13 SIMD architecture and show how it advances current architectures. We also discuss the compiler and math library support for this new architecture and provide a few examples of applications that can be accelerated by this new hardware feature on the z13. © 1957-2012 IBM.
Big Data helps facilitate information visibility and process automation in design and manufacturing engineering. It also helps analyze trends through analytics and predict inventory, manufacturing output and equipment lifespan and cycles, etc. This paper introduces Big Data, its characteristics and a number of issues of Big Data in design and manufacturing engineering. These issues include design and manufacturing data, Big Data benefits and impacts and its applications and opportunities. Methods, technologies and some technology progress around Big Data are presented in this study. General challenges of Big Data and Big Data challenges in design and manufacturing engineering are also discussed. © 2015 Lidong Wang and Cheryl Ann Alexander.
The present study is focused on the thorough analysis of cause-effect relationships between pellet formulation characteristics (pellet composition as well as process parameters) and the selected quality attribute of the final product. The shape using the aspect ratio value expressed the quality of pellets. A data matrix for chemometric analysis consisted of 224 pellet formulations performed by means of eight different active pharmaceutical ingredients and several various excipients, using different extrusion/spheronization process conditions. The data set contained 14 input variables (both formulation and process variables) and one output variable (pellet aspect ratio). A tree regression algorithm consistent with the Quality by Design concept was applied to obtain deeper understanding and knowledge of formulation and process parameters affecting the final pellet sphericity. The clear interpretable set of decision rules were generated. The spehronization speed, spheronization time, number of holes and water content of extrudate have been recognized as the key factors influencing pellet aspect ratio. The most spherical pellets were achieved by using a large number of holes during extrusion, a high spheronizer speed and longer time of spheronization. The described data mining approach enhances knowledge about pelletization process and simultaneously facilitates searching for the optimal process conditions which are necessary to achieve ideal spherical pellets, resulting in good flow characteristics. This data mining approach can be taken into consideration by industrial formulation scientists to support rational decision making in the field of pellets technology. © 2015 Elsevier B.V. All rights reserved.
Business ecosystems consist of a heterogeneous and continuously evolving set of entities that are interconnected through a complex, global network of relationships. However, there is no well-established methodology to study the dynamics of this network. Traditional approaches have primarily utilized a single source of data of relatively established firms
The huge amount of information available and its heterogeneity has surpassed the capacity of current data management technologies. Dealing with huge amounts of structured and unstructured data, often referred as Big Data, is a hot research topic and a technological challenge. In this paper, the authors present an approach aimed to enable OLAP queries over different, heterogeneous, data sources. Their approach is based on a MapReduce paradigm, which integrates different formats into the recent RDF Data Cube format. The benefits of their approach are that it is capable of querying different sources of information, while maintaining at the same time, an integrated, comprehensive view of the data available. The paper discusses the advantages and disadvantages, as well as the implementation challenges that such approach presents. Furthermore, the approach is evaluated in detail by means of a case study. Copyright © 2015, IGI Global.

To realize the American Occupational Therapy Association's Centennial Vision, occupational therapy practitioners must embrace practices that are not only evidence based but also systematic, theoretically grounded, and driven by data related to outcomes. This article presents a framework, the Data-Driven Decision Making (DDDM) process, to guide clinicians' occupational therapy practice using systematic clinical reasoning with a focus on data. Examples are provided of DDDM in pediatrics and adult rehabilitation to guide practitioners in using data-driven practices to create evidence for occupational therapy. Copyright © 2015 by the American Occupational Therapy Association, Inc.
Orlando Health has brought its hospital and physician practice revenue cycle systems into better balance using four sets of customized analytics: Physician performance analytics gauge the total net revenue for every employed physician. Patient-pay analytics provide financial risk scores for all patients on both the hospital and physician practice sides. Revenue management analytics bridge the gap between the back-end central business office and front-end physician practice managers and administrators. Enterprise management analytics allow the hospitals and physician practices to share important information about common patients.

This paper addresses information processing weaknesses and limitations that can impede the effective use and analysis of Big Data in an audit environment. Drawing on the literature from psychology and auditing, we present the behavioral implications Big Data has on audit judgment by addressing the issues of information overload, information relevance, pattern recognition, and ambiguity. We also discuss the challenges that auditors encounter when incorporating Big Data in audit analyses and the various analytical tools that are currently used by companies in the analysis of Big Data. The manuscript concludes by raising questions that future research might address related to utilizing Big Data in auditing. © 2015 American Accounting Association. All rights reserved.
New data streams from social media, passive data capture and other sources are creating opportunities to support decision making. Also, data volume, data velocity and data variety continue to increase. Data-driven decision making using these new data streams, often call “big” data, is an important topic for continuing discussion and research. Given the costs of this data it is important to understand “big” data and any decision making use cases. Current use cases demonstrate how new data streams can support some operating decisions. Claims that new data streams can support strategic decision making by senior managers have not been demonstrated. Managers want better data and desire the “right” data at the “right time” and in the “right format” to support targeted decisions. This article explores the challenges of identifying novel use cases relevant to decision making, especially important, strategic long-term decisions. Analyzing “big data” to find a great business plan or to identify the next revolutionary product idea seems however like wishful thinking. Data is useful and we have more of it than ever before and the volume is increasing because data capture and storage is inexpensive. “Big data” and advanced analytics may provide facts for experienced and talented strategic decision makers, but those uses are not clearly defined. At present, the major strategic decision related to “big” data for senior managers is how much time, talent and money to allocate to capturing, storing and analyzing new data streams. Better defined decision making use cases can help senior managers assess the value of new data sources. © Springer International Publishing Switzerland 2015.
Effectively extracting reliable and trustworthy information from Big Data has become crucial for large business enterprises. Obtaining useful knowledge to enable better decisions to be made in order to improve business performance is not a trivial task. The most fundamental challenge for Big Data extraction is to handle the uncertainty of data to meet emerging business needs, such as marketing analysis, future prediction and decision making. In this paper, we firstly propose a novel approach called Dominating Top-k Aggregate Query (DA-Topk) to provide trustworthy and reliable informative knowledge from uncertain Big Data by combining the techniques of skyline and top-k queries. Then, we design a number of pruning rules to reduce the search space and terminate the ranking process as early as possible. Next, we provide a deeper analysis regarding the satisfaction of the six ranking properties (i.e. exact-k, containment, unique-rank, value-invariance, stability and faithfulness) between our approach and existing approaches to demonstrate that our method is the only one which satisfied all of these properties. Extensive experiments with both real and synthetic data sets have been conducted to verify the efficiency and effectiveness of our proposed approach compared to the state-of-the-art approaches. Our approach can help managers make strategic decisions quickly and accurately in competitive market places. © 2015 Elsevier Inc. All rights reserved.
Information systems have traditionally been constructed and used for increasing corporate value, developing competitive products, improving employee productivity, and accelerating the decision-making process. Today, however, dramatic improvements in hardware perfor-mance, advances in various types of sensors, and changes in the social environment such as consumer-driven dissemination of information via social networking services (SNSs) are gen-erating a great need for big data analysis. In its early stages, the use of big data depended on information and communications technology (ICT) specialists and expert analysts located at support sites. Now, to speed up decision-making at sites directly involved in increasing sales and lowering costs, analysis based on big data is being performed at these frontline sites, and some marketing and planning departments have already begun to base their actions on big data. Moreover, high-function middleware that facilitates such big data usage has been ap-pearing. However, issues arising in actual implementations of systems with such middleware have hindered advanced decision-making in the field. This paper introduces Fujitsu's approach to resolving these issues by providing integrated solutions that optimally combine big-data middleware and analysis scenarios and describes the effects of introducing these solutions.
The IPHealth project's main objective is to design and implement a platform with services that enable an integrated and intelligent access to related in the biomedical domain. We propose three usage scenarios: (i) assistance to healthcare professionals during the decision making process at clinical settings, (ii) access to relevant information about their health status and dependent chronic patients and (iii) to support evidence-based training of new medical students. Most effective techniques are proposed for reveral NLP tecniques and extraction of information from large data sets from sets of sensors and using open data. A Web application framework and an architecture that would enable integration of processes and techniques of text and data mining will be designed. Also, this architecture have to allow an integration of information in a fast, consistent and reusable (via plugins) way. Sensors, Personal Health © 2015 Sociedad Española para el Procesamiento del Lenguaje Natural Borrajo, L., Seara, A., Iglesias, E.L. 2015 TCBR-HMM: An HMM-ba se d text classifier with a CBR system, Applied Soft Computing, Volume 26, January2015, Pages 463-473.

The process we use to gather information in making decisions can be as important as the decisions themselves. Do you rely more on sophisticated analytics or intuition? Using a self-report exercise, this article assists the reader in recognizing their decision-making style and offers a framework to enhance the process. © 2015 Pepperdine University.
This paper is a preliminary study that investigates whether the Big Data technology contributes to making the strategic decision-making more objective. Decisions depend on the decision-makers´ knowledge and intuition, the characteristics of decision task as well as the quality of the data analysed. Focusing on the decision objective component, it appears that the contemporary business context is characterised by: (i) a vast amount of data, but not all of this is meaningful
Smart-Evac manages emergency evacuation systems after a natural disaster. Unlike existing disaster risk reduction (DRR) systems, Smart-Evac takes cloud computing and big data characteristics into consideration during decision making. The authors consider human anxiety to be a major contributor to network congestion immediately following a disaster. They envision tracing clusters of trapped peoples where network usage is comparatively high. The Smart-Evac system ranks these clusters based on volume, velocity, and variety (the 3Vs of big data). Based on this ranking, the system provides immediate evacuation service to the top-listed clusters to mitigate the exponential rise of network congestion. The system used the analytical hierarchy process (AHP) to achieve the proper ranking of the clusters to support decision makers efficiently. In addition, Smart-Evac provides immediate cloud-based basic healthcare facilities and ambulatory medical services to victims after successful evacuation. Immediate priority-based healthcare in turn reduces the haphazard rush to nearby hospitals. © 2015 IEEE.
This paper explores the applicability of flood impact databases in the flood risk governance process. This study begins with a twofold analysis of three hydrographical basins: one analysis based on the data of a recently constructed flood-impact database for Portugal and another based on selected socioeconomic and biophysical variables that characterize the basins' territorial context. From these sets of data, two fuzzy inference systems are assembled: one for the resource criteria and another for the time criteria. When plotted, the fuzzy analysis results are associated with distinct flood risk management strategies: operational and strategic, hard and soft measure-based. The three basins differ substantially in terms of flood-impact characteristics, with impacts being distinguished in terms of human and material consequences. Socioeconomic factors seem to be more explicative of flood impacts than the biophysical contexts that generate floods. The fuzzy logic analysis suggested priorities of action: early warning and information for one of the basins (Mondego) and a less operational solution, combining structural mitigation and land-use planning, for the other two basins (Lis and Vouga). Considering the current implementation of the Floods Directive, design of flood risk maps and flood risk management plans can benefit from the integration of the presented methodology. © 2015 by the authors.
This article discusses the implementation of local level education data-driven planning as implemented by the Office of the Senior Special Assistant to the President of Nigeria on the Millennium Development Goals (OSSAP-MDGs) in partnership with The Earth Institute, Columbia University. It focuses on the design and implementation of the Conditional Grants Scheme-Local Government Areas (CGS-LGA). CGS-LGA is an active federal policy programme in Nigeria aimed at transforming education policy into practice by providing technology and training to local, decision-makers to create local level plans based on data and then identifying the funding gaps for implementation of the plans. This article discusses the importance of the use of data in planning and in substantiating grant applications, as well as how CGS-LGA was introduced into Nigeria, and lessons learnt from the introduction of the program. A literature review provides information on the development of data-driven planning, and important components of such planning: development of data systems, importance of information to reform education practices, linking information through data to build accountability, and data-driven decision-making.
Introduction: Health systems globally and within the USA have introduced nursing home pay-for-performance (P4P) programmes in response to the need for improved nursing home quality. Central to the challenge of administering effective P4P is the availability of accurate, timely and clinically appropriate data for decision making. We aimed to explore ways in which data were collected, thought about and used as a result of participation in a P4P programme. Methods: Semistructured interviews were conducted with 232 nursing home employees from within 70 nursing homes that participated in P4P-sponsored quality improvement (QI) projects. Interview data were analysed to identify themes surrounding collecting, thinking about and using data for QI decision making. Results: The term 'data' appeared 247 times in the interviews, and over 92% of these instances (228/247) were spontaneous references by nursing home staff. Overall, 34% of respondents (79/232) referred directly to 'data' in their interviews. Nursing home leadership more frequently discussed data use than direct care staff. Emergent themes included using data to identify a QI problem, gathering data in new ways at the local level, and measuring outcomes in response to P4P participation. Alterations in data use as a result of policy change were theoretically consistent with the revised version of the Promoting Action on Research Implementation in Health Services framework, which posits that successful implementation is a function of evidence, context and facilitation. Conclusions: Providing a reimbursement context that facilitates the collection and use of reliable local evidence may be an important consideration to others contemplating the adaptation of P4P policies. © 2015, BMJ Publishing Group. All rights reserved.

To realize the American Occupational Therapy Association's Centennial Vision, occupational therapy practitionersmust embrace practices that are not only evidence based but also systematic, theoretically grounded, and driven by data related to outcomes. This article presents a framework, the Data-Driven Decision Making (DDDM) process, to guide clinicians' occupational therapy practice using systematic clinical reasoning with a focus on data. Examples are provided of DDDM in pediatrics and adult rehabilitation to guide practitioners in using data-driven practices to create evidence for occupational therapy. © 2015, American Occupational Therapy Association, Inc. All rights reserved.
With the increase in volume and the wide range of data needed to inform decisions at all levels of education system, federal and state ministries of education, districts and also schools have been demanding proactive and data-driven decision making (DDDM) frameworks, as an approach to enhance policy and decision making. In dealing with DDDM, higher education institutions (HEI), among schools, are found to be the most in need of such decision-support frameworks, because of the volume of the data they deal with, and the sensitivities of the decisions they make. To attend to this, though with an exclusive focus on the HEI’s students, we propose a DDDM conceptual framework, using a Design Research Method. The method includes critical literature review, survey instrument administration on the HEI policymakers, and interview sessions with data management personnel. Our proposed conceptual framework shows (1) how DDDM can achieve effective students’ data usage and improved students-related decision making, (2) HEI decision makers’ data usages, their preferred explicit knowledge from the data, and the relationship with the appropriate data variables. © 2015 American Scientific Publishers.
Data-driven decision-making (DDDM) is a difficult topic to cover, but typically required, in the applied educational psychology course or other courses required for teacher licensure in the United States. While a growing body of literature indicates in-service teachers are resistant to DDDM and underprepared to engage in it, little has been done to understand pre-service teachers while they are still in the ideal arena in which to address resistance and subsequently build DDDM skills. The purpose of this study was to examine pre-service teachers’ affective response to the classroom-level DDDM via their concerns profile (n = 78). Participants’ concerns profile revealed that much like in-service teacher literature suggests, this sample of pre-service teachers were resistant to learning more about DDDM, believed they knew of better innovations for use, and are unlikely to use DDDM in their future classrooms. The findings provide important insight for those of us tasked with covering this topic in our educational psychology courses. © 2016, © The Author(s) 2016.
As accountability systems have increased demands for evidence of student learning, the use of data in education has become more prevalent in many countries. Although school and administrative leaders are recognizing the need to provide support to teachers on how to interpret and respond to data, there is little theoretically sound research on data-driven decision making (DDDM) to guide their efforts. Drawing on sociocultural learning theory, extant empirical literature, and findings from a recent study, this paper develops a framework for understanding how to build teacher capacity to use data, specifically informing what practices administrators might employ, when in the DDDM process to employ these practices, and how these mechanisms may build teacher knowledge and skills. Given the global economic climate, administrators face difficult choices in how to invest scarce resources to support data use and once invested, how to ensure that teachers gain, and sustain, the needed capabilities once the supports are removed. The framework provided herein presents a set of concepts that may be useful in guiding these decisions. Implications for leadership practice, as well as suggestions to guide future research and theory development, are discussed. © The Author(s) 2014
The stock market is gaining in relevance with each day. Much research has been done in the area of finding a means to forecast the fluctuations. Yet, decision-making remains a challenging task. Taking risks or testing one's luck is a human trait prevalent among shareholders. They want to test their luck or take a risk by investing their last harvest. Misfortune can be avoided if some precaution is taken. This research hopes to alleviate one from that misery. It also hopes to glean a handsome profit from the stock market with the exception of natural disasters and other unforeseen incidents. Our proposed algorithm uses autoregressive methods to assist with the decision to buy as well as the selling point for any stock price. The proposed algorithm is useful for the shareholder and the trader. This decision-making tool can be essential to the formation of the business plan, and its viability is proven by the significant amount of profit that has already been yielded. © 2015 International Information Institute.
According to the problems such as low accuracy and long time when standard C4.5 algorithm in data mining to the table tennis match, an evaluation and decision model of table tennis match based on improved C4.5 algorithm is proposed. First introduce the optimization theory, improve the formula of amount of information by using the properties of convex function, to improve the accuracy of the classification in decision tree classifier. Then draw lessons from the thought of information gain, and optimize rules of candidate attributes to C4.5 algorithm by using the parameters support and credibility of association rule mining, and then optimize the operation time by the simplified calculation method of the division of information. Finally, apply the improved C4.5 algorithm to evaluation and decision of table tennis match, and perform the video data mining of table tennis game. Seen from the simulation results, compared to standard C4.5 algorithm, the proposed evaluation and decision model for the of table tennis match based on improved C4.5 algorithm, has better performance in data mining. © 2014 CAFET-INNOVA TECHNICAL SOCIETY. All rights reserved.
This paper builds on academic and industry discussions from the 2012 and 2013 pre-ICIS events: BI Congress III and the Special Interest Group on Decision Support Systems (SIGDSS) workshop, respectively. Recognizing the potential of “big data” to offer new insights for decision making and innovation, panelists at the two events discussed how organizations can use and manage big data for competitive advantage. In addition, expert panelists helped to identify research gaps. While emerging research in the academic community identifies some of the issues in acquiring, analyzing, and using big data, many of the new developments are occurring in the practitioner community. We bridge the gap between academic and practitioner research by presenting a big data analytics framework that depicts a process view of the components needed for big data analytics in organizations. Using practitioner interviews and literature from both academia and practice, we identify the current state of big data research guided by the framework and propose potential areas for future research to increase the relevance of academic research to practice. © 2015 by the Association for Information Systems.
Purpose – The purpose of this paper is to identify Big Data and Business Analytics (BDBA) skills and further propose an education and training framework for a successful career in BDBA. Design/methodology/approach – The present study adopts a review of extant literature and appreciative enquiry (AI) which is a quasi-ethnographic approach to identify the skills required for BDBA. Findings – The study helps to identify skills for BDBA and based on extant literature and AI, proposes a theoretical framework for education and training for a successful career in BDBA. Further research directions are outlined which can help take the present research to the next level. Research limitations/implications – The paper presents a theoretical framework, but it has to be validated through empirical data. This research will generate a lot of interest to develop a more practical framework and conduct empirical and case study research. Practical implications – The present study has outlined skills for BDBA. The authors have also proposed a theoretical framework which can further help an educational or training institute to embrace the framework to train young undergraduates or graduates to acquire BDBA skills. It may also motivate an institution to structure their curriculum for a BDBA program. Social implications – This research is a timely one to develop necessary skills for being successful in BDBA career and in turn contribute to the well-being of business community and society. Originality/value – This research is a novel one as there is no research done earlier on this new and emerging areas of research, namely, education and training for BDBA. © Emerald Group Publishing Limited.
Purpose – This paper aims to present a solution that enables organizations to monitor and analyse the performance of their business processes by means of Big Data technology. Business process improvement can drastically influence in the profit of corporations and helps them to remain viable. However, the use of traditional Business Intelligence systems is not sufficient to meet today’s business needs. They normally are business domain-specific and have not been sufficiently process-aware to support the needs of process improvement-type activities, especially on large and complex supply chains, where it entails integrating, monitoring and analysing a vast amount of dispersed event logs, with no structure, and produced on a variety of heterogeneous environments. This paper tackles this variability by devising different Big-Data-based approaches that aim to gain visibility into process performance. Design/methodology/approach – Authors present a cloud-based solution that leverages (BD) technology to provide essential insights into business process improvement. The proposed solution is aimed at measuring and improving overall business performance, especially in very large and complex cross-organisational business processes, where this type of visibility is hard to achieve across heterogeneous systems. Findings – Three different (BD) approaches have been undertaken based on Hadoop and HBase. We introduced first, a map-reduce approach that it is suitable for batch processing and presents a very high scalability. Secondly, we have described an alternative solution by integrating the proposed system with Impala. This approach has significant improvements in respect withmapreduce as it is focused on performing real-time queries over HBase. Finally, the use of secondary indexes has been also proposed with the aim of enabling immediate access to event instances for correlation in detriment of high duplication storage and synchronization issues. This approach has produced remarkable results in two real functional environments presented in the paper. Originality/value – The value of the contribution relies on the comparison and integration of software packages towards an integrated solution that is aimed to be adopted by industry. Apart from that, in this paper, authors illustrate the deployment of the architecture in two different settings. © 2015 Emerald Group Publishing Limited.
In the era of accelerating digitization and advanced big data analytics, harnessing quality data for designing and delivering state-of-the-art services will enable innovative business models and management approaches (Boyd and Crawford, 2012
New technologies are promising us many upsides like enhanced health, convenience, productivity, safety, and more useful data, information and knowledge for people and organizations. The potential downsides are challenges to personal privacy, over-hyped expectations, increasing technological complexity that boggles us. Our point is this change requires scientific discussion from the point of management, leadership and organizations – that is, it is time to discuss the meaning of these challenges seriously also in terms of existing traditions of management science. This review type article discusses the nature and role of the Internet of Things (IoT), Big Data and other key technological waves of ubiquitous revolution vis-á-vis the existing knowledge on management, organizations and knowledge management practices in organizations. Recent changes in the fields of robotics, artificial intelligence and automation technology indicate that all kinds of intelligence and smartness are increasing and organizational cultures are going to change indicating fast changes in the field of modern management and management sciences. Organizational processes form the base for the knowledge-based decision-making. Developing and utilizing smart solutions – like the utilization of Big Data – emphasize the importance of open system thinking. Digitalized services can for instance create new interfaces between service providers and users. Service users create social value while they are participating in co-producing activities. Hence, the IoT and Big Data undoubtedly strengthen the role of participation in service production, service economy, innovativeness in-between organizations (as a joint processes) and leadership models incorporated in service-dominant –logic. Moreover, IoT, Big Data, and especially digitalization bring about the renaissance of knowledge in decision-making. © Springer International Publishing Switzerland 2015.
Accountable Care Organizations (ACOs), a major component of the Affordable Care Act, seek to provide patients with better quality health care at a lower cost and have been praised for their ability to help repair our country’s broken health care system. Despite their potential benefits, however, ACOs also raise significant antitrust concerns—concerns that may pit consumer surplus and total surplus against one another. In an attempt to address these concerns, the Department of Justice and Fair Trade Commission announced that they will use market share screens and rule of reason treatment to evaluate ACOs participating in the Medicare Shared Savings Program. The use of market share screens and rule of reason treatment allows the antitrust agencies to avoid prioritizing either consumer surplus or total surplus in the first instance but leaves open two critical questions: What will the rule of reason treatment afforded to ACOs look like? And how will the antitrust agencies ultimately determine whether ACOs benefit or harm consumers? In order to address these questions, this Note proposes that the antitrust agencies use the “big data” collected under the Affordable Care Act to conduct a structured rule of reason review of ACOs that takes into account both the consumer surplus and total surplus through a burden-shifting framework. © 2015 by Shaun E. Werbelow.
The decades old manufacturing operations question of choosing the right answer in the face of disturbances created by certain unexpected events, or by their aggregation, could now be one step further to receive the right answer. A sensor-packed manufacturing system in which each process or piece of equipment makes available event and status information, coupled with market research for true advanced Big Data analytics, seem to be the right ingredients for event response selection, and thus moving manufacturing closer to the cloud manufacturing systems paradigm. Besides the inherent obvious advantages that come with the cloud manufacturing capabilities, the resulting manufacturing cyber-physical system will be also subjected to the known setbacks of the software and Internet-based systems, from which cyber-security needs to be addressed at the forefront. © Springer International Publishing Switzerland 2015.
To solve system integration and data management problems in the textile manufacturing process, through the textile manufacturing technology process, the massive data of each process is analyzed, and the inefficient convergence phenomenon of textile information between production planning layer and workshop manufacturing layer is studied. On the basis of the original system data, and text data of raw materials, sensor, yarn defect detection image data, and so on, a three layer textile big data storage system based on Hadoop is built. And then, through using theoretical methods of D-S evidence and incremental clustering, the technical difficulties that are multi-source textile data fusion is designed, the appropriate algorithms and models are proposed, and functions of the system are designed and implemented. Through the test, the results show that the system we designed have realizes the effective information link between planning layer and production layer by the correlation between data, solves the information island problem, and can provide a new method for real-time online detection of fabric quality in big data environment. ©, 2015, Journal of Mechanical Engineering.
This paper explores the support provided by big data systems developed in the cloud for empowering modern logistics services through fostering synergies among 3/4PL (third /fourth party logistics) in order to establish interoperable or highly integrated and sustainable logistics supply chain services. However, big data applications could have limited capabilities of providing performant logistics services without addressing the quality and accuracy of data. The main outcome of the paper is the definition of an architectural framework and associated research and development agenda for the application of cloud computing for the development and deployment of a Big Data Logistics Business Platform (BDLBP) for supply chain network management services. The capabilities embedded in the BDLBP can provide powerful decision support to logistics networking and stakeholders. Two of the three strategic and operational capabilities as operational capacity planning, and real-time route optimisation are built upon literature based on operational research, and are extended to the scope of dynamic and uncertain situations. The third capability, strategic logistics network planning is currently under researched and this approach aims at covering this capability supported by big data analytics in the cloud. © Springer International Publishing Switzerland 2015.
Big Data is a rapidly evolving and maturing field which places significant data storage and processing power at our disposal. To take advantage of this power, we need to create new means of collecting and processing large volumes of data at high speed. Meanwhile, as companies and organizations, such as health services, realize the importance and value of “joined-up thinking” across supply chains and healthcare pathways, for example, this creates a demand for a new type of approach to Business Activity Monitoring and Management. This new approach requires Big Data solutions to cope with the volume and speed of transactions across global supply chains. In this paper we describe a methodology and framework to leverage Big Data and Analytics to deliver a Decision Support framework to support Business Process Improvement, using near real-time process analytics in a decision-support environment. The system supports the capture and analysis of hierarchical process data, allowing analysis to take place at different organizational and process levels. Individual business units can perform their own process monitoring. An event-correlation mechanism is built into the system, allowing the monitoring of individual process instances or paths. © 2015, SciKA.
Recently, the computing environment is changing rapidly by a wide spread and the development of a wireless terminal. In particular, computing environment of enterprise is changing largely due to cloud computing and big data. Utilization value of big data is expanded across all sectors of society of administration, welfare transportation, health care, finance, manufacturing, etc. and the importance of big data as a driving force of the national economy is increased. In Korea, it is estimated that there is lack of professional staff, and a technology gap with developed countries although it is expected that demand of technology and professional staff will increase rapidly because big data is a source of future competitive advantage. In this paper, we identify concept and organization of big data business ecosystem and analyze the development step of the ecosystem. In particular, we investigated features by step of the big data business and analyzed systematically the development direction accordingly. The analyzed result in this paper is utilized to many companies and related policies of big data and will provide a number of advantages. © 2015 SERSC.
Governments around the world want to develop their ICT industries. Researchers and policymakers thus need a clear picture of digital businesses, but conventional datasets and typologies tend to lag real-world change. We use innovative 'big data' resources to perform an alternative analysis for all active companies in the UK, focusing on ICT-producing firms. Exploiting a combination of observed and modelled variables, we develop a novel 'sector-product' approach and use text mining to provide further detail on key sector-product cells. We find that the ICT production space is around 42% larger than SIC-based estimates, with around 70,000 more companies. We also find ICT employment shares over double the conventional estimates, although this result is more speculative. Our findings are robust to various scope, selection and sample construction challenges. We use our experiences to reflect on the broader pros and cons of frontier data use. © 2015 Elsevier B.V. All rights reserved.
The current work presents a new approach for designing business intelligence solutions. In the Era of Big Data, former and robust analytical concepts and utilities need to adapt themselves to the changed market circumstances. The main focus of this work is to address the acceleration of building process of a “data-centric” Business Intelligence (BI) solution besides preparing BI solutions for Big Data utilization. This research addresses the following goals: reducing the time spent during business intelligence solution’s design phase
The last years we faced a tremendous development of social media powered by innovative technologies related to web services, web 2.0 and social networks. Nowadays we are entering in to the next digitally enriched generation of social media and enabled by thrilling, currently under extensive development technologies, like Big Data, Cloud Computing, Virtual/Augmented Reality and Internet of Things. To our understanding the new generation of social media and the integrated business models that will support them are within a converging area where social features and ubiquitous technologies are met. Social elements, related to behavior, self-esteem, attachment and other psychological dimensions of personality will be transparently integrated to a number of technologies. Thus for the next years we have to wait for a number of new applications and systems, all targeted in a meta-existence level where the characteristics of human identity will be mixed with various digital identity elements. This basic trend and direction in the Social Media research in the next decade will boost the transparency of technologies and will stimulate an extremely different Web than the one we are exploiting together. The key dimension of the Social Media sphere will be the dependent social identification of humans by a number of technologies. © J.UCS.
This article proposes that a complementary relationship exists between the formalised nature of digital loyalty card data, and the informal nature of small business market orientation. A longitudinal, case-based research approach analysed this relationship in small firms given access to Tesco Clubcard data. The findings reveal a new-found structure and precision in small firm marketing planning from data exposure

Business intelligence (BI), “big data”, and analytics solutions are being deployed in an increasing number of organizations, yet recent predictions point to severe shortages in the number of graduates prepared to work in the area. New model curriculum is needed that can properly introduce BI and analytics topics into existing curriculum. That curriculum needs to incorporate current big data developments even as new dedicated analytics programs are becoming more prominent throughout the world. This paper contributes to the BI field by providing the first BI model curriculum guidelines. It focuses on adding appropriate elective courses to existing curriculum in order to foster the development of BI skills, knowledge, and experience for undergraduate majors, master of science in business information systems degree students, and MBAs. New curricula must achieve a delicate balance between a topic’s level of coverage that is appropriate to students’ level of expertise and background, and it must reflect industry workforce needs. Our approach to model curriculum development for business intelligence courses follows the structure of Krathwohl’s (2002) revised taxonomy, and we incorporated multi-level feedback from faculty and industry experts. Overall, this was a long-term effort that resulted in model curriculum guidelines. © 2015 by the Association for Information Systems.
Offering unprecedented levels of intelligence concerning the habits of consumers and rivals, big data promises to revolutionize the way enterprises are run. And yet, the concept of big data is one of the most poorly understood terms in business today. The implications for big data analytics are not as straightforward as they might seem-particularly when it comes to the so-called dark data from social media. Increases in the volume of data, the velocity with which they are generated and captured, and the variety of formats in which they are delivered all must be taken into account. To make the best use of the ever-burgeoning store of knowledge and insight at their fingertips, organizational leaders must confront two commonly held fallacies: that methodological issues no longer matter and that big data offers a complete and unbiased source of information on which to base their decisions. © 2015 Wiley Periodicals, Inc.
With the development of information technology, massive data resources with heterogeneous structure appear in the cyberspace, which is known as the network big data and has attracted extensive attentions. For mining the useful information from the network big data, it is required to efficiently organize the data resources in the cyberspace and realize the semantic-based similarity search. For an efficient data organization and search, we firstly need to extract the features/attributes of the big data to construct its high-dimensional semantic space, then define the data resources and queries as feature vectors or high-dimensional points in the semantic space, and finally can calculate the semantic similarity by the distance of high-dimensional points or the cosines of the angles between feature vectors. The multidimensional indexes can efficiently organize data resources in the semantic space, realizing the semantic-based similarity search. In addition, the dimensionality reduction technology can avoid the effects of “curse of dimensionality” when the dimensionality of the semantic space is too high. In this paper, the existing multidimensional indexes and dimensionality reduction technologies are reviewed systematically. Moreover, the existing semantic-based similarity search technologies using distributed technology are analyzed, and some suggestions about future research work are discussed. ©, 2014, Science Press. All right reserved.
In this paper, a market demand driven modeling framework is developed. The market demand is modeled as an end-of-line virtual machine based on which the market demand dissatisfaction (MDD) can be measured as production loss using event-based analysis. A general Markovian continuous-flow model is developed for market demand-driven systems with multistage production networks combining manual and automatic processes. Machine failure bottlenecks (MF-BNs) and machine capacity bottlenecks (MC-BNs) are defined and identified based on event-based indicators. A supervisory control algorithm is integrated in the framework to reduce MDD and improve system productivity through identification and mitigation of MF-BNs and MC-BNs. Simulation-based analysis will also be utilized, and case studies are performed to validate the effectiveness of the modeling framework and the supervisory control policies. © 2014 IEEE.

In the propylene polymerization process, the melt index (MI), as a critical quality variable in determining the product specification, cannot be measured in real time. What we already know is that MI is influenced by a large number of process variables, such as the process temperature, pressure, and level of liquid, and a large amount of their data are routinely recorded by the distributed control system. An alternative data-driven model was explored to online predict the MI, where the least squares support vector machine was responsible for establishing the complicated nonlinear relationship between the difficult-to-measure quality variable MI and those easy-to-measure process variables, whereas the independent component analysis and particle swarm optimization technique were structurally integrated into the model to tune the best values of the model parameters. Furthermore, an online correction strategy was specially devised to update the modeling data and adjust the model configuration parameters via adaptive behavior. The effectiveness of the designed data-driven approach was illustrated by the inference of the MI in a real polypropylene manufacturing plant, and we achieved a root mean square error of 0.0320 and a standard deviation of 0.0288 on the testing dataset. This proved the good prediction accuracy and validity of the proposed data-driven approach. © 2014 Wiley Periodicals, Inc.
Background/Objectives: This research study has been conducted in a scientific way to help manufacturing engineers and the management team to find out the hidden information from the data which are generated during the everyday manufacturing process. Methods/Statistical analysis: The methodology adopted in this activity is applying outlier analysis which is a data mining technique. The inter quartile range findings and analysis has been used here to find the hidden useful information from the process data with which a better insight could be established towards the improvement of quality of the product. The data used here have been collected from the automotive engine assembly and testing process. The study compares the results between the conventional and outlier analysis. Findings: The conventional style of checking and approving the engines based on the value pattern of Specific Fuel Consumption (SFC) which uses the design specification comparison with the actual data generally yields very minimal scope for the improvement of product quality in the perspective of design, Safety and reliability of the product because of the adherence of the same design specifications of the part drawings supplied by various suppliers. The competitive automotive manufacturing domain demands a different approach with which a better scope could be identified towards the improvement of product quality which is undoubtedly data mining. The outlier analysis using inter quartile range on the sample data of 500 engines revealed many important aspects where the improvement scope for quality has been identified as 15,000 Parts Per Million (PPM) against400 PPM of conventional quality analysis for the same data. Improvement/Application: This research is to offer an inclusive model hypothetically with actual data both for engineering and management people of manufacturing domain about the insights and benefits of employing data mining techniques towards the improvement of product quality with proven results.
In many modern manufacturing industries, data that characterize the manufacturing process are electronically collected and stored in databases. Due to advances in data collection systems and analysis tools, data mining (DM) has widely been applied for quality assessment (QA) in manufacturing industries. In DM, the choice of technique to be used in analyzing a dataset and assessing the quality depend on the understanding of the analyst. On the other hand, with the advent of improved and efficient prediction techniques, there is a need for an analyst to know which tool performs better for a particular type of dataset. Although a few review papers have recently been published to discuss DM applications in manufacturing for QA, this paper provides an extensive review to investigate the application of a special DM technique, namely support vector machine (SVM) to deal with QA problems. This review provides a comprehensive analysis of the literature from various points of view as DM concepts, data preprocessing, DM applications for each quality task, SVM preliminaries, and application results. Summary tables and figures are also provided besides to the analyses. Finally, conclusions and future research directions are provided. © 2015 EDP Sciences.
In semi-structured case-oriented business processes, the sequence of process steps is determined by case workers based on available document content associated with a case. Transitions between process execution steps are therefore case specific and depend on independent judgment of case workers. In this paper, we propose an instance-specific probabilistic process model (PPM) whose transition probabilities are customized to the semi-structured business process instance it represents. An instance-specific PPM serves as a powerful representation to predict the likelihood of different outcomes. We also show that certain instance-specific PPMs can be transformed into a Markov chain under some non-restrictive assumptions. For instance-specific PPMs that contain parallel execution of tasks, we provide an algorithm to map them to an extended space Markov chain. This way existing Markov techniques can be leveraged to make predictions about the likelihood of executing future tasks. Predictions provided by our technique could generate early alerts for case workers about the likelihood of important or undesired outcomes in an executing case instance. We have implemented and validated our approach on a simulated automobile insurance claims handling semi-structured business process. Results indicate that an instance-specific PPM provides more accurate predictions than other methods such as conditional probability. We also show that as more document data become available, the prediction accuracy of an instance-specific PPM increases. © 2013, Springer-Verlag London.

This paper presents the applications of dependency of attributes in information systems for data mining from business datasets. Firstly, we present the theoretical framework for data clustering on small business dataset. It is based on a construction of a hierarchical rough set approximation in an information system for data splitting. The hierarchy is defined by the notion of a nested sequence of indiscernibility relations that can be defined from the dependency of attributes. Secondly, an application of such hierarchy for mining maximal association from a business transactional data is presented. It is shown that the dependency provides clear and provable theoretical approach for data clustering and maximal association rules mining. © 2015 SERSC.

The quality of drugs is an important aspect for medication safety. With his knowledge in pharmaceutical manufacturing and quality control the hospital pharmacist has the ability to evaluate the quality of medicines with the help of analytical methods. Not only the analytical testing of raw material, formulations and bulk ware plays a major role but also compatibility and stability tests are of great importance. In this article the different applications of pharmaceutical analytics are being displayed using practical examples.
Manufacturers are using data obtained from sensors embedded in products to create innovative after-sales service offerings. Service offers companies significant opportunities to create and capture economic value. Firms are increasingly focusing on how they can deliver services that help their customers deliver value. Service providers are shifting from being ‘doers’ to becoming ‘problem solvers’, capable of orchestrating the delivery of complex services. ABC, a product manufacturing company is moving from a product manufacturer to a product-service system (PSS). However, the shift from a product to a PPS system is not trivial. This case study shows how the use of the product data analytics service (PDAS) model can help companies who are contemplating using Big Data to provide competitive services. Understanding what is happening in the shift to services is vital to the future success of all product companies. Keywords: Servitization, Data analytics, Big data, Product-service system (PSS) © Springer International Publishing Switzerland 2015.
Business intelligence (BI) combined with business analytics (BA) is an increasingly prominent strategic objective for many organizations. As a pedagogical subject, BI/BA is still in its infancy, and, in order for this to mature, we need to develop an undergraduate model BI/BA curriculum. BI/BA as an academic domain is emerging as a hybrid of disciplines, including information systems, statistics, management science, artificial intelligence, computer science, and business practice/theory. Based on IS 2010’s model curriculum constructs (Topi et al., 2010), we explore two curricular options: a BI/BA concentration in a typical IS major and a comprehensive, integrated BI/BA undergraduate major. In support, we present evidence of industry need for BI/BA, review the current state of BI/BA education, and compare anticipated requirements for BI/BA curricula with the IS 2010 model curriculum. For this initial phase of curricular design, we postulate a preliminary set of knowledge areas relevant for BI/BA pedagogy in a multidisciplinary framework. Then we discuss avenues for integrating these knowledge areas to develop professionally prepared BI/BA specializations at the undergraduate level. We also examine implications for both AACSB and ABET accreditation and describe the next phase of applying the IS 2010 concept structure to BI/BA curriculum development. © 2015 by the Association for Information Systems.
The telecom industry is phasing towards a business model which is more data centric, as the industry sees a decline in its legacy services. As a result of this shift, the legacy metrics and KPIs which were once used to scale and understand respectively the performance of the industry need to evolve as per needs of the new environment to provide a more accurate picture of the industry as it undergoes a business transformation. As a result of shift from voice to data there is a change in the business trends in the telecom industry which in turn necessitates the need for new Key Perfomance Indicators to measure the performance of the telecom industry. So the objective of the paper is to study the changes happening in the telecom industry due to the shift of voice from data and the effect of this shift on the telecom business trends. The paper attempts to come up with new key performance indicators which will help the telecom managers' scale the usage and revenue generated by data services more accurately.
Business analytics (BA) systems create value and provide competitive advantage for organisations. They involve technology and data infrastructure, BA capabilities and business processes that provide useful insights and support decision-making. To provide value and competitive advantage, BA capabilities should be valuable, rare and inimitable, and have organisational support (VRIO). In this paper, we develop and evaluate a prototype dashboard for the VRIO assessment of BA capabilities. The dashboard is intended to support the strategic management of BA capabilities. We discuss implications of the prototype dashboard for researchers and practitioners and suggest directions for future research. © 2015 Taylor & Francis.
In this paper, the data sources are published as diverse service types and provided to upper business applications. Based on these data services, a series of geovisual analytical modules are designed and implemented. These modules aim to provide a set of intuitivw, dynamic, and interactive tools for site selection, market analysis and forecasting, and trade area analysis. The applications of this system demonstrate that it can help users efficiently and conveniently explore, extract, and analyze spatiotemporal patterns from complex and high-dimensional business data, and provide valuable support for business decisions.
Creating industrial policy and programmes, especially in technology, is fraught with high levels of uncertainty. These programmes target the development of products that will not be sold for several years
Purpose – The purpose of this paper is to investigate the development of software pricing, following the advent of cloud-based business intelligence&analytics (BI&A) Software.Avalue-based conceptual software model is developed to ignite and structure further research. Design/methodology/approach – A two-step research approach is applied. In step one, the available literature is screened and evaluated, and this is followed by ten semi-structured expert interviews. With that input, a conceptual software pricing model is designed. In step two, this model is validated and refined through discussions with representatives of the five leading business intelligence suites. Findings – The paper sheds light on the value perception of customers and suggests a clear focus on the interaction between customers and vendors, and less on technical issues. The developed customer-centric, value-based pricing framework helps to improve pricing techniques and strategies. Research limitations/implications – The research is focused on the pricing strategy of software houses and excludes differentiations of technical specifications and functionalities. Practical implications – The research can support practitioners in the field of BI&A in rethinking their pricing methods. Placing the customer at center stage can lead to lower customer churn rates, higher customer satisfaction and more pricing flexibility. Originality/value – This empirical study reveals the importance of a customer-centric pricing approach in the specific case of BI&A. It can also be applied to other fast-developing sectors of the software industry. © Emerald Group Publishing Limited

Business analytics (BA) capabilities can potentially provide value and lead to better organisational performance. This paper develops a holistic, theoretically-grounded and practically relevant business analytics capability framework (BACF) that specifies, defines and ranks the capabilities that constitute an organisational BA initiative. The BACF was developed in two phases. First, an a priori conceptual framework was developed based on the Resource- Based View theory of the firm and a thematic content analysis of the BA literature. Second, the conceptual framework was further developed and refined using a three round Delphi study involving 16 BA experts. Changes from the Delphi study resulted in a refined and confirmed framework including detailed capability definitions, together with a ranking of the capabilities based on importance. The BACF will help academic researchers and industry practitioners to better understand the capabilities that constitute an organisational BA initiative and their relative importance. In future work, the capabilities in the BACF will be operationalised to measure their as-is status, thus enabling organisations to identify key areas of strength and weakness and prioritise future capability improvement efforts. © 2015 Cosic, Shanks & Maynard.
On a business to business (B2B) manufacturer website, web registration form is used to grant users access for web services. In this study, we have tried to identify quantitative difference in user registrations and factors for the difference using web analytics. We have observed and analysed web form in context and in its content perspectives. The analysis in context includes change in registration procedure and number of steps. Also, the analysis in web form content covers difference in questions being asked on the web forms. We have compared numbers of web user registration in pre- and post-web form modification periods. Then, we confirmed that change in context and content of web registration form increased rate of B2B user registration as a result. Copyright © 2015 Inderscience Enterprises Ltd.
Business intelligence and analytics (BIA) initiatives are costly, complex and experience high failure rates. Organizations require effective approaches to evaluate their BIA capabilities in order to develop strategies for their evolution. In this paper, we employ a design science paradigm to develop a comprehensive BIA effectiveness diagnostic (BIAED) framework that can be easily operationalized. We propose that a useful BIAED framework must assess the correct factors, should be deployed in the proper process context and acquire the appropriate input from different constituencies within an organization. Drawing on the BIAED framework, we further develop an online diagnostic toolkit that includes a comprehensive survey instrument. We subsequently deploy the diagnostic mechanism within three large organizations in North America (involving over 1500 participants) and use the results to inform BIA strategy formulation. Feedback from participating organizations indicates that the BIA diagnostic toolkit provides insights that are essential inputs to strategy development. This work addresses a significant research gap in the area of BIA effectiveness assessment. © 2015 Foshay, Yeoh, Boo, Ong & Mattie.
In recent years, supply chain performance measurement has received much attention from researchers and practitioners. Effective supply chain performance through supply chain antecedents such as business analytics has become a potentially valuable way of securing competitive advantage and improving supply chain performance. Many organizations have begun to recognize that supply chain management is the key to building a sustainable competitive advantage for their products or services in an increasingly crowded marketplace. Despite the fact that, determining supply chain performance through its antecedents is considered as a unique methodology, theoretical and comprehensive studies on supply chain performance through the methodology is few and far between. This study addressed the lack of the empirical studies by developing a comprehensive model to examine the effect of plan analyse, source analyse, make analyse and delivery analyse on supply chain performance considering moderating effect of WS on the relationships. A quantitative methodology using a cross-sectional survey method was used to investigate the relationship between variables. Data were collected from automotive companies in Iran. The overall response rate was 86.72%. The relationships between variables were examined using structural equation modelling (SEM) technique and partial least squares (PLS) software was used to examine the proposed hypotheses. The results revealed there is a significant and positive relationship between four areas of business analytics namely plan analyse, source analyse, make analyse and delivery analyse and supply chain performance. Moreover, the results indicated that the aforementioned relationship is moderated by WS. The study combined resource- based theory, resource dependence theories to develop a new theoretical framework to demonstrate the importance of the four areas of SCOR approach businesses analytic
Emergency logistics system is mainly composed of emergency command center, emergency logistics center, and emergency logistics information system. Among them, emergency logistics information system is a very important part of the emergency logistics throughout the whole emergency logistics rescue process, which is consisted of early-warning, reserve and distribution, monitoring, decision-making and evaluation six subsystems. This paper discussed the data mining technology applied to the construction process of emergency logistics information system, used data mining to find valuable potential information and provided it to each subsystem of emergency logistics information system, in order to support emergency decision-making.
Root cause detecting and rapid yield ramping for advanced technology nodes are crucial to maintain competitive advantages for semiconductor manufacturing. Since the data structure is increasingly complicated in a fully automated wafer fabrication facility, it is difficult to diagnose the whole production system for fault detection. A number of approaches have been proposed for fault diagnosis and root cause detection. However, many constraints in real settings restrict the usage of conventional approaches, due to the big data with complicated data structure. In particular, a batch may not be considered as a run in the present sub-batch processing system for wafer fabrication, in which the processing paths of the wafers in a batch could be different. Motivated by realistic needs, this paper aims to develop a root cause detection framework for the sub-batch processing system. Briefly, the proposed framework consists of three phases: data preparation, data dimension reduction, and the sub-batch processing model construction and evaluation. The proposed approach has been validated by a sequence of simulations and an empirical study conducted in a leading semiconductor manufacturing company in Taiwan. The results have shown practical viability of the proposed approach. Indeed, the developed approach is incorporated in the engineering data analysis system in this case company. © 1988-2012 IEEE.
ARECENT and high profile, forecast by IDC (International Data Corporation), predicts a $16 billion dollar industry for big data hardware, software and services in 2014. Growth in big data is expected to grow six times faster than general investments in information and computing technology. What could justify such an expense? Where is the business and decision-making value in this technology? What does such market growth say about the emerging use of big data in society? Let's attempt an overview of the variety of business cases made on behalf of big data. Perhaps after doing so we can gain clearer insight into who is using big data and how big data contributes value. An overview of the discussion suggests there are three distinct arguments, and a number of distinct variations within these arguments. We take a look at each of these arguments, treating each of these arguments as a thought experiment about the role of big data in society. Then we evaluate the respective merits of each argument in light of where actual expenditure seems to be occurring. © 1973-2011 IEEE.
This special issue turns the spotlight on business intelligence (BI) as an area of inquiry and explores beyond the current standard practices. The articles in the issue describe the processes that practitioners currently follow in this area and how new BI techniques and capabilities will help users understand and act on widely disparate types of data. The articles also report on how organizations make decisions around datasets, what purposes visualizations are used for, and what different representations people use to show and explore data. © 1981-2012 IEEE.
Richard R. Lee states that successful organizations require leadership that can instill big data and analytics competencies. A considerable amount of current conversation in the area of big data and analytics focuses on the virtues of solving all the challenges that organizations face when using this new paradigm in the business world. Senior executives and business managers need to make efforts to create the core competencies and to develop analytical insights that enable them to become big data and analytics leaders within their industries. Education, mentorship, and consultation with outside advisors should be implemented to gain the knowledge necessary to attain a leadership role. Big data and analytics success should be driven by the business and from the ranks of its senior executives and managers.
DB2 with BLU Acceleration on Power Systems may transform the way organizations run analytics. For law enforcement, real-time analysis of crime patterns can give police the ability to stop crime bef ore it happens. And in healthcare, big data and analytics allow providers to improve services and reduce costs. Innovations today are assisting these sectors and many others to help meet and quickly solve time-driven data challenges. In the world of big data and analytics, databases are workhorses used in analyzing data to generate insight. As the need f or speed and agility increases, many organizations are switching to databases that use in-memory processing. The POWER8 processor is designed for big data, and it is capable of running increased concurrent queries in parallel quickly and across multiple cores by utilizing more threads per core. DB2 with BLU acceleration is a next-generation database technology for in-memory computing. Organizations can analyze data rapidly and efficiently to uncover insights for helping grow revenue, reduce cost, and minimize risk.
Business analytics on IBM System z streamlines well-informed decision making to gain a competitive edge. A significant proportion of the data used for analytics originates on IBM® zEnterprise® System mainframe platforms. Running business analytics on zEnterprise System can build on the strengths of zEnterprise through easy consolidation, high availability, streamlined management, and simplified governance. In many organizations, the highly critical data can reside on this platform. The optimized on-board floating-point architecture of System z enables re-hosting a real-time analytics transactional scoring application on z/OS while helping avoid network latency delays and adding value to OLTP transactions.
Acute care facilities are experiencing fiscal challenges as noted by decreasing admissions and lower reimbursement creating an unsustainable fiscal environment as we move into the next era of health care. This situation necessitates a strategy to move away from acting solely on hunches and instinct to using analytics to become a truly data-driven organization that identifies opportunities within patient populations to improve the quality and efficiency of care across the continuum. A brief overview of knowledge management philosophies will be provided and how it is used to enable organizations to leverage data, information, and knowledge for operational transformation leading to improved outcomes. This article outlines the 5 key pillars of an Analytics Center of Excellence
This case study describes tensions that became apparent between community members and school administrators after a proposal to close a historically African American public high school in a large urban Southwestern city. When members of the city's longstanding African American community responded with outrage, the school district's senior administration backed away from their proposal to close the school, despite making what it felt was a neutral and technical-rational decision. However, the local community interpreted this move as the historical continuation of racist behaviors and policies that had been experienced by the community over a period of several decades. Critical race theory (CRT) allows for an analysis regarding the nature of these beliefs about race and indicates the need for school administrators to engage the realities of the community members they serve, rather than merely enacting technical-rational administrative behaviors that serve to continue regimes of marginalization and oppression. © The Author(s) 2013.
In an environment where communications service providers (CSPs) increasingly have the same service offers and devices, offering a superior customer experience is a priority to compete. Solutions that have the ability to highlight what really matters in driving customer satisfaction and deliver actionable insights from their wide-reaching customer, network, and service data are key differentiators for CSPs. This paper explores ways of integrating big data insights with automated and assisted processes related to key customer touchpoints to ultimately improve the customer experience. We show how innovation from Alcatel-Lucent and Bell Labs helps CSPs improve their business performance, using unique methodology designed to select the right key quality indicators, build accurate key business objective formula, predict customer behavior, and ultimately understand which factors are influencing the most. This can be used for example to improve the Net Promoter Score (NPS). The net result is a happier customer and a higher customer value. ©2014 Alcatel-Lucent.
The purpose of this study is to qualitatively identify the typologies and characteristics of the big data marketing strategy in major companies that are taking advantage of the big data business in Korea. Big data means piles accumulated from converging platforms such as computing infrastructures, smart devices, social networking and new media, and big data is also an analytic technique itself. Numerous enterprises have grown conscious that big data can be a most significant resource or capability since the issue of big data recently surfaced abruptly in Korea. Companies will be obliged to design their own implementing plans for big data marketing and to customize their own analytic skills in the new era of big data, which will fundamentally transform how businesses operate and how they engage with customers, suppliers, partners and employees. This research employed a Q-study, which is a methodology, model, and theory used in 'subjectivity' research to interpret professional panels' perceptions or opinions through in-depth interviews. This method includes a series of q-sorting analysis processes, proposing 40 stimuli statements (q-sample) compressed out of about 60 (q-population) and explaining the big data marketing model derived from in-depth interviews with 20 marketing managers who belong to major companies(q-sorters). As a result, this study makes fundamental contributions to proposing new findings and insights for small and medium-size enterprises (SMEs) and policy makers that need guidelines or direction for future big data business. © 2014 KSII.
Business analytics systems are seen by many to be a growing source of value and competitive advantage for businesses. However, it is not clear if increasingly advanced analytical capabilities create opportunities for radical change in business or just represent an incremental improvement to existing systems. What are the key questions that researchers should be focusing on to improve our understanding of analytics? And are Information Systems (IS) programs teaching students the right things to be successful in this environment? This panel at International Conference on Information Systems (ICIS) 2012 took stock of technological possibilities, practical experience and leading research to assess the current state and future direction of business analytics. In doing so, it brought together senior researchers and industry representatives to share the leading challenges, opportunities and good practice that they see. © 2014 by the Association for Information Systems.

In December 2012, the AIS Special Interest Group on Decision Support, Knowledge and Data Management Systems (SIGDSS) and the Teradata University Network (TUN) cosponsored the Business Intelligence Congress 3 and conducted surveys to assess academia's response to the growing market need for students with Business Intelligence (BI) and Business Analytics (BA) skill sets. This panel report describes the key findings and best practices that were identified, with an emphasis on what has changed since the BI Congress efforts in 2009 and 2010. The article also serves as a call to action for universities regarding the need to respond to emerging market needs in BI/BA, including Big Data. The IS field continues to be well positioned to be the leader in creating the next generation BI/BA workforce. To do so, we believe that IS leaders need to continuously refine BI/BA curriculum to keep pace with the turbulent BI/BA marketplace. © 2014 by the Association for Information Systems.
The development of information technologies has led to the era of big data
Cal Poly Pomona University Library launched its first mobile Web site in 2007. The continuous development of the library's mobile Web sites has mostly been informed by feedback from library staff rather than from student users. As mobile devices and tablets become more prevalent among college students, it is crucial to gain a better understanding of students’ mobile and tablet usage patterns so that more decisions are data-driven and based on user expectations. This article details the findings from a study assessing the usability of the library's Web site for tablets and mobile devices, which includes survey feedback and three rounds of usability testing. © 2014, Published with license by Taylor & Francis.
The Dutch government and School Inspectorate encourage schools to use the student performance data they can obtain from their student monitoring systems to maximize student performance in a systematic and goal-oriented way. Research by the same Inspectorate (Inspectie van het Onderwijs, 2010) shows that students in schools which do so outperform students in schools where data-driven decision making (DDDM) is as yet less developed. The University of Twente developed a training course in which school teams learn to utilize data from computerized student monitoring systems in order to improve instructional quality and student performance. Parallel to the training activities, training effects are studied. The research findings show that the training activities had a positive effect on school staff's DDDM knowledge and DDDM skills. Staff attitudes towards DDDM were already high on the pre-tests and remained high on the post-tests. © 2013 Elsevier Ltd.
This article describes the process and role frontline access and public service staff play in needs assessment and evaluation of user services, specifically in understanding the voice of the customer. Information includes how the University of Arizona Libraries have incorporated daily data collection into the strategic planning process, resources required (staff and technology), sources of data and methodology for data collection, tools used in the assistance of analysis, and sample outcomes implemented as a result of this process. Methods and approaches presented can be adopted or modified for use at other institutions. © 2014 Published with license by Taylor & Francis.
This study utilizes for the first time integrated knowledge-driven and data-driven methods for groundwater potential zoning in the hard-rock terrain of Ahar River catchment, Rajasthan, India by employing remote sensing, geographical information system, multi-criteria decision making (MCDM), and multiple linear regression (MLR) techniques. Thematic maps of the 11 hydrological/hydrogeological factors i.e., geomorphology, soil, topographic elevation, slope, drainage density, proximity to surface waterbodies, pre- and post-monsoon groundwater depths, net recharge, transmissivity, and land use/land cover, influencing the groundwater occurrence were used. The themes and their features were assigned suitable weights, which were normalized by the MCDM technique. Finally, the knowledge-driven groundwater potential map, generated by weighted linear combination, revealed that the good, moderate and poor groundwater potential zones are spread over 90.94 km2 (26 %), 135 km2 (39 %) and 122.36 km2 (35 %), respectively. Furthermore, the data-driven precise groundwater potential index (GPI) map was computed by MLR technique. The results of both the knowledge- and data-driven approaches were validated from the well yields of 18 sites and were found to be comparable to each other. Moreover, exogenous and endogenous factors affecting the good, moderate and poor groundwater potential were identified by applying principal component analysis. The results of the study are useful to water managers and decision makers for locating appropriate positions of new productive wells in the study area. The novel approach and findings of this study may also be used for developing policies for sustainable utilization of the groundwater resources in other hard-rock regions of the world. © 2014, Springer-Verlag Berlin Heidelberg.
Public health agencies face difficult decisions when allocating scarce resources to control the spread of HIV/AIDS. Decisions are often made with few local empirical data. We demonstrated the use of the robust decision making approach in Los Angeles County, an approach that is data driven and allows decision makers to compare the performance of various intervention strategies across thousands of simulated future scenarios. We found that the prevailing strategy of emphasizing behavioral risk reduction interventions was unlikely to achieve the policy goals of the national HIV/AIDS strategy. Of the alternative strategies we examined, those that invested most heavily in interventions to initiate antiretroviral treatment and support treatment adherence were the most likely to achieve policy objectives. By employing similar methods, other public health agencies can identify robust strategies and invest in interventions more likely to achieve HIV/AIDS policy goals © 2014 Project HOPE-The People-to-People Health Foundation, Inc.
Purpose-This paper discusses how educational policies have shaped the development of large-scale educational data and reviews current practices on the educational data use in selected states. Our purposes are to: (1) analyze the common practice and use of educational data in postsecondary education institutions and identify challenges as the educational crossroads
Data-driven decision making (D3M) has shown great promise in professional pursuits such as business and government. Here, policymakers collect and analyze data to make their operations more efficient and equitable. Progress in bringing the benefits of D3M to everyday life has been slow. For example, a student asks, If I pursue an undergraduate degree at this university, what are my expected lifetime earnings?. Presently there is no principled way to search for this, be-cause an accurate answer depends on the student and school. Such queries are personalized, winnowing down large datasets for specific circumstances, rather than applying welldefined predicates. They predict decision outcomes by extrapolating from relevant examples. This vision paper introduces a new approach to D3M that is designed to empower the individual to make informed choices. Here, we highlight research opportunities for the data management community arising from this proposal. © 2014 VLDB Endowment.
The former Head of Access Services of the University of Tennessee at Chattanooga Library reports on the recent attempt to restructure the interlibrary loan unit of the department. Reviewing workflows, data reports, technology use, and staff assignments precipitated staffing changes that allowed the Library to become more efficient and to reallocate librarian time to library areas in need of greater support due to changing workflows across the organization. The greatest challenges proved to be moving beyond unfounded perception (or anecdata) when making decisions, determining staff members' skill strengths and weaknesses, and changing established work habits. Short-term benefits included improved workflows, improved ability to handle absences and student worker scheduling, and reduced staff stress. © 2014 Published with license by Taylor & Francis.
In this paper, we first introduce some operations on interval-valued intuitionistic fuzzy sets, such as Einstein sum, Einstein product, Einstein exponentiation, etc., and further develop the induced interval-valued intuitionistic fuzzy Einstein ordered weighted average (I-IVIFEOWA) operator. We also establish some desirable properties of this operator, such as commutativity, idempotency and monotonicity. Then, we apply the induced interval-valued intuitionistic fuzzy Einstein ordered weighted average (I-IVIFEOWA) operator based on the data mining to deal with multiple attribute decision making under interval-valued intuitionistic fuzzy environments. Finally, an illustrative example about selecting an ERP system is given to verify the developed approach and to demonstrate its practicality and effectiveness. © 2014 - IOS Press and the authors. All rights reserved.
The recent leap advances in sensor and communication technologies made possible the Internet connectivity of the physical world: the Internet of Things, where not only documents and images are created, shared, or modified in the cyberspace, but also the physical resources interact over Internet and make decisions based on shared communication. The Big Data revolution has set the stage for the use of large data sets to predict the behaviour of consumers, organizations, and markets, taking into account the real-time outcomes of complex or unexpected events. Manufacturing can benefit from both these advances and move the manufacturing community closer towards the predictive manufacturing systems paradigm. Prediction in manufacturing operations could vary from simple resource failure prediction to more complex predictions of consumer behaviour and adaptation of manufacturing operations to address the expected changes in the business environment. © Springer International Publishing Switzerland 2015.
W. H. Inmon shares his views on the way that generates business value when looking at big data. There are many ways to look at big data and one such way is to look at the sheer volume of data. Another way to look at big data is by the method of its collection. The satellite looks at patterns of clouds and ocean currents and other measurements of the earth, such as oxygen levels in the ocean. There are a lot of records that are created by the analog computer. One place to start is by looking at clickstream data where there is some business value. The problem with such clickstream records is that there are a lot of them and finding those few records containing business value is not easy to do.
While many studies on big data analytics describe the data deluge and potential applications for such analytics, the required skill set for dealing with big data has not yet been studied empirically. The difference between big data (BD) and traditional business intelligence (BI) is also heavily discussed among practitioners and scholars. We conduct a latent semantic analysis (LSA) on job advertisements harvested from the online employment platform monster.com to extract information about the knowledge and skill requirements for BD and BI professionals. By analyzing and interpreting the statistical results of the LSA, we develop a competency taxonomy for big data and business intelligence. Our major findings are that (1) business knowledge is as important as technical skills for working successfully on BI and BD initiatives


In recent years, according to the exponential growth of data, Big data is the key issue for data application. Many companies and government agencies are trying to adopt Big Data technologies for finding a new way of problem solving. The Korean government concentrates effort to promote Big data market by funding R&D, disseminating service infrastructures, and preparing the legal system. To create new business opportunities by government driven strategy for Big data, the activities for ensuring interoperability should be continued in parallel. In this paper, we drew the potential business models by using the actors of Big data ecosystem. On this basis, we assigned the roles of government actors and, finally we drew the standardization requirements for supporting each role. The results of this study could be used for planning a road map of Big data standardization. © 2014 SERSC.
Pharma and BioPharma industries are aware of the impact of production processes on sustainability of business operations. To improve performance, companies have recognised that it is necessary to better understand the drivers of both costs and revenues and the actions that can be put in place to address them. In the past, commercial manufacturing emphasis was on full compliance with initially established product specifications leading to a perception of quality assurance based on testing, and avoiding later changes after regulatory submission. Although final product testing is an important element of quality control, final product quality can be measured but not modified, leading to product rejection or reprocessing activities and underperforming business outcomes, two kinds of waste according to lean manufacturing.
Huge amount of data becomes available from the pharmaceutical manufacturing process with wide application of industrial automatic control technology in traditional Chinese medicine (TCM) industry. The industrial big data thus provides golden opportunities to better understand the manufacturing process and improve the process perfonnanoe. Therefore it is important to implement data integration and management systems in TCM plants to easily collect, integrate, store, analyze, communicate and visulize the data with high efficiently. It could break the data island and discover useful information and knowledge to improve the manufacturing process performance. The key supjxnting technologies for TCM manufacturing and industrial big data management were introduced in this paper, with a specific focus on data mining and visualization technologies. Using historic data collected from a manufacturing plant of Shengmai injection of SZYY group, we illustrated the usefulness and discussed future prospects of data mining and visualization technologies.
It is difficult for manufacturers of printed circuit boards (PCBs) to remain competitive because of the ever-increasing complexity of circuit board designs and processes that increase the product cost while decreasing the product yield. It is particularly difficult to ensure high yields because the products are made through sequential nano-scale processes, and the quality of each process may be affected by the results of the upstream processes. This type of cumulative effect makes it difficult to determine the machines that introduce product faults. In this paper, we develop a data mining approach to which large amounts of trace data are inputted to infer fault-introducing machines in the form of a L ? R rule, where R contains the fault type and L contains a machine sequence that is the primary cause of the fault type. We tested our approach with industrial lot trace data collected from a PCB manufacturing line with 33 machines and six fault types. From the work-site experiment, we found 26 composite rules that showed a significant cumulative effect. The average fault detection accuracy of the rules was 87.2%. In addition, we found 13 rules that affected more than one fault type. © 2014 Korean Society for Precision Engineering and Springer-Verlag Berlin Heidelberg.
The Machining Intelligence Network (MachInNet) project tackles the challenges to unearthing manufacturing knowledge from NC codes (numerical control codes), tool layouts and other manufacturing documents, and to making it accessible for daily use, e.g., for feature-based NC planning. A new approach using data mining algorithms and semantic search technologies makes it possible to reverse engineer data from different sources and make it available for explicit use with the help of a semantic, Internet-based knowledge network. The business rationale of MachInNet is to help SMEs (small and medium-sized enterprises) to manage a large variety of technologies, to avoid redundant engineering efforts, and to accelerate industrial engineering of mechanical parts. © Springer International Publishing Switzerland 2014.
Family members and professionals in a Substance Abuse and Mental Health Services Administration Children's Mental Health Systems of Care Initiative in Houston, Texas conducted a participatory evaluation to examine wraparound implementation. Results guided systematic, theory-based program revisions. By focusing through empirically derived frameworks for implementation, the evaluation team identified and generated useful data sources to support and improve wraparound provision. Despite working with a more diverse population in which youth displayed more severe behaviors than in similar grants, after 18 months more families received service and outcomes improved as fidelity scores advanced above the national mean. © 2014 Copyright Taylor and Francis Group, LLC.
Purpose – This paper aims to trace the history, application areas and users of Classical Analytics and Big Data Analytics. Design/methodology/approach – The paper discusses different types of Classical and Big Data Analytical techniques and application areas from the early days to present day. Findings – Businesses can benefit from a deeper understanding of Classical and Big Data Analytics to make better and more informed decisions. Originality/value – This is a historical perspective from the early days of analytics to present day use of analytics. © Emerald Group Publishing Limited.
The juice manufacturing industry is a nascent industry. The increased pace of life and focus on a healthy life style has given rise to the industry. Orange Juice Company (OCJ), based in California, is one of the leading juice co-packers in the nation. However, despite their excellence in manufacturing, their technology adoption is behind that of their competition, especially in quality assurance. Because quality assurance plays an essential part in their businesses, they want to improve the processes by digitization and enable quality assurance analytics. They hope that by investing in quality assurance, they would be able to gain competitive advantage in juice manufacturing. Copyright © 2014, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.

Leveraging available technologies for high-throughput screening (HTS), to enable the rapid delivery of comprehensive data packages for drug discovery programs, is a primary goal in developing new molecular entities for clinical applications. Pharmaceutical companies like Bristol-Myers Squibb (BMS) must constantly evolve their assay methods to ensure an effective and timely impact to business. This article is focused on a novel three step approach, using Linguamatics I2E text analytics software to mine the full text of patents, to identify (1) kinase assay technology information, and (2) kinase group information that is associated with therapeutic areas for drug screening. © 2014 Elsevier Ltd.
Association rule mining is an important data mining method primarily used for market basket analysis. However, the method usually generates a large number of association rules
Social network plays a vital role in Chinese business and is highly valued by business people. However, social network analysis is difficult due to issues in data collection, natural language processing, social network detection and construction, relationship mining, etc. Thus, we develop the Corporate Leaders Analytics and Network System (CLANS) to tackle some of these problems. Our contributions are in three aspects: 1) we collect data from multiple sources and do the preprocessing to make it available to use
Virtual Radiologic (vRad), the largest teleradiology company in the United States, faces the difficult problem of matching more than 400 radiologists with time-varying and seasonal demand. In addition to the constraints that traditional medical facilities face, vRad is subject to supply and demand requirements that are unique to the teleradiology business environment. In this paper, we present a forecasting and capacity-planning model that more accurately assesses demand and plans system capacity to provide better service to vRad's customers. We discuss the underlying reasons for improvement and quantify the impact on vRad's entire system. We explain managerial insights that will help both vRad and other companies in the service sector with similar service-response requirements and demand patterns. We also highlight the implementation challenges our teams faced. © 2014 INFORMS.
While firms view services as the main source of their revenue and competitive advantage, understanding of service and service innovation is limited. This lack of understanding is especially significant in IT-Enabled Services (IESs) and IES innovation. Much work is needed to understand the contemporary trend of integrating diverse material and social resources to address complex organizational and individual needs. This article proposes a novel framework for IES and IES innovation and develops propositions and implications for research and practice. This work draws upon the tenet of complexity theory and conceptualizes IES as complex adaptive systems (CAS), with such properties and behaviors as diverse adaptive elements, nonlinear interaction, self-organization, and adaptive learning, and IES innovation as a co-evolutionary process of variation, selection, and retention (VSR). The proposed framework is illustrated using business analytics (BA) as a new kind of decision support service (DSS) throughout this paper. Several propositions are developed. Finally, we present a discussion and implications. © 2013 Elsevier B.V.
Customer activity and turnover is a critical component in measuring profitability and market performance. Understanding customer behavior is a vital in examining firms the marketplace. The purpose of this study is to examine of the use of marketing analytics to measure customer behavior in small business enterprises (SME). The study used three hypotheses to guide the direction of the research. Building on key theoretical concepts grounded in accounting, finance and marketing literature, this study used analytics to measure both customer behavior and firm behavior patterns. This study examined three significant marketing analytics: (a) customer behavior analytics (customer turnover/frequency
Interest in business analytics (BA) is currently popular. Professional consultancies and software houses are both touting it as the next wave in business, claiming that the need for BA skills is large and growing. Universities are beginning to respond by offering undergraduate majors and minors, Master of Science degrees, certificates, and concentrations within their Master of Business Administration programs. But what subjects are being covered in these programs? We surveyed some of the largest, most established, and best-known programs (predominantly in the United States, but some international) and interviewed representatives of these programs to better understand the requirements for students entering, the required and elective course topics covered, and job opportunities for graduates. In this article, we summarize our findings and provide some conclusions about analytics programs, including the current landscape, suggestions for development, and our vision for the future. We believe this report is useful to institutions that offer analytics programs, to those considering such offerings, and to the employers who are hiring analytics professionals. These employers need to better understand the skills that professionals are acquiring. Finally, it should help prospective students who seek to understand the analytics programs being offered to find the best match for their skills and interests. © 2014 INFORMS.

Synthesizing prior research, this paper designs a relatively comprehensive and holistic characterization of business analytics - one that serves as a foundation on which researchers, practitioners, and educators can base their studies of business analytics. As such, it serves as an initial ontology for business analytics as a field of study. The foundation has three main parts dealing with the whence and whither of business analytics: identification of dimensions along which business analytics possibilities can be examined, derivation of a six-class taxonomy that covers business analytics perspectives in the literature, and design of an inclusive framework for the field of business analytics. In addition to unifying the literature, a major contribution of the designed framework is that it can stimulate thinking about the nature, roles, and future of business analytics initiatives. We show how this is done by deducing a host of unresolved issues for consideration by researchers, practitioners, and educators. We find that business analytics involves issues quite aside from data management, number crunching, technology use, systematic reasoning, and so forth. © 2014 Elsevier B.V.
Mainland China is an emerging market for business intelligence (BI). However, Chinese senior manager's ingrained guanxi-based decision making practices are considered the largest obstacle of the BI system diffusion to local enterprises. In the current research, we perform a comparison between business intelligence-based decision making philosophies (i.e., business analytics) and guanxi-based decision making philosophies in four dimensions: 1) the key player dimension
The American healthcare system is at a crossroads, and analytics, as an organizational skill, figures to play a pivotal role in its future. As more healthcare systems capture information electronically and begin to collect more novel forms of data, such as human DNA, how will we leverage these resources and use them to improve human health at a manageable cost? In this article, we argue that analytics will play a fundamental role in the transformation of the American healthcare system. However, there are numerous challenges to the application and use of analytics: the lack of data standards, barriers to the collection of high-quality data, and a shortage of qualified personnel to conduct such analyses. There are also multiple managerial issues, such as how to get end users of electronic data to employ it consistently to improve healthcare delivery and how to manage the public reporting and sharing of data. In this article, we explore applications of analytics in healthcare, barriers and facilitators to its widespread adoption, and ways in which analytics can help us achieve the goals of the modern healthcare system: high-quality, responsive, affordable, and efficient care. © 2014 Kelley School of Business, Indiana University.
In this paper, we describe both applied and analytical work in collaboration with a large multistate gas utility. The project addressed a major operational resource allocation challenge that is typical to the industry. We study the resource allocation problem in which some of the tasks are scheduled and known in advance, and some are unpredictable and have to be addressed as they appear. The utility has maintenance crews that perform both standard jobs (each must be done before a specified deadline) as well as respond to emergency gas leaks (that occur randomly throughout the day and could disrupt the schedule and lead to significant overtime). The goal is to perform all the standard jobs by their respective deadlines, to address all emergency jobs in a timely manner, and to minimize maintenance crew overtime. We employ a novel decomposition approach that solves the problem in two phases. The first is a job scheduling phase, where standard jobs are scheduled over a time horizon. The second is a crew assignment phase, which solves a stochastic mixed integer program to assign jobs to maintenance crews under a stochastic number of future emergencies. For the first phase, we propose a heuristic based on the rounding of a linear programming relaxation formulation and prove an analytical worst-case performance guarantee. For the second phase, we propose an algorithm for assigning crews that is motivated by the structure of an optimal solution. We used our models and heuristics to develop a decision support tool that is being piloted in one of the utility's sites. Using the utility's data, we project that the tool will result in a 55% reduction in overtime hours. © 2014 INFORMS.
Data visualization offers librarians the ability to better manage, explore, and present information collected by various individuals throughout a library organization. This article discusses The Ohio State University Libraries experiments with Tableau, a sophisticated data visualization and rapid analytics software. Tableau allows librarians to blend and leverage data collected from a number of disparate sources, including transaction logs, Google Analytics, and e-resource usage reports. The article provides context for incorporating data visualization into the Ohio State University Libraries' assessment program and shares examples of visualizations created for two data analysis projects. The benefits of blending and simultaneously viewing visualizations of data from multiple sources are articulated and explored. The article concludes with a short discussion of potential future projects for visualizing library data using Tableau Desktop. © 2013 Copyright Taylor and Francis Group, LLC.
The objective of this paper is a) to provide a conceptual analysis of the term big data and b) to introduce linked data applications such as SKOS-based knowledge organization systems as new tools for the analysis, organization, representation, visualization and access to big data.
The direct-marketing business process applied in Slovenian publishing company was inefficient because of inadequate procedure used to create a list of potential customers for a marketing campaign. Considering the nature of the problem, data mining was selected to solve the problem. The company's direct marketing business process was renovated based on the CRISP-DM methodology and by using data mining for decision-making elements of the process. The paper represents a methodology enabling implementation of data mining into the business processes and its use in a direct marketing business process for a Slovenian publishing company.
The industry for business analytics within the BI sphere is growing significantly and the distinction in organizations between transactional information systems and decision-oriented systems breaks down. Firms need to understand both the opportunity and the potential of business analytics. Reporting, which is getting a handle on what happened in organizations, is complemented by analytics that is rather explanatory and predictive. Leveraging business analytics means to use analytics applications in order to analyse business problems and produce related business recommendations to improve business process performance. Business analytics must but be a part of a value creating process operating together with other systems and organisational factors in a synergistic manner, including people, processes, knowledge and relationship assets, culture, structure, and policies. In order for companies to be efficient, they need to automate processes, workflows and make rules. Effectiveness, on the other hand, is about making better decisions, perhaps using the same data that their competitors may have. What matters is not necessarily the technologies deployed, but emerging competence that the firm uses to support its business. A specific mindset needs to be installed for companies to invest into business analytics. Organisations need to better understand how best to exploit their data and convert them into information and sense-making capabilities. Business capabilities can be enhanced not only by exploitation of analytical tools, but also by the sophisticated use of information. This leads to a truly sense-making capability or analytical mindset. The primary data covers 398 data sets, where firms have been asked about the specifics of their information management. The data is used as input to statistical tests and the value of business analytics is being analyzed in an empirical way.
In recent times, business analytics and big data have gained momentumboth in industry practice and academic research. The objective of this paper is to provide both a research and teaching introduction to business analytics in the context of both current and prospective perspective of the business analytics domain. It begins by providing a quick overviewof the three types of analytics. To assist the future analytics professionals, we identify, group and discuss nine different participants of the analytics industry into clusters. We then include a brief description of some current research projects under way in our team. We also note some research opportunities in Big Data analytics. The paper also concludes with a discussion of teaching opportunities in analytics.
Continuous improvement of business processes is a challenging task that requires complex and robust supporting systems. Using advanced analytics methods and emerging technologies - such as business intelligence systems, business activity monitoring, predictive analytics, behavioral pattern recognition, and 'type simulations' - can help business users continuously improve their processes. However, the high volumes of event data produced by the execution of processes during the business lifetime prevent business users from efficiently accessing timely analytics data. This article presents a technological solution using a big data approach to provide business analysts with visibility on distributed process and business performance. The proposed architecture lets users analyze business performance in highly distributed environments with a short time response. This article is part of a special issue on leveraging big data and business analytics. © 1999-2012 IEEE.
Objective - In an environment of shrinking budgets and reduced staffing, this study seeks to identify a comprehensive, integrated assessment strategy to better focus diminished resources within special collections repositories. Methods - This article presents the results of a single case study conducted in the Special and Digital Collections department at a university library. The department created an holistic assessment model, taking into account both public and technical services, to explore inter-related questions affecting both day-to-day operations as well as long-term, strategic priorities. Results - Data from a variety of assessment activities positively impacted the department's practices, informing decisions made about staff skill sets, training, and scheduling
Business process reengineering (BPR) can help organisations to identify and improve their business processes. A major problem is the high volume of business process datasets with characteristics such as high dimensionality, noise, uncertainty in process datasets and complicated interactions among process variables. Data mining (DM) techniques facilitate the identification and analysis of business processes, and improve their performance by extracting the hidden knowledge in business process datasets. In this paper, we present the application of DM to BPR, based on two novel approaches. By a literature review, the first approach proposes DMbBPR model, mainly focuses on the applications of data mining to each BPR phase. The second approach presents a novel combinational model based on the knowledge management cycle and CRISP-DM process in the framework of process monitoring architecture. To achieve better results, both approaches should be considered simultaneously in order to effectively identify, analyse, and improve business processes. Copyright © 2013 Inderscience Enterprises Ltd.
The globalization of the world's economies is a major challenge to local industry and it is pushing the manufacturing sector to its next transformation - predictive manufacturing. In order to become more competitive, manufacturers need to embrace emerging technologies, such as advanced analytics and cyber-physical system-based approaches, to improve their efficiency and productivity. With an aggressive push towards Internet of Things, data has become more accessible and ubiquitous, contributing to the big data environment. This phenomenon necessitates the right approach and tools to convert data into useful, actionable information. © 2013 Society of Manufacturing Engineers (SME).
In this paper, we present a sequence analysis method, which is one of the advanced data mining techniques, to identify and extract unique patterns from wafer manufacturing data. Wafer fabrication in the semiconductor industry is one of the most complex manufacturing processes. For such highly complicated operations, maintaining high yields through the statistical process control as a sole monitoring method for quality control is obviously inefficient. We thus investigate the intelligent and semi-automatic technique to help industrial engineers analyzing their production data. Our proposed method has the ability to induce patterns that can reveal and differentiate low performance processes from the normal ones. We also provide program coding of the proposed sequence analysis method, implemented with the R language, for easy experimental repetition.
PURPOSE: To reveal hidden patterns and knowledge present in nursing care information documented with standardized nursing terminologies on end-of-life (EOL) hospitalized patients. METHOD: 596 episodes of care that included pain as a problem on a patient's care plan were examined using statistical and data mining tools. The data were extracted from the Hands-On Automated Nursing Data System database of nursing care plan episodes (n=40,747) coded with NANDA-I, Nursing Outcomes Classification, and Nursing Intervention Classification (NNN) terminologies. System episode data (episode=care plans updated at every hand-off on a patient while staying on a hospital unit) had been previously gathered in eight units located in four different healthcare facilities (total episodes=40,747
Big business has embraced big data with enthusiasm. It is a love-fest
The foundational concept of Network Enabled Capability relies on effective, timely information sharing. This information is used in analysis, trade and scenario studies, and ultimately decision making. In this paper, the concept of visual analytics is explored as an enabler to facilitate rapid, defensible and superior decision making. By coupling analytical reasoning with the exceptional human capability to rapidly internalize and understand visual data, visual analytics allows individual and collaborative decision making to occur in the face of vast and disparate data, time pressures and uncertainty. An example visual analytics framework is presented in the form of a decision-making environment centered on the Lockheed C-5A and C-5M aircraft. This environment allows rapid trade studies to be conducted on design, logistics and capability within the aircraft's operational roles. Through this example, the use of a visual analytics decision-making environment within a military environment is demonstrated. © 2011 The Society for Modeling and Simulation International.
This article describes the Financial Industry Business Ontology (FIBO) as a set of formal models that define unambiguous shared meaning for financial industry concepts. An account is given of the history and development of the FIBO series of standards and the theoretical underpinnings of these as a business or 'conceptual' model. Some initial proof of concept work is described, demonstrating how in addition to the use of FIBO as a conceptual model, it is possible to derive semantic technology-based applications that may be used to carry out novel types of processing on data. The development roadmap of the FIBO series of standards within the Object Management Group is also described, so that readers can have an idea of what to expect from FIBO and when. © 2013 Macmillan Publishers Ltd.
CIOs need to maximize the value from the significant investment their organizations make in business analytics (BA) initiatives. We explore two themes for maximizing BA value-speed to insight and pervasive use, and present a BA case study at GUESS? INC., a fashion retailer. We provide recommendations for how IT leaders can maximize value from their BA investments. © 2013 University of Minnesota.
All social networks involve unstructured data of various sorts especially user posts, page impressions, and clicks. Within any customer-facing social business infrastructure, businesses can leverage the power of big data to drive several applications. Social media analytics leverage advanced analytics tools, reporting, dashboarding, visualization, search, predictions, text mining, and so on to find patterns of awareness, sentiment, and propensity among current and potential customers, as surfaced up from social media such as Twitter and Facebook. Graph analysis mines transactions, interactions, and other behavioral information that may be sourced from social media, and/or just as often from CRM, billing, and other internal systems. Where customer engagement is concerned, big data infrastructure can drive targeted recommendations, offers, conversations, and experiences throughout social business channels.
Cost models of manufacturing processes are an important tool enabling enterprises to make reasonable predictions and forecasts in relation to the production costs for existing and new products. Accurate and robust cost models can help to provide significant competitive advantage for manufacturing organisations. Advanced computational methods such as virtual manufacturing and data mining have been identified as potentially powerful techniques for generating cost models that bypass the problems associated with traditional cost modelling processes. Part I, of this two-part paper, described the development of a cost model development methodology that makes use of virtual manufacturing models and data mining techniques and used case study data to validate this methodology. A critical part of this methodology is the selection and use of effective data analysis techniques that can identify accurate and robust cost estimating relationships. Part II now examines in detail the effectiveness of alternative data mining algorithms in terms of their ability to develop relationships that are (1) representative of the real causal relationships that exist and (2) able to provide a high level of estimating accuracy. More specifically, it focuses on the data generated by virtual manufacturing models and how the size and complexity of the generated data sets impact the accuracy of the cost estimating relationships. © 2012 Springer-Verlag London Limited.
Big data provides a powerful resource for defining your organization's value to one's customers. A successful innovation needs a powerful marketer and an organization that can invest in the creation of that innovation. This strategy works in most cases. In cases where the model does not flourish, the innovation itself does not last long. Platforms designed and built for handling scalability problems for search engines and social media platforms now provide the computing and storage platform for creating the enterprise compute and processing platform for large and multi-format, multi-structured datasets. The extensibility of this platform into the enterprise data repository is the game-changer for many enterprises. The biggest transformation that can be brought to bear is the overall approach of the business to its prospects or customers.
The ageing problems are much worse in the developed countries, so qualities of life are now critical issues faced by many countries with ageing population. A woman's body type changes significantly with age, especially during elderly age. Elderly aged women are a significant customer group because of their interest in apparel matched by an increase in financial abilities. However, most apparel currently marketed is produced based on the younger women, and does not reflect the elderly aged body types. Standard elderly aged size charts are crucial standards for apparel industries. However, these industries suffering from production often find it hard to obtain the elderly aged standards. This study aims to fill the gap by proposing an ANN-based data mining framework to generate useful patterns for developing standard size charts. By conducting an empirical study, the results can provide apparel industries with standard size charts for women to improve manufacturing competitiveness. © 2013 Copyright Taylor and Francis Group, LLC.
This paper reports on research, the aim of which has been to investigate the use of virtual manufacturing and data mining techniques to automate the identification of manufacturing process time estimating relationships that form the basis of product and process cost models. Such models provide information that is critical to all stages of the product development process and to ensuring that development of cost-effective product design and production methods. Use of virtual manufacturing models and data mining techniques enables the majority of the activities involved in the cost model development process to be automated. Hence, reducing the time and effort required, reducing the current high level of reliance on expert judgment and enabling higher levels of cost detail to be estimated. The research reported, therefore, focuses on the use of virtual manufacturing to generate datasets which are then analysed using data mining to identify suitable cost models for manufacturing processes. Part I, of this two-part paper, describes the development of a model development methodology that makes use of virtual manufacturing models and data mining techniques and uses case study data to validate this methodology. Part II will then examine in detail the effectiveness of alternative data mining algorithms in terms of their ability to develop relationships that are (1) representative of the real causal relationships that exist and (2) provide a high level of estimating accuracy. © 2012 Springer-Verlag London Limited.
Intel and IBM collaboration delivers a quantum leap forward with 25x faster performance in IBM DB2 with BLU Acceleration. It uses a revolutionary combination of columnar data store, memory optimization, and hardware exploitation to deliver a quantum leap in performance for analytics workloads. Analytics users can even ask a business question and draw on both row and columnar data at the same time to get the answer. The experience of Intel and IBM engineers with big scientific and research databases also confirms that columnar data is much easier to compress. DB2 with BLU Acceleration uses advanced encoding for extreme compression. That translates to faster analytics, because it means more data can fit into main memory on the database server, where the server's processors can access it quickly. Reliability, availability, and serviceability (RAS) features built into Intel Xeon processors help ensure the database server is highly available as well as high performing, and provide advanced data integrity for DB2 with BLU Acceleration databases.

Business Intelligence (BI) has become indispensible to modern business decision-making. Organizations rely on BI to interpret the mass amounts of data circulating throughout the world. However, integration of BI into university business programs does not parallel industry demands. The purpose of this paper is to introduce an innovative business intelligence project tutorial for Information Systems (IS) education. The applied tutorial was designed to help students learn how to design and publish a report using SQL Server Reporting Services to analyze current stock market data. This tutorial exposes students to the decision-making power derived from raw data analysis and assists in development of business professionals who can maximize profitability through effective use of business intelligence. © 2013 The Clute Institute.

Data-driven decision-making (DDDM) reform has proven to be an effective means for improving student learning. However, little DDDM reform has happened at the classroom level, and little research has explored variables that influence teacher adoption of DDDM. The authors propose a model in which teachers sense of efficacy for the skills that support classroom-level DDDM and DDDM anxiety significantly influenced teachers DDDM efficacy, which then influenced collaboration concerns that influenced refocusing concerns. The authors used structural equation modeling to analyze data on 537 teachers in order to validate this hypothesized model. Results supported this model and are discussed. © 2013 Copyright Taylor and Francis Group, LLC.
Classroom level data driven decision-making (DDDM) involves the use of data to identify patterns of performance that reveal students' academic strengths and weaknesses relative to established learning goals, and the planning of instructional practices to support academic success for all students. Although DDDM is not a new paradigm in education, little is known about what variables facilitate teacher adoption of DDDM practices. The aim of this work was to introduce two such variables, DDDM efficacy and DDDM anxiety, and a measure of these constructs, the DDDM efficacy and anxiety (3D-MEA) inventory. The 1728 participants in this study were K-12 teachers who had experienced varying levels of DDDM professional development in a Pacific Northwestern state. Exploratory factor analysis (EFA) (n= 864) and confirmatory factor analysis (CFA) (n= 864) were utilized to evaluate the psychometric properties of the 3D-MEA Inventory. Results supported a five-factor model. A discussion of the results and their implications ensue. © 2012 Elsevier Inc.
Global-scale meteorological observations and scientific research development has brought meteorological big data management and analysis bottlenecks. Aiming at this problem, this study focuses on the organization and management of meteorological sensor network collected big data in cloud computing environment based on the analysis of big data characteristics, application technology status and the main features and application requirements of meteorological sensor network data. And a meteorological sensor network big data organization and management framework based on the storage- Calculation integrated cluster environment is proposed. Particularly and focuses on the meteorological sensor network collected big data model in the cluster environment is put forward. The above framework and data model are put into practice in the meteorological data management and service system of Zhejiang province. © 2013 Asian Network for Scientific Information.
The quality of a product is dependent on both facilities/equipment and manufacturing processes. Any error or disorder in facilities and processes can cause a catastrophic failure. To avoid such failures, a zero- defect manufacturing (ZDM) system is necessary in order to increase the reliability and safety of manufacturing systems and reach zero-defect quality of products. One of the major challenges for ZDM is the analysis of massive raw datasets. This type of analysis needs an automated and self-organized decision making system. Data mining (DM) is an effective methodology for discovering interesting knowledge within a huge datasets. It plays an important role in developing a ZDM system. The paper presents a general framework of ZDM and explains how to apply DM approaches to manufacture the products with zero-defect. This paper also discusses 3 ongoing projects demonstrating the practice of using DM approaches for reaching the goal of ZDM. © 2013 Shanghai University and Springer-Verlag Berlin Heidelberg.
Nowadays, organisations aim to automate their business processes to improve operational efficiency, reduce costs, improve the quality of customer service and reduce the probability of human error. Business process intelligence aims to apply data warehousing, data analysis and data mining techniques to process execution data, thus enabling the analysis, interpretation, and optimisation of business processes. Data mining approaches are especially effective in helping us to extract insights into customer behaviour, habits, potential needs and desires, credit associated risks, fraudulent transactions and etc. However, the integration of data mining into business processes still requires a lot of coordination and manual adjustment. This paper aims at reducing this effort by reusing successful data mining solutions. We propose an approach for implementation of data mining into a business process. The confirmation of the suggested approach is based on the results achieved in eight commercial companies, covering different industries, such as telecommunications, banking and retail. © 2013 Copyright Vilnius Gediminas Technical University (VGTU) Press Technika.
Social networks have an outstanding marketing value and developing data mining methods for viral marketing is a hot topic in the research community. However, most social networks remain impossible to be fully analyzed and understood due to prohibiting sizes and the incapability of traditional machine learning and data mining approaches to deal with the new dimension in the learning process related to the large-scale environment where the data are produced. On one hand, the birth and evolution of such networks has posed outstanding challenges for the learning and mining community, and on the other has opened the possibility for very powerful business applications. However, little understanding exists regarding these business applications and the potential of social network mining to boost marketing. This paper presents a review of the most important state-of-the-art approaches in the machine learning and data mining community regarding analysis of social networks and their business applications. The authors review the problems related to social networks and describe the recent developments in the area discussing important achievements in the analysis of social networks and outlining future work. The focus of the review in not only on the technical aspects of the learning and mining approaches applied to social networks but also on the business potentials of such methods. Copyright © 2013, IGI Global.
Purpose: Increased business competition requires even more rapid and sophisticated information and data analysis. These requirements challenge performance management to effectively support the decision making process. Business analytics is an emerging field that can potentially extend the domain of performance management to provide an improved understanding of business dynamics and lead to a better decision making. The purpose of this positional paper is to introduce performance management analytics as a potential extension of performance management research and practice. The paper clarifies the possible application areas of business analytics and their advantages within the context of performance management. Design/methodology/approach: The paper employs a literature based analysis and from this a conceptual argument is established. Finally, a business analytical model is presented to be used to undertake future research. Findings: The paper clarifies the possible application areas of business analytics and their advantages within the context of organizational performance management. Originality/value: The main implication is that the paper provides evidence of the use of business analytics for understanding organizational performance. Several insights are provided for management accounting research and education. © Emerald Group Publishing Limited.

In recent years data mining has become a very popular technique for extracting information from the database in different areas due to its flexibility of working on any kind of databases and also due to the surprising results. This paper is an attempt to introduce application of data mining techniques in the manufacturing industry to which least importance has been given. A taste of implement-able areas in manufacturing enterprises is discussed with a proposed architecture, which can be applied to an individual enterprise as well as to an extended enterprise to get benefit of data mining technique and to share the discovered knowledge among enterprises. The paper proposes conceptual methods for better use of different data mining techniques in product manufacturing life cycle. These techniques include statistical techniques, neural networks, decision trees and genetic algorithms. An integrated and unified data mining platform is anticipated then to improve overall manufacturing process.

Previous research has demonstrated that business analytics systems create value and provide competitive advantage for organisations. We argue that organisational and dynamic capabilities, enabled by business analytics systems, can explain how value is created, sustained and renewed. An evolutionary, process-oriented theoretical framework describes how dynamic and organisational capabilities interact over time to create value. The framework is then used to explain how business analytics systems provided value and competitive advantage in a longitudinal case study of a large financial institution. Several critical success factors for the use of business analytics are identified and implications are discussed. © 2012 Taylor & Francis.
Business is running ever faster-generating, collecting and using increas-ing volumes of data about every aspect of the interactions between sup-pliers, manufacturers, retailers and customers. Within these mountains of data are seams of gold-patterns of behavior that can be interpreted, classified and analyzed to allow predictions of real value. Which treat-ment is likely to be most effective for this patient? What can we offer that this particular customer is more likely to buy? Can we identify if that transaction is fraudulent before the sale is closed? To these questions and more, operational analytics-the combination of deep data analysis and transaction processing systems-has an answer. This paper describes what operational analytics is and what it offers to the business. We explore its relationship to business intelligence (BI) and see how traditional data warehouse architectures struggle to support it. Now, the combination of advanced hardware and software technologies provide the opportunity to create a new integrated platform delivering powerful operational analytics within the existing IT fabric of the enterprise. With the IBM DBAnalytics Accelerator, a new hardware/software offer-ing on System z, the power of the massively parallel processing (MPP) IBM Netezza is closely integrated with the mainframe and accessed directly and transparently via DBon z/OS. The IBM DB2 Analytics Accelerator brings enormous query performance gains to analytic queries and enables direct integration with operational processes. This integrated environment also enables distributed data marts to be re-turned to the mainframe environment, enabling significant reductions in data management and total ownership costs. © 2012, 9sight Consulting, all rights reserved.
This document is an example of the type of report an organisation would receive at the end of a HP Security Analytics engagement. The focus is on the analysis of the security risks and performance of the organisation's Security Incident Management Processes and related Security Operation Centre (SOC)'s activities. HP Labs carried out the underlying R&D work in collaboration with HP Enterprise Security Services (HP ESS) and involved analysis of processes, probabilistic modeling, simulation and what-if analysis for some of HP's key customers. The outcome of this was a set of case studies from which we have been able to create this more general anonymised report illustrating the richness of the risk assessment and what-if analysis that has been carried out. The lifecycle management of security is critical for organisations to protect their key assets, ensure a correct security posture and deal with emerging risks and threats. It involves various steps, usually carried out on an ongoing, regular basis, including: risk assessment
The article maps data governance policies to a simple process to monitor oil field sensor data. Oil and gas companies install sensors on facilities as well as the seabed to monitor production, the state of the facility, health and safety, and adherence to environmental regulations. Companies may also install sensors on the seabed to monitor environmental conditions such as flow, temperature, and turbidity. Turbidity is a measurement of water quality based on the cloudiness of water caused by individual particles that might not be visible to the naked eye. Oil and gas companies also create dashboards to monitor energy production across facilities. Oil and gas companies create common operations centers so they can monitor production from a central location. Environmental sensors may be in operation before, during, and after the operating life of the platform.
The expectation that educators will use data in the service of school improvement and planning is a major feature of national and local reform agendas. Prior research has found that the principal plays a critical role in making policymakers' visions for data use a reality at the school and classroom levels. Most prior studies, however, have not fleshed out how the principal functions as a key agent in influencing other key players in data use. This article will illustrate the actions of the principal, teachers, students, and district personnel through simulation models of principal leadership that we developed based on a case study of a high school implementing this reform. We use these models both as a framework for understanding our findings and as a way to enhance understanding of the processes by which educational reform is co-constructed through the simultaneous mediation of the multiple agents involved in the system. © 2012 Copyright Taylor and Francis Group, LLC.
This chapter focuses on various algorithms and techniques in video analytics that can be applied to the business intelligence domain. The goal is to provide the reader with an overview of the state of the art approaches in the field of video analytics, and also describe the various applications where these technologies can be applied. We describe existing algorithms for extraction and processing of target and scene information, multi-sensor cross camera analysis, inferencing of simple, complex and abnormal video events, data mining, image search and retrieval, intuitive UIs for efficient customer experience, and text summarization of visual data. We have also presented the evaluation results of each of these technology components using in-house and other publicly available datasets. © 2012 Springer Berlin Heidelberg.
To increase productivity, companies are in search of techniques that enable them to make faster and more effective decisions. Data mining and fuzzy clustering algorithms can serve for this purpose. This paper models the decision making process of a ceramics production company using a fuzzy clustering algorithm and data mining. Factors that affect the quality of slurry are measured over time. Using this data, a fuzzy clustering algorithm assigns the degrees of memberships of the slurry for the different quality clusters. An expert can decide on acceptance or rejection of slurry based on calculated degrees of memberships. In addition, by using data mining techniques we generated some rules that provide the optimum conditions for acceptance of the slurry.
Data-driven decision making has become an essential component of educational practice across all levels, from chief state school officers to classroom teachers, and has received unprecedented attention in terms of policy and financial support. It was included as one of the four pillars in the American Recovery and Reinvestment Act (2009), indicating that federal education officials seek to ensure that data and evidence are used to inform policy and practice. This article describes the emergence of data-driven decision making as a topic of interest, some of the challenges to and opportunities for data use, and how the principles of educational psychology can and must be used to inform how educators are using data and the examination of its impact on educational practice. © 2012 Copyright Taylor and Francis Group, LLC.
Predictive analytics are driving many healthcare organizations' hiring decisions in today's competitive market. It's important for organizations to capture the right data to help optimize their recruitment strategies and eliminate the time and money holes spent on untargeted recruitment campaigns.
The technological evolution in the last 20 years has steadily increased the amount of data in digital form, sensed from the environment, produced by private companies or collected from individuals' activities. Communications becoming more and more pervasive make these data available to any application, as if they were in a single database. The real value comes from information and services derived from data correlation, tailored to specific interest. We claim that the availability of a Living Open Data Framework enables the exploitation of data through services, created by many parties, in several contexts. However, data correlation is fraught with perils, from the obvious of privacy breaching to more subtle ownership and value protection. These issues are of fundamental importance and have to be addressed to create a viable business based on data and require advances in technology and in the regulatory framework. This is what is being pursued by the Italian EIT ICT Labs Trento Node through the cooperation of several parties, including the Autonomous Province of Trento, Telecom Italia, and several universities and SMEs. The paper aims at reporting the directions and the results so far obtained.
Textual databases are useful sources of information and knowledge and if these are well utilised then issues related to future project management and product or service quality improvement may be resolved. A large part of corporate information, approximately 80%, is available in textual data formats. Text Classification techniques are well known for managing on-line sources of digital documents. The identification of key issues discussed within textual data and their classification into two different classes could help decision makers or knowledge workers to manage their future activities better. This research is relevant for most text based documents and is demonstrated on Post Project Reviews (PPRs) which are valuable source of information and knowledge. The application of textual data mining techniques for discovering useful knowledge and classifying textual data into different classes is a relatively new area of research. The research work presented in this paper is focused on the use of hybrid applications of text mining or textual data mining techniques to classify textual data into two different classes. The research applies clustering techniques at the first stage and Apriori Association Rule Mining at the second stage. The Apriori Association Rule of Mining is applied to generate Multiple Key Term Phrasal Knowledge Sequences (MKTPKS) which are later used for classification. Additionally, studies were made to improve the classification accuracies of the classifiers i.e. C4.5, K-NN, Naïve Bayes and Support Vector Machines (SVMs). The classification accuracies were measured and the results compared with those of a single term based classification model. The methodology proposed could be used to analyse any free formatted textual data and in the current research it has been demonstrated on an industrial dataset consisting of Post Project Reviews (PPRs) collected from the construction industry. The data or information available in these reviews is codified in multiple different formats but in the current research scenario only free formatted text documents are examined. Experiments showed that the performance of classifiers improved through adopting the proposed methodology. © 2011 Elsevier Ltd. All rights reserved.
This work discusses scripts for processing molecular simulations data written using the software package R: A Language and Environment for Statistical Computing. These scripts, named moleculaRnetworks, are intended for the geometric and solvent network analysis of aqueous solutes and can be extended to other H-bonded solvents. New algorithms, several of which are based on graph theory, that interrogate the solvent environment about a solute are presented and described. This includes a novel method for identifying the geometric shape adopted by the solvent in the immediate vicinity of the solute and an exploratory approach for describing H-bonding, both based on the PageRank algorithm of Google search fame. The moleculaRnetworks codes include a preprocessor, which distills simulation trajectories into physicochemical data arrays, and an interactive analysis script that enables statistical, trend, and correlation analysis, and other data mining. The goal of these scripts is to increase access to the wealth of structural and dynamical information that can be obtained from molecular simulations. Copyright © 2012 Wiley Periodicals, Inc.
The rapidly increasing penetration and use of the Internet, in conjunction with the explosion of various technologies based on it, gave rise to the development of numerous e-services, such as e-business, e-banking, e-government and e-learning ones. The websites providing these e-services collect large amounts of users' activity and evaluation data. It is necessary to transform these data into useful business analytics that allow a better understanding of the strengths and weaknesses of the e-service, the various types of value it generates, and its whole value generation mechanism, and at the same time provide guidance for its improvement and optimization. We propose and validate a methodology for transforming user evaluation data into useful business analytics, founded on the technology acceptance model, the IS success model and e-services. We define a three-layer value model for e-services, including concerning its efficiency, effectiveness and impact on users' future behavior respectively. This value model is used for collecting and processing service evaluation data from users. We calculated two classes of business analytics: average users' ratings to identify strengths and weaknesses of e-services
Due to the rapid development of information technologies, abundant data have become readily available. Data mining techniques have been used for process optimization in many manufacturing processes in automotive, LCD, semiconductor, and steel production, among others. However, a large amount of missing values occurs in the data set due to several causes (e.g.
Data mining tools are nowadays becoming more and more popular in the semiconductor manufacturing industry, and especially in yield-oriented enhancement techniques. This is because conventional approaches fail to extract hidden relationships between numerous complex process control parameters. In order to highlight correlations between such parameters, we propose in this paper a complete knowledge discovery in databases (KDD) model. The mining heart of the model uses a new method derived from association rules programming, and is based on two concepts: decision correlation rules and contingency vectors. The first concept results from a cross fertilization between correlation and decision rules. It enables relevant links to be highlighted between sets of values of a relation and the values of sets of targets belonging to the same relation. Decision correlation rules are built on the twofold basis of the chi-squared measure and of the support of the extracted values. Due to the very nature of the problem, levelwise algorithms only allow extraction of results with long execution times and huge memory occupation. To offset these two problems, we propose an algorithm based both on the lectic order and contingency vectors, an alternate representation of contingency tables. This algorithm is the basis of our KDD model software, called MineCor. An overall presentation of its other functions, of some significant experimental results, and of associated performances are provided and discussed. © 2011 IEEE.
Visual analytics (VA)the fusion of analytical reasoning and computational data analysis with rich, interactive visual representationspromises to provide many relevant techniques for business-ecosystem-intelligence systems. However, the effectiveness of such systems requires the careful curation of complex, heterogeneous, and distributed data
This empirical study employs a keyword analysis and text mining technique using the Provalis mixed methods research suite to examine the following research question, What is the impact of cultural context on decision-making processes in global virtual teams (GVTs) of transnational civil society? Our approach to cultural context is primarily driven by Hall's (1976) theory of high vs. low context dimensions and Hofstede's (1980, 2001, 2005) five-dimensional framework for analysing culture. Our four-stage conceptual model of decision-making draws on Zakaria (2006), Kingdon (1995), Adler (2002), and Güss (2002). We further explore this model of culture and decision-making with data from a four-year public e-mail archive (2002-2005) of the GVT of transnational civil society involved in UN World Summit on Information Society (WSIS). From a methodological perspective, we find that for our explicit cultural variables (gender and region), keyword content analysis and data mining techniques are powerful tools to unlock massive datasets. In contrast, for our implicit cultural variables (high-context and low-context communication styles and cultural values), these techniques were not yet as helpful. © 2012 Inderscience Enterprises Ltd.
Objectives The aim of this study was to assess the applicability of knowledge discovery in database methodology, based upon data mining techniques, to the investigation of lung cancer surgery. Methods According to CRISP 1.0 methodology, a data mining (DM) project was developed on a data warehouse containing records for 501 patients operated on for lung cancer with curative intention. The modelling technique was logistic regression. Results The fi nally selected model presented the following values: sensitivity 9.68%, specifi city 100%, global precision 94.02%, positive predictive value 100% and negative predictive value 93.98% for a cut-off point set at 0.5. A receiver operating characteristic (ROC) curve was constructed. The area under the curve (CI 95%) was 0.817 (0.740- 0.893) (p<0.05). Statistical association with perioperative mortality was found for the following variables [odds ratio (CI 95%)]: age over 70 [2.3822 (1.0338-5.4891)], heart disease [2.4875 (1.0089-6.1334)], peripheral arterial disease [5.7705 (1.9296-17.2570)], pneumonectomy [3.6199 (1.4939-8.7715)] and length of surgery (min) [1.0067 (1.0008-1.0126)]. Conclusions The CRISP-DM process model is very suitable for lung cancer surgery analysis, improving decision making as well as knowledge and quality management.
Business intelligence and analytics (BI&A) has emerged as an important area of study for both practitioners and researchers, reflecting the magnitude and impact of data-related problems to be solved in contemporary business organizations. This introduction to the MIS Quarterly Special Issue on Business Intelligence Research first provides a framework that identifies the evolution, applications, and emerging research areas of BI&A. BI&A 1.0, BI&A 2.0, and BI&A 3.0 are defined and described in terms of their key characteristics and capabilities. Current research in BI&A is analyzed and challenges and opportunities associated with BI&A research and education are identified. We also report a bibliometric study of critical BI&A publications, researchers, and research topics based on more than a decade of related academic and industry publications. Finally, the six articles that comprise this special issue are introduced and characterized in terms of the proposed BI&A research framework.
As states approach the funding cliff marking the end of federal stimulus help for education, school districts will be feeling more financial pain than they're experiencing now. But there's good news amid the bad: Big city districts are showing schools nationwide a way to save money and improve efficiency by working together. They've created the Key Performance Indicator (KPI) system, which allows each of the nation's largest urban school districts to compare the performance of business operations, finance, human resources, and information technology across all 65 of the largest urban school districts in the nation. As a result, districts are saving millions of dollars.
Existing literature supports the inclusion of students in education reform, documenting benefits for both students and educators. When student voice is not included in reform efforts, these efforts are more likely to flounder. The emerging educational reform of data-driven decision making (DDDM) offers promise for increasing student achievement. However, scant research documents the involvement of students in DDDM reforms. Using a theoretical framework that advocates for democratically involving students in education reform, this cross-case analysis examines the role of students in DDDM reforms in elementary and high schools known to be exemplars of data-driven decision making. Based on findings of efforts made by exemplar districts as well as actions they did not take to involve students, the authors conclude that a new typology is necessary for assessing student involvement in DDDM. Consequently, the authors propose a new three-tiered typology for conceptualizing this phenomenon. © 2011 SAGE Publications.
Management practices and information technologies to handle knowledge of satellite manufacturing organizations may prove to be complex. As such knowledge (with its explicit and tacit constituents) is assumed to be one of the main variables whilst a distinguishing factor of such organizations
In the different phase of the business cycle, companies may face different financial crises with different financial attributes. In this paper we take the phases of the business cycle into consideration and use data mining as a technique to predict firms that may face a potential financial crisis. Since we hypothesize that financial crises are closely related to the business cycle, we have determined which periods show expansionary or recessionary trends. The objective of this paper is to determine important variables affecting financial crises and use this information to improve the accuracy of financial predictions. The electronic industry has been selected as the main focus, as this is a very important industry in Taiwan. Five experimental models have been designed for empirical study and an optimal model has also been established. From the results, we have discovered some important financial variables that can cause business's financial crises in different phases of the business cycle. In considering the business cycle, the model achieved better predictive accuracy and a support vector model has a higher predictive accuracy than other data mining techniques.
During the last decades, the disciplines of Data Mining and Operations Research have been working mostly independent of each other. However, the increasing complexity of today's applications in areas such as business, medicine, and science requires more and more interaction between both disciplines. On the one hand, several data mining algorithms are based on optimization methods. On the other hand, in several applications the pure Knowledge Discovery in Databases (KDD) process is not sufficient since it does not take explicitly into account the entire decision process. This report presents future trends in Business Analytics and Optimization discussed at the panel sessions during the workshop on Business Analytics and Optimization (BAO'2010). © 2011 - IOS Press and the authors. All rights reserved.
In the semiconductor manufacturing process, fault detection is a major step of process control aiming at constructing a decision tool to help detecting as quickly as possible any equipment or process faults in order to maintain high process yields in manufacturing. Traditional statistical based techniques such as univariate and multivariate analyses have long been employed as a tool for creating model to detect faults. Unfortunately, modern semiconductor industries have the ability to produce measurement data collected directly from sensors during the production process and such highly voluminous data are beyond the capability of traditional process control method to detect fault in a timely manner. We thus propose the techniques based on the data mining technology to automatically generate an accurate model to predict faults during the wafer fabrication process of the semiconductor industries. In such process control context, the measurement data contain over 500 signals or features. The feature selection technique is therefore a necessary tool to extract the most potential features. Besides the feature selection method, we also propose an over-sampling technique to handle the highly imbalance situation of fail versus pass test cases. Such imbalance cases refer to rare class prediction in the data mining context. The experimental results support our assumption that choosing the right features and over-sampling rare cases can considerably improve detection accuracy of fault products and processes.

Regarding the exponential growth of information throughout the world, the companies are always engaged with a great deal of digital information growing. One of the most important challenges for data mining is finding the right and quick relation between data. Apriori algorithm is the most popular technique for discovering the repetitious patterns. Nevertheless, when we are using this technique, a database should be scanned numerously to calculate many sets of chosen items. Parallel and distributed calculations are among the effective strategies to accelerate the data mining process. In this paper, parallel distributed Apriori algorithm has been introduced as a solution for the problem. In this method, only one scan from database is needed. The procedure considers one factor from a collection of items. Therefore, production of the equilibrial work volume among the processors and reduction of the idle processor time is mentioned. In order to show the work of the proposed technique, it is compared with some other parallel algorithms of data mining. Finally using structural procedures, credibility and stability of the algorithm is studied.
Purpose: Over the past few years, developments in business analytics have provided strategic planners with promising instruments for dealing with turbulent environments. This study aims to reveal whether or not the application of business analytics in strategic planning contributes to better company performance, and to formulate recommendations on how to integrate business analytics in companies' performance management systems. Design/methodology/approach: Based on a survey conducted with 89 respondents from high-technology firms, a group comparison between firms with strong performance and those with weak performance reveals significant differences between the two groups' strategic planning processes and application of business analytics. Findings: The empirical survey's results show that better-performing companies are characterized by a more sophisticated analytical planning process. Lower-performing firms acknowledge this competitive advantage. Based on these findings, the authors develop recommendations on how to integrate business analytics in performance management contexts. Research limitations: The empirical study's results are limited to high-technology industries in the cultural setting of Germany. Practical implications: The empirical results emphasize the competitive advantage gained by applying business analytics. The recommendations concerning analytical performance management should help managers to sensibly integrate the analytical toolbox in performance management contexts. Originality/value: This paper combines insights on the best usage of business analytics from the perspective of strategic planning experts, with recommendations for the integration of business analytics into the performance management framework from an academic perspective. © Emerald Group Publishing Limited.
Today's business applications demand high flexibility in processing information and extracting knowledge from data. Thus, data mining becomes more and more an integral part of operating a business. However, the integration of data mining into business processes still requires a lot of coordination and manual adjustment. This paper aims at reducing this effort by reusing successful data mining solutions. We describe a novel approach to facilitating the integration based on process patterns for data mining and demonstrate that these patterns allow for easy reuse and can significantly speed up the process of integration. We empirically evaluate our approach in a case study of fraud detection in the healthcare domain. Copyright © 2011 Inderscience Enterprises Ltd.

Baniya merchants of the Mughal Empire, burgher merchants of the Swedish Empire, and chonin merchants of the Tokugawa Shogunate had the same questions on their mind as businesspeople do today. To which townspeople should I sell my wares? Of folks that buy from me, are there any that might stop buying from me? Which groups buy which goods? Which saris should I show Ranna Devi to make as much money as I can? How much timber will people want in the coming weeks and months?. © 2011 IEEE.
In this paper, we combine the characteristics of data mining technology and the characteristics of the university enrollment data, and use Self Organizing Map (SOM) neural network, association rule and Fayyad data mining model to establish a university admissions decision-making model. First, SOM is applied to the cluster analysis of china's 31 provinces, and the feature and level of each province can be obtained, which is used for making sub-province enrollment scheme and sub-province enrollment propaganda
The customer relationship management (CRM) has helped organisation to increase customer satisfaction level and understand the customer behaviour in much better ways. Organisations need to find innovative ideas to deal with customers and understand them better. This requires that they should be able to uncover hidden and unknown information from customer database using analytical tools like data mining (DM). This application of DM in CRM is explored by analysing the opinion of the employees and executives of service sector in India. The application of DM in CRM is being found through the questionnaires which are being framed by the authors. These questionnaires helped us to frame and test certain hypothesis which will help us to find the real existing situations of service sector in Indian service sector. We hope that this would help to create a clear picture of the importance of DM in CRM of service sector. © 2011 Inderscience Enterprises Ltd.
Survey results from the 'Best Practices in Asset Management and Reliability Study', conducted by Virginia Tech., aimed at identifying how organizations are managing their equipment assets, are presented. The study found that diagnostic information generated in the field by intelligent instruments and condition monitors are capable of obtaining critical information on plant assets to raise alarms and determine that repairs can be delayed. The study also correlate a lack of information availability and use with lower performance and increased reactive maintenance levels. An APM homepage can be customized for each user, providing a quick view of the current state of asset performance, availability, and maintenance in a specific plant. Higher level performance, including overall equipment effectiveness (OEE) and availability, is reflected on the home page with details available by drilling down.
An effective incident information management system needs to deal with several challenges. It must support heterogeneous distributed incident data, allow decision makers (DMs) to detect anomalies and extract useful knowledge, assist DMs in evaluating the risks and selecting an appropriate alternative during an incident, and provide differentiated services to satisfy the requirements of different incident management phases. To address these challenges, this paper proposes an incident information management framework that consists of three major components. The first component is a high-level data integration module in which heterogeneous data sources are integrated and presented in a uniform format. The second component is a data mining module that uses data mining methods to identify useful patterns and presents a process to provide differentiated services for pre-incident and post-incident information management. The third component is a multi-criteria decision-making (MCDM) module that utilizes MCDM methods to assess the current situation, find the satisfactory solutions, and take appropriate responses in a timely manner. To validate the proposed framework, this paper conducts a case study on agrometeorological disasters that occurred in China between 1997 and 2001. The case study demonstrates that the combination of data mining and MCDM methods can provide objective and comprehensive assessments of incident risks. © 2010 Elsevier B.V. All rights reserved.
To improve the performance of copper-matte Peirce-Smith Converting (PSC), data driven optimal decision making modeling method for converting process is studied. In view of some general characteristics (such as containing noise, small sample and so on) of the process data, a robust modeling method based on combined ANN (artificial neural network) is proposed
Purpose: The purpose of the paper is to provide a thorough analysis of the concepts of business intelligence (BI), knowledge management (KM) and analytical CRM (aCRM) and to establish a framework for integrating all the three to each other. The paper also seeks to establish a KM and aCRM based framework using data mining (DM) techniques, which helps in the enterprise decision-making. The objective is to share how KM and aCRM can be integrated into this seamless analytics framework to sustain excellence in decision making using effective data mining techniques and to explore how working on such aCRM system can be effective for enabling organizations delivering complete solutions. Design/methodology/approach: This paper is based on focused and dedicated study of the literature present on the aCRM, KM and data mining techniques. The paper considered how to develop a strategy and operational framework that would build aCRM on the foundation of existing DM techniques and KM approach to meet the business challenges. Based on this research, a customized, integrated framework, to match the needs of business was designed. Findings: KM focuses on managing knowledge within the organization and aCRM focuses on gaining analytical information from the customer data. Both KM and aCRM help in the decision making process and understanding. This knowledge is difficult to uncover. Hence, this paper explains the importance of data mining tools and techniques to uncover knowledge by the integration between KM and aCRM. This paper presents an integrated KM and aCRM based framework using DM techniques. Research limitations/implications: All the firms may not be in favor of adopting KM while implementing aCRM. The KM requires a convalesce of organizational culture, technology innovations, effective work force in culminating knowledge dissemination in all business domains. Practical implications: The organizations implementing this knowledge enabled aCRM framework would be easily able to convert their business knowledge via the analytical CRM to solve many business issues, such as increase response rates from direct mail, telephone, e-mail, and internet delivered marketing campaigns, increased sales and increased services. With aCRM, firms can identify their most profitable customers and use this knowledge for promotional schemes for those customers as well as identify future customers with prediction on ROI. Originality/value: The need for the integration of KM and aCRM is clear. It is written for practitioners who are looking for approaches to improve business performance and maintain high profits for their business by incorporating knowledge-enabled aCRM in their setup. © Emerald Group Publishing Limited.
Simulation has been used to evaluate various aspects of manufacturing systems. However, building a simulation model of a manufacturing system is time-consuming and error-prone because of the complexity of the systems. This paper introduces a generic simulation modeling framework to reduce the simulation model build time. The framework consists of layout modeling software and a data-driven generic simulation model. The generic simulation model was developed considering the processing as well as the logistics aspects of assembly manufacturing systems. The framework can be used to quickly develop an integrated simulation model of the production schedule, operation processes and logistics of a system. The framework was validated by developing simulation models of cellular and conveyor manufacturing systems. © 2010 Elsevier Ltd. All rights reserved.

The globalisation of markets and worldwide competition forces manufacturing enterprises to enter into alliances, leading to the creation of distributed manufacturing enterprises. Before forming a partnership, it is essential to evaluate the viability of a proposed manufacturing network. This paper presents a generic simulation model that attempts to create reusable enterprises modules for evaluating supply-chain performance. The simulation is developed using ARENA software coupled with a spreadsheet configuration tool. The latter constitute a manufacturing library from which various configurations of supply chain can be implemented and simulated. Copyright © 2011 Inderscience Enterprises Ltd.
The rapid development of the internet introduced new trend of electronic transactions that is gradually dominating all aspects of our daily life. The amount of data maintained by websites to keep track of the visitors is growing exponentially. Benefitting from such data is the target of the study described in this paper. We investigate and explore the process of analyzing log data of website visitor traffic in order to assist the owner of a website in understanding the behavior of the website visitors. We developed an integrated approach that involves statistical analysis, association rules mining, and social network construction and analysis. First, we analyze the statistical data on the types of visitors that come to the website, as well as the steps they take to reach and satisfy the goal of their visit. Second, we derive association rules in order to identify the correlations between the web pages. Third, we study the links between the web pages by constructing a social network based on the frequency of access to the web pages such that two web pages get linked in the social network if they are identified as frequently accessed together. The value added from the overall analysis of the website and its related data should be considered valuable for ecommerce and commercial website owners

Background: Scientists striving to unlock mysteries within complex biological systems face myriad barriers in effectively integrating available information to enhance their understanding. While experimental techniques and available data sources are rapidly evolving, useful information is dispersed across a variety of sources, and sources of the same information often do not use the same format or nomenclature. To harness these expanding resources, scientists need tools that bridge nomenclature differences and allow them to integrate, organize, and evaluate the quality of information without extensive computation.Results: Sidekick, a genomic data driven analysis and decision making framework, is a web-based tool that provides a user-friendly intuitive solution to the problem of information inaccessibility. Sidekick enables scientists without training in computation and data management to pursue answers to research questions like What are the mechanisms for disease X or Does the set of genes associated with disease X also influence other diseases. Sidekick enables the process of combining heterogeneous data, finding and maintaining the most up-to-date data, evaluating data sources, quantifying confidence in results based on evidence, and managing the multi-step research tasks needed to answer these questions. We demonstrate Sidekick's effectiveness by showing how to accomplish a complex published analysis in a fraction of the original time with no computational effort using Sidekick.Conclusions: Sidekick is an easy-to-use web-based tool that organizes and facilitates complex genomic research, allowing scientists to explore genomic relationships and formulate hypotheses without computational effort. Possible analysis steps include gene list discovery, gene-pair list discovery, various enrichments for both types of lists, and convenient list manipulation. Further, Sidekick's ability to characterize pairs of genes offers new ways to approach genomic analysis that traditional single gene lists do not, particularly in areas such as interaction discovery. © 2010 Doderer et al
School personnel currently lack an effective method to pattern and visually interpret disaggregated achievement data collected on students as a means to help inform decision making. This study, through the examination of longitudinal K-12 teacher assigned grading histories for entire cohorts of students from a school district (n=188), demonstrates a novel application of hierarchical cluster analysis and pattern visualization in which all data points collected on every student in a cohort can be patterned, visualized and interpreted to aid in data driven decision making by teachers and administrators. Additionally, as a proof-of-concept study, overall schooling outcomes, such as student dropout or taking a college entrance exam, are identified from the data patterns and compared to past methods of dropout identification as one example of the usefulness of the method. Hierarchical cluster analysis correctly identified over 80% of the students who dropped out using the entire student grade history patterns from either K-12 or K-8.© is retained by the first or sole author.
We describe an automated system for improving yield, power consumption and speed characteristics in the manufacture of semiconductors. Data are continually collected in the form of a history of tool usage, electrical and other real-valued measurements-a dimension of tens of thousands of features. Unique to this approach is the inference of patterns in the form of binary regression rules that demonstrate a significantly higher or lower performance value for tools relative to the overall mean for that manufacturing step. Results are filtered by knowledge-based constraints, increasing the likelihood that empirically validated rules will prove interesting and worth further investigation. This system is currently installed in the IBM 300 mm fab, manufacturing game chips and microprocessors. It has detected numerous opportunities for yield and performance improvement, saving many millions of dollars. © 2009 Springer Science+Business Media, LLC.
In recent years, knowledge has received significant attention in manufacturing to built a competitive advantage in the sector. Knowledge induction from data is an important issue in manufacturing to find the failure of the process then predict and improve the future system performance. This research examines the improvement of manufacturing process via data mining. Not only do we detect and isolate machine breakdowns in carpet manufacturing, but also we propose a C4.5 decision tree model. In addition, we use attribute relevance analysis to select the qualitative attribute's variables. Consequently, manufacturing process is redeveloped. © 2010 Elsevier B.V. All rights reserved.
Engineering asset management processes rely heavily on input of data and also produce a large amount of data. Many asset management organisations need to manage their data for a long period of time (e.g. Water supply pipelines data will be kept for more than 100 years in utility companies). Due to the inability to access original data requirements and system design documentation, it is difficult for these organisations to redesign their asset management systems which often results in ongoing data quality problems. Our nation-wide data quality survey of 2500 Australian engineering management organisations and a pilot study have revealed that many of the data quality problems emanate from inconsistent applications of business rules that govern the behaviour of data (e.g. data management, data flow, system interactions and so on) within asset management information systems. Thus, this research will investigate the problems of business rule-based data and information integration from disparate sources in various forms found in asset management systems (e.g. Databases, Excel spreadsheets, etc.). This research aims at developing innovative methods to automatically discover undocumented business rules from disparate data sources and to use business rules for automated data integration in order to deliver quality asset data sets. © Springer 2010.
Due to the information technology improvement and the growth of internet, enterprises are able to collect and to store huge amount of data. Using data mining technology to aid the data processing, information retrieval and knowledge generation process has become one of the critical missions to enterprise, so how to use data mining tools properly is user concern. Since not every user completely understand the theory of data mining, choosing the best solution from the functions which data mining tools provides is not easy. If user is not satisfied with the outcome of mining, communication with IT employees to adjust the software costs lots of time. To solve this problem, a selection model of data mining algorithms is proposed. By analyzing the content of business decision and application, user requirements will map to certain data mining category and algorithm. This method makes algorithm selection faster and reasonable to improve the efficiency of applying data mining tools to solve business problems. © 2010 Elsevier Ltd. All rights reserved.
As a result of today's competitive business environment, companies have been trying to improve the utilization of funds effectively in their budgets for information technology investments. These companies retrieve more information with the same set of resources by means of business intelligence methods. According to Rubin (Chabrow, 2004) IT budgets are not simply declining or levelling off, rather, companies are shifting from a pure cost-cut mode to a model that emphasises agility and efficiency. Tremendous daily growth of the company data requires more funds and investment for establishing the technologies and infrastructure necessary for gathering fast and crucial information that supports the decision making process. This necessity gave birth to various business intelligence methods, which mainly aim to process mass amount of collected data from their existing application, and represent it in a way with which companies can apply to their daily competitive decisions. This application primarily concerns the implementation of business intelligence for a retail business company. The aim is to implement built-in business intelligence solutions of the Microsoft SQL Server that holds the commercial information of the company for the past three years. The customer company has already been using Microsoft products. The key items used for analyzing data are sales, momentary inventory and logistics information. The application can be grouped in five main areas: Building the data warehouse, constructing OLAP cubes, applying data mining algorithms on OLAP cubes, representing the results in reports with reporting services, and implementation. © 2010 World Scientific Publishing Co.
In the context of technological expansion and development, companies feel the need to renew and optimize their information systems as they search for the best way to manage knowledge. Business ontologies within the semantic web are an excellent tool for managing knowledge within this space. The proposal in this article consists of a methodology for integrating information in companies. The application of this methodology results in the creation of a specific business ontology capable of semantic interoperability. The resulting ontology, developed from the information system of specific companies, represents the fundamental business concepts, thus making it a highly appropriate information integration tool. Its level of semantic expressivity improves on that of its own sources, and its solidity and consistency are guaranteed by means of checking by current reasoning tools. An ontology created in this way could drive the renewal processes of companies' information systems. A comparison is also made with a number of well-known business ontologies, and similarities and differences are drawn, highlighting the difficulty in aligning general ontologies to specific ones, such as the one we present. © 2010 Elsevier B.V. All rights reserved.
This article examines the convergence of two popular school improvement policies: instructional coaching and data-driven decision making (DDDM). Drawing on a mixed methods study of a statewide reading coach program in Florida middle schools, the article examines how coaches support DDDM and how this support relates to student and teacher outcomes. Authors find that although the majority of coaches spent time helping teachers analyze student data to guide instruction, data support was one among many coach activities. Estimates from models indicate that data analysis support, nevertheless, has a significant association with both perceived improvements in teaching and higher student achievement. © 2010 The Author(s).
The growing importance of customer-centric approach is evident from the fact that organisations started working towards the customer's choices and preferences. The organisations need to interact more often with customers to understand their needs and preferences, which is possible only when the barriers of customer interactions are enhanced (e.g., accessing remotely located customer, etc.). The paper proposes the guiding principles for successful mobile CRM (mCRM) in organisation considering data mining (DM) tools and techniques. The gap between the combined use of proven technologies like wireless communication, DM tools and techniques in system like CRM is big which can be only narrowed through guiding principles. The approach to identify critical business principles for mCRM implementation is based on intensive information gathering process from officials and staff of customer-centric organisations like banking, retails, insurance and automobile which are very critical for successful implementation of mCRM in the organisation from DM perspective. © 2010 Inderscience Enterprises Ltd.
The first part of this paper provides an overview of need for efficient decision-making tools in different public and private sectors. Data mining has been offered as a decision support tool particularly for criminal justice system. Advantages and disadvantages of various data mining tools were discussed in terms of informatics research background. Agent, host and environment relationships of data mining tools were applied in criminal justice system Copyright © 2010 Inderscience Enterprises Ltd.
Hard disk drive manufacturing has recently played an important role in Thailand's economy, with the number of hard disk drives produced increasing rapidly. The case study company is a manufacturer of metal frames for actuators
The production of vaccines is a complex biological process, with long cycle times and high variation in raw materials, growth rates, and test methods. Hundreds of variables are monitored for every batch of vaccine produced
The paper investigates the relationship between analytical capabilities in the plan, source, make and deliver area of the supply chain and its performance using information system support and business process orientation as moderators. Structural equation modeling employs a sample of 310 companies from different industries from the USA, Europe, Canada, Brazil and China. The findings suggest the existence of a statistically significant relationship between analytical capabilities and performance. The moderation effect of information systems support is considerably stronger than the effect of business process orientation. The results provide a better understanding of the areas where the impact of business analytics may be the strongest. © 2010 Elsevier B.V. All rights reserved.

As budgets have tightened, library administrators are looking for more data to help them in making difficult collections decisions. They then often provide these data to higher-level administrators outside the library. When collecting library data, several questions come to mind: Who collects the data? What kinds of data exist? When are data collected? Where does the responsibility of data collection reside? How are data collected and interpreted? In this article, the author aims to address all these questions so as to give the reader a sense of direction in how these data can be applied to drive collections and collection-development policy.© Taylor & Francis Group, LLC.
This paper presents a research effort undertaken to explore the applicability of data mining and knowledge discovery (DMKD) in combination with Geographic Information System (GIS) technology to pavement management to better decide maintenance strategies, set rehabilitation priorities, and make investment decisions. The main objective of the research is to utilize data mining techniques to find pertinent information hidden within the pavement database. Mining algorithm C5.0, including decision trees and association rules, has been used in this analysis. The selected rules have been used to predict the maintenance and rehabilitation strategy of road segments. A pavement database covering four counties within the state of North Carolina, which was provided by North Carolina DOT (NCDOT), has been used to test this method. A comparison was conducted in this paper for the decisions related to a rehabilitation strategy proposed by the NCDOT to the proposed methodology presented in this paper. From the experimental results, it was found that the rehabilitation strategy derived by this paper is different from that proposed by the NCDOT. After combining with the AIRA Data Mining method, seven final rules are defined. Using these final rules, the maps of several pavement rehabilitation strategies are created. When their numbers and locations are compared with ones made by engineers at the Institute for Transportation Research and Education (ITRE) at North Carolina State University, it has been found that error for the number and the location are various for the different rehabilitation strategies. With the pilot experiment in the project, it can be concluded: (1) use of the DMKD method for the decision of road maintenance and rehabilitation can greatly increase the speed of decision making, thus largely saving time and money, and shortening the project period
The study of consumer behavior in the refurbishment industry is crucial to the business operation of firms, but there is a lack of research in this regard. With reference to the EKB model specific to consumer behavior, this paper discusses the relationship among consumption characteristics, firm selection behavior and satisfaction degree of refurbishment customers. 242 valid questionnaire copies were collected from refurbishment customers, and analyzed using Decision Tree Analysis and Association Rules in Data Mining. The research results show that, over half of the customers tend to entrust the refurbishment to well-reputed firms. Moreover, the integrity of refurbishment equipment, response of refurbishment personnel, professionalism and confidence are key elements in service quality (SQ). The best marketing policy for the customers is one which provides more attractive services. These research findings may provide a useful reference for innovative refurbishment firms in their decision-making.
The lengthy manufacturing processes of thin film transistor-liquid crystal displays (TFT-LCDs) are complex, in which many factors can cause different types of defects on the panel and result in low yield. Examples are line defects, point defects, and Mura defects. Engineers rely on personal experience for trouble shooting during TFT-LCD manufacture, which does not quickly locate possible fault root causes using their own domain knowledge or rules of thumb. In a fully automated manufacturing environment in TFT-LCD factories, large amounts of raw data are increasingly accumulated from various sources, automatically or semi-automatically, for fault diagnosis and process monitoring. This study aims to propose a data mining framework for diagnosing the root causes of defects in factories. The extracted information and knowledge is helpful to engineers as a basis for trouble shooting and defect diagnosis. To examine the validity of this approach, an empirical study was conducted in a TFT-LCD company in Taiwan, and the results demonstrated the practical viability of this approach. © 2010 Chinese Institute of Industrial Engineers.
In recent years, different metaheuristic methods have been used to solve clustering problems. This paper addresses the problem of manufacturing cell formation using a modified particle swarm optimization (PSO) algorithm. The main modification that this work made to the original PSO algorithm consists in not using the vector of velocities that the standard PSO algorithm does. The proposed algorithm uses the concept of proportional likelihood with modifications, a technique that is used in data mining applications. Some simulation results are presented and compared with results from literature. The criterion used to group the machines into cells is based on the minimization of intercell movements. The computational results show that the PSO algorithm is able to find the optimal solutions in almost all instances, and its use in machine grouping problems is feasible. © 2009 Elsevier Ltd. All rights reserved.

In order to solve the problem of the redundant information to distinguish in the risk decision-making, in this paper, the data mining algorithms based on Rough Sets is studied. And we know the risk decision-making is an important aspect in the management practice. In the risk decision process of a project decision-making, it is necessary to use the algorithm to discover valuable knowledge and make a right decision. In the paper, a data mining method called Rough Sets is introduced in the field. And the algorithmic process of data mining based on Rough Set is studied. According to the Rough Sets theory, firstly, the factors set is established including condition attribute and decision attribute. Secondly, experts qualitatively describe risk factors and establish a decision database, called decision table. Thirdly, the attribute reduction algorithm based on Rough Sets is used to eliminate the redundant risk factor and its value of decision table. Fourthly, the minimum decision rules are abstracted based on data mining technology. Finally, the process of risk decision based on data mining of Rough Sets is analyzed in a case study.

A better control of extrusion processes offers clear advantages in the manufacturing of rubber profiles for the automotive industry. This work reports our experience in developing a support system aimed to ease the work of the extruder machinist while improving the quality of the profiles obtained. In order to build the system, an approach based on facts was adopted, following ISO 9000 standard quality principles. The data warehouse service available provided a wealth of information on the conditions of the running processes. The collected data, after being analysed with the appropriate data-mining techniques, allowed us to gain a better understanding of the process and to identify the main causes of variance. In particular, principal components analysis, Sammon projection and several classification techniques were applied for exploratory purposes. Different behaviours could be described for the extrusion process, allowing for the definition of a control strategy and, eventually, the development of a manufacturing support system. The estimates displayed by the system greatly improve the responsiveness of the machinist when the process departs from expected behaviour. The results of using this system in a local factory proved highly satisfactory and encouraging. © 2010 Taylor & Francis.
This chapter introduces the volume on Data Mining (DM) for Business Applications. The chapters in this book provide an overview of some of the major advances in the field, namely in terms of methodology and applications, both traditional and emerging. In this introductory paper, we provide a context for the rest of the book. The framework for discussing the contents of the book is the DM methodology, which is suitable both to organize and relate the diverse contributions of the chapters selected. The chapter closes with an overview of the chapters in the book to guide the reader. © 2010 The authors and IOS Press. All rights reserved.
Business logistics have played an important role of business operations of procurement, purchasing, inventory, warehousing, distribution, transportation, customer support, financial and human resources. Human body type classifications are also very crucial issue for garment manufacturing. Data mining has been widely used in many fields. But, there is lack of research in the area of establishing of garment-sizing systems for business logistics. This research aims to establish sizing systems of body types from the anthropometric data of females by using a fuzzy clustering-based data mining approach. Certain advantages may be observed when the sizing systems are established, using fuzzy clustering-based data mining procedure. Body types could be accurately classified for garment production according the newly the sizing systems. This approach is found to be effective in processing the anthropometric data, and obtaining regular rules for the development of sizing systems. The results of this study can provide an effective procedure of identifying the clusters of human body type to establish the sizing systems for integrating logistics operations internally with different functions inside the organization and also externally with business partners.
A data-mining approach is proposed in this study for estimating the interval cycle time of each job in a semiconductor manufacturing system, which was seldom investigated in the past studies but is a critical task to the semiconductor manufacturing system. The proposed methodology applies the look-ahead SOM-FBPN approach to estimate the step cycle times of the beginning step and the ending step, and then derives the interval cycle time. For evaluating the effectiveness of the proposed methodology, production simulation is also applied in this study to generate some test data. © 2009 Praise Worthy Prize S.r.l. - All rights reserved.
Web analytics are being increasingly used in the business sector to provide data-driven information that improve performance. Several successful online business organizations have realized that they can gain intelligence about customers' experience and engagement, products, and collections management by continuously data mining, monitoring, and analyzing econtent. Business Intelligence (BI) interprets data and transforms the information into insights that can be used to guide strategy formulation. A large number of web analytics tools are being used to report and collect data regarding different aspects of business. Reports from these tools show relationships among predetermined metrics and help in managing growing demand for intelligence. These efforts result in maximizing the value of econtent, mining, and analyzing transactional and customer data in real time.
The purpose of this paper is to present value drivers and their alignment with business operation from the Data Mining (DM) perspective. The paper also presents the benefits to any enterprise that will be generated by aligning the business with value drivers from the DM perspective. DM is the extraction of previously unknown and hidden information from large data records. Its tools can help to find the hidden value drivers of any business domain. They provide the hidden patterns and predictive information that experts may miss. Value drivers help business executives to better concentrate on key business factors that are of prime importance in any organisation. The value drivers and the DM tools together help to find useful and unknown information. Copyright © 2009, Inderscience Publishers.
In this paper, we present a data mining based methodology for optimizing the outcome of a batch manufacturing process. Predictive data mining techniques are applied to a multi-year set of manufacturing data with the purpose of reducing the variation of a crystal manufacturing process, which suffers from frequent fluctuations of the average outgoing yield. Our study is focused on specific defects that are the most common causes for scraping a manufactured crystal. A set of probabilistic rules explaining the likelihood of each defect as a function of interaction between the controllable variables are induced using the single-target and the multi-target Information Network algorithms. The rules clearly define the worst and the best conditions for the manufacturing process, also providing a complete explanation of all major fluctuations in the outgoing quality observed over the recent years. In addition, we show that an early detection of nearly the same predictive model was possible almost two years before the end of the data collection period, which could save many of the flawed crystals. The paper provides a detailed description of the optimization process, including the decisions taken at various stages and their outcomes. Conclusions applicable to similar engineering tasks are also outlined. © 2008 Springer Science+Business Media, LLC.
In modern manufacturing environments, vast amounts of data are collected in database management systems and data warehouses from all involved areas, including product and process design, assembly, materials planning, quality control, scheduling, maintenance, fault detection etc. Data mining has emerged as an important tool for knowledge acquisition from the manufacturing databases. This paper reviews the literature dealing with knowledge discovery and data mining applications in the broad domain of manufacturing with a special emphasis on the type of functions to be performed on the data. The major data mining functions to be performed include characterization and description, association, classification, prediction, clustering and evolution analysis. The papers reviewed have therefore been categorized in these five categories. It has been shown that there is a rapid growth in the application of data mining in the context of manufacturing processes and enterprises in the last 3 years. This review reveals the progressive applications and existing gaps identified in the context of data mining in manufacturing. A novel text mining approach has also been used on the abstracts and keywords of 150 papers to identify the research gaps and find the linkages between knowledge area, knowledge type and the applied data mining tools and techniques. © 2008 Springer Science+Business Media, LLC.


We often take reading and writing for granted. There was a time, not so long ago, when literacy was not available to everyone. Louis Braille recognized the importance of access to information, and his simple but ingenious code has had an impact on the lives of generations of people who are blind. Two hundred years after the birth of Louis Braille, parents and educators must still work to ensure that students with visual impairments receive appropriate literacy instruction and access to information. © 2009 AFB.
In this paper we use a large firm-level dataset to extend previous studies by augmenting the endogenous growth accounting framework with a data mining technique to analyze the complex relationships between the use of IT and organizational practices. There is emerging evidence of recent emphasis on organizational factors and a greater shift towards IT complementarities in which value addition is linked to combining complementary organizational practices with IT investments. Our findings indicate that the set of interrelated organizational practices that complement positively to IT use is different from the set of practices hindering IT use. The presence of clustering among organizational practices clearly implies that some combinations of practices make it difficult to precisely empirical examine. We have found that our technique was able to show some organizational factors may have different pathways to affect organizational performance and such organizational practices have often been overlooked but can play a weak yet non-trivial role in production and organizational processes. © 2008 Elsevier Ltd. All rights reserved.
Various data mining methodologies have been proposed in the literature to provide guidance towards the process of implementing data mining projects. The methodologies describe a data mining project as comprised of a sequence of phases and highlight the particular tasks and their corresponding activities to be performed during each of the phases. It seems that the large number of tasks and activities, often presented in a checklist manner, are cumbersome to implement and may explain why all the recommended tasks are not always formally implemented. Additionally, there is often little guidance provided towards how to implement a particular task. These issues seem to be especially dominant in case of the business understanding phase which is the foundational phase of any data mining project. In this paper, we present an organizationally grounded framework to formally implement the business understanding phase of data mining projects. The framework serves to highlight the dependencies between the various tasks of this phase and proposes how and when each task can be implemented. An illustrative example of a credit scoring application from the financial sector is used to exemplify the tasks discussed in the proposed framework. © 2008 Elsevier Ltd. All rights reserved.
Chemotherapy-induced nausea and vomiting (CINV) remains a problem in the management of cancer patients. Although improved antiemetics are available, clinical inertia, cost concerns in community practices, and perceptions of agent equivalence within a class can hinder the adoption of a newer drug indicated for prevention of both acute and delayed CINV - a newer drug that may provide more healthcare value in terms of patient outcomes and management costs than an older agent whose efficacy is limited to acute CINV. Georgia Cancer Specialists (GCS) is a 30-site community-based group practice that uses electronic medical records (EMRs) to collect information that allows for continuous quality improvement of cancer care. Using EMRs and billing records, GCS examined retrospectively whether using palonosetron rather than ondansetron reduced the incidence of resource-consuming extreme CINV events, including rescue antiemetics and office visits for CINV treatment. Initial use of palonosetron to prevent CINV reduced the incidence of extreme CINV events over a 5-day period by 76% in moderately emetogenic chemotherapy and by 54% in highly emetogenic chemotherapy, compared with ondansetron. In more than 3,000 patients evaluated, the use of palonosetron versus ondansetron was associated with an estimated reduction in staff management time totaling approximately 4 work months. © 2009 Elsevier Inc. All rights reserved.
Bottlenecks within a production line significantly affect system productivity. Most current bottleneck detection schemes focus on the long-term bottleneck detection problem using an analytical or simulation model. Furthermore, these studies are restricted to serial tandem lines only. This research focuses on extending the newly developed data-driven method for throughput bottleneck detection from a serial line to a manufacturing system with a complex layout. Within these complex systems, two specific layouts are considered: the concurrent process and the closed-loop feedback process. The method is verified using simulation case studies. An industrial case study is examined to demonstrate the practicality of this approach and to validate the efficiency of the proposed bottleneck detection method.
Organizations are seeking to deploy information to a wider audience to enable better decision-making. However, as information is more widely deployed, fewer information users understand how to analyze and visualize the data in the appropriate manner to make sound business decisions. In addition, an individual analysis may lead the user to one or more analyses, or to other information sources such as ERP or CRM system. Structured Analytics provides the means for the organization to define knowledge-based process map of how business experts use certain information and knowledge to make business decisions. Within the framework of Knowledge Management, Structured Analytics provides the means to share the organization experts' knowledge and analysis decision-oriented thought process with other decision makers and analysts using guided and structured business decisionmaking processes. The analysis process map captures the knowledge and thought processes of expert decision makers and provides a dynamic graphical analysis environment that is composed of roleconfigurable and context-based analytics that guide decision-making professionals through specific analysis processes. These powerful analysis maps can reveal correlation across a broad range of business parameters. The analysis map has three essential components: Guided Analysis based on best practices scenarios, shared logic repository, and dynamically interlinked views and reports. Structured analytics can be considered a Knowledge Management framework for building analytic applications to provide the ability to reuse business logic. While the market has not actually defined a requirement for structured analytics, inadequate capture and usage of knowledge by organizations and low user adoption and high abandonment rates of current knowledge-based systems are problems that can be solved by Structured Analytics. The power and dynamic nature of Structured Analytics will lead to the formation a company's Knowledge Expert System that allows organizations to sustain and create competitive advantages. Furthermore, tacit knowledge and cognitive processes can now be captured and shared throughout the organization to create The Intelligent Organization. The foundation of the above research was obtained through several concentrated interviews of large business organizations in the US and Europe regarding their needs and use of knowledge asset. These organizations are: PepsiCo (US), Proctor & Gamble (US), Roche Pharmaceuticals (Germany), Reckitt-Benckiser (UK), Alfa Laval (Sweden), Siemens (Germany), Reuters (England), and ABB (Switzerland). These interviews led the author to the conclusion that organizations of all types require a platform of knowledge capture and use. Thus, every organization needs to become an intelligent organization that capable of continuous value creation and continuous innovations by creating and using corporate-wide knowledge base. This will naturally leads the intelligent organization to become an innovator in its space and other spaces. © 2009 WASET.ORG.
Bottlenecks within a production line significantly reduce the productivity. Quick and correct identification of the bottleneck locations can lead to an improvement in the operation management of utilising finite manufacturing resources, increasing the system throughput, and minimising the total cost of production. Most of the current bottleneck detection schemes focus on the long-term bottleneck detection problem and an analytical or simulation model is usually needed. Due to recent developments, short-term process control and quick decision making on the plant floor have emerged as important qualities for operation management. This research proposes a new data driven method for throughput bottleneck detection in both the short and long term. The method utilises the production line blockage and starvation probabilities and buffer content records to identify the production constraints without building an analytical or simulation model. The method has been verified both analytically and by simulation. An industrial case study has also been used in order to demonstrate the implementation and validate the efficiency of the proposed bottleneck detection method.
Determination of the most significant manufacturing process parameters using collected past data can be very helpful in solving important industrial problems, such as the detection of root causes of deteriorating product quality, the selection of the most efficient parameters to control the process, and the prediction of breakdowns of machines, equipment, etc. A methodology of determination of relative significances of process variables and possible interactions between them, based on interrogations of generalized regression models, is proposed and tested. The performance of several types of data mining tool, such as artificial neural networks, support vector machines, regression trees, classification trees, and a naïve Bayesian classifier, is compared. Also, some simple non-parametric statistical methods, based on an analysis of variance (ANOVA) and contingency tables, are evaluated for comparison purposes. The tests were performed using simulated data sets, with assumed hidden relationships, as well as on real data collected in the foundry industry. It was found that the performance of significance and interaction factors obtained from regression models, and, in particular, neural networks, is satisfactory, while the other methods appeared to be less accurate and or/less reliable.© IMechE 2008.

Background: Accountability demands are increasingly pushing school leaders to explore more data and do more sophisticated analyses. Data-driven decision making (DDDM) has become an emerging field of practice for school leadership and a central focus of education policy and practice. Purpose: This study examined principals' DDDM practices and identified the factors influencing DDDM using the theoretical frame of information use environments. Participants: Participants were 183 public high school principals in a Midwestern state. Research Design: The research design was cross-sectional survey research. Data Collection and Analysis: Survey instruments were developed and administered to principals. Structural equation modeling was conducted to determine what factors significantly affect principals' DDDM practices in different leadership dimensions. Findings: Principals used data more frequently in instructional and organization operational leadership than in the leadership dimensions of school vision and collaborative partnerships. Different contextual factors affected data use in different leadership dimensions. Human-related factors such as perceptions of data quality and data analysis skills seemed to have direct effects on data use in addressing administrative problems in instruction and organizational operation, in which data were used frequently. Organization-related factors such as school district requirement and accessibility of data tended to have more direct influence on data use in the leadership dimensions of school vision and collaborative partnerships, where data were used less often. Conclusions: This study supports the proposition that as information behavior, DDDM is situational, multidimensional, and dynamic. Results provide insights into practice, research, and theoretical foundation for the emerging topic of DDDM. © 2008 The University Council for Educational Administration.
Nowadays data mining plays an important role in decision making. Since many organizations do not possess the in-house expertise of data mining, it is beneficial to outsource data mining tasks to external service providers. However, most organizations hesitate to do so due to the concern of loss of business intelligence and customer privacy. In this paper, we present a Bloom filter based solution to enable organizations to outsource their tasks of mining association rules, at the same time, protect their business intelligence and customer privacy. Our approach can achieve high precision in data mining by trading-off the storage requirement. © Springer-Verlag London Limited 2007.
The purpose of this article is to improve our understanding of data-driven decision-making strategies that are initiated at the district or system level. We apply principal-agent theory to the analysis of qualitative data gathered in a case study of 4 urban school systems. Our findings suggest educators at the school level need not only systemic support but also enough decision-making autonomy to make site-level decisions on the basis of data. Secondly, we found that building expertise and capacity at the school site for data-driven decision-making is necessary but not a sufficient condition for success. Finally, in designing an accountability system, the imbalance in the distribution of information between the central office and the schools must be accommodated. Implications for further research and policy, based on these findings, are also discussed.
Purpose - Data mining (DM) has been considered to be a tool of business intelligence (BI) for knowledge discovery. Recent discussions in this field state that DM does not contribute to business in a large-scale. The purpose of this paper is to discuss the importance of business insiders in the process of knowledge development to make DM more relevant to business. Design/methodology/ approach - This paper proposes a blog-based model of knowledge sharing system to support the DM process for effective BI. Findings - Through an illustrative case study, the paper has demonstrated the usefulness of the model of knowledge sharing system for DM in the dynamic transformation of explicit and tacit knowledge for BI. DM can be an effective BI tool only when business insiders are involved and organizational knowledge sharing is implemented. Practical implications - The structure of blog-based knowledge sharing systems for DM process can be practically applied to enterprises for BI. Originality/value - The paper suggests that any significant DM process in the BI context must involve data miner centered DM cycle and business insider centered knowledge development cycle. © Emerald Group Publishing Limited.
Purpose - Rapid innovation and globalization have generated tremendous opportunities and choices in the marketplace for firms and customers. Competitive pressures have led to sourcing and manufacturing on a global scale resulting in a significant increase in products. The paper tries to identify the need for real time business intelligence (BI) in supply chain analytics. Design/methodology/approach - The paper provides argument and analysis of the advantages and hurdles in BI. Findings - The paper focuses on the necessity to revisit the traditional BI concept that integrates and consolidates information in an organization in order to support firms that are service oriented and seeking customer loyalty and retention. Enhancing effectiveness and efficiency of supply chain analytics using a BI approach is a critical component in a company's ability to achieve its competitive advantage. Originality/value - This paper furthers understanding of the issues surrounding the use of BI systems in supply chains.
Analysis of consumer-related and consumer-generated data is a very important way to measure the success of on-line retailing. The software packages for data analysis have two major shortcomings: (1) solutions are not offered as a service reachable by standard procedures over the Internet, but as isolated standalone applications or ERP system modules
Data plays a vital role as a source of information to organizations, especially in times of information and technology. One encounters a not-so-perfect database from which data is missing, and the results obtained from such a database may provide biased or misleading solutions. Therefore, imputing missing data to a database has been regarded as one of the major steps in data mining. The present research used different methods of data mining to construct imputative models in accordance with different types of missing data. When the missing data is continuous, regression models and Neural Networks are used to build imputative models. For the categorical missing data, the logistic regression model, neural network, C5.0 and CART are employed to construct imputative models. The results showed that the regression model was found to provide the best estimate of continuous missing data
Skytide Business Analytics Server used with IBM DB2.9 stores XML in its inherent hierarchical format and analyzes XML data without having to map it to a relational format. The Skytide provides reporting and analysis capabilities for both structured relational data and semi-structured data. Analytic models for large-scale XML data is represented by the XML database transaction processing benchmark (TPoX). Skytide server increased processing of XML, stored as character large objects (CLOBs) when used with DB2.9 as its data source. The TPoX benchmark models a financial transaction processing application using XML data. The Skytide uses two types of data cube that are, using all data of the respective scale factor and considering information related to the US customers only. Skytide directly loads the relevant data values during the cube process and links only the XML elements and attributes.
This paper examines cell quality performance improvement through the integration of data mining using Artificial Neural Network (ANN) techniques and cellular manufacturing. The aim of this paper is to study and predict the factors that impact quality product in cellular manufacturing, such as used material, material complexity, operation type (lathe, mill, thread, groove, bore, etc.), machine, machinist and quantity to improve cell performance. The outcome suggests improvement in the part processing sequence, machining process capability, formation of family products, design, the machine operator performance and work schedule in order to improve the machine performance in cellular manufacturing. © 2008 Inderscience Enterprises Ltd.
This chapter introduces the volume on Applications of Data Mining in E-Business and Finance. It discusses how application-specific issues can affect the development of a data mining project. An overview of the chapters in the book is then given to guide the reader. © 2008 The authors and IOS Press. All rights reserved.
We develop techniques for mining labor records from a large number of historical IT consulting projects in order to discover clusters of projects exhibiting similar resource usage over the project life-cycle. The clustering results, together with domain expertise, are used to build a meaningful project taxonomy that can be linked to project resource requirements. Such a linkage is essential for project-based workforce demand forecasting, a key input for more advanced workforce management decision support. We formulate the problem as a sequence clustering problem where each sequence represents a project and each observation in the sequence represents the weekly distribution of project labor hours across job role categories. To solve the problem, we use a model-based clustering algorithm based on explicit state duration left-right hidden semi-Markov models (HsMM) capable of handling high-dimensional, sparse, and noisy Dirichlet-distributed observations and sequences of widely varying lengths. We then present an approach for using the underlying cluster models to estimate future staffing needs. The approach is applied to a set of 250 IT consulting projects and the results discussed. © 2008 The authors and IOS Press. All rights reserved.
In this paper, several aspects of perception based time series data mining based on the methodology of computing with words and perceptions are discusses. First, we consider possible approaches to precisiate perception based patterns in time series data bases and types of fuzzy constraints used in such precisiation. Next, several types of associations in time series data bases and the possible approaches to convert these associations in generalized constraint rules are discussed. Finally, we summarize the methods of translation of expert knowledge and retranslation of solutions. © 2007 Springer-Verlag Berlin Heidelberg.
Travel and Entertainment (T&E) expenses are under increasing scrutiny as one of the largest controllable indirect expenses in a firm. This involves internal audits and analysis by business controls personnel to identify fraud and misuse and to take appropriate corrective actions. We have developed a set of statistical models to identify suspicious behavior for further investigation. Our Behavioral Shift Models (BSM) leverage domain knowledge in the form of simple, generic templates that represent classes of fraud and abuse. The emphasis is on robustly detecting repeated, out-of-the-norm behaviors as opposed to single instance occurrences. In this paper, we describe the application of these models and characterize their detection capabilities empirically. We also present validated results and insights generated by our approach when applied to production data from multiple firms for several T&E scenarios. © 2007, Australian Computer Society, Inc.
Trends in data processing for decision support show that business users need business analytics, i.e. analytical applications which incorporate a variety of business oriented data analysis techniques and task-specific knowledge. The paper discusses the feasibility of investment in two models of implementing business analytics: custom development and packed analytical applications. The consequences of both models are shown on two models of business analytics implementation in Croatia.

I survey the transformation of the data mining and knowledge discovery field over the last 10 years from the unique vantage point of KDnuggets as a leading chronicler of the field. Analysis of the most frequent words in KDnuggets News leads to revealing observations. © 2007 Springer Science+Business Media, LLC.
The work presented in this paper is result of a rapid increase of interest in game theoretical analysis and a huge growth of game related databases. It is likely that useful knowledge can be extracted from these databases. This paper argues that applying data mining algorithms together with Game Theory poses a significant potential as a new way to analyze complex engineering systems, such as strategy selection in manufacturing analysis. Recent research shows that combining data mining and Game Theory has not yet come up with reasonable solutions for the representation and structuring of the knowledge in a game. In order to examine the idea, a novel approach of fusing these two techniques has been developed in this paper and tested on real-world manufacturing datasets. The obtained results have been indicated the superiority of the proposed approach. Some fruitful directions for future research are outlined as well. © 2007 Springer Science+Business Media, LLC.
Recent advances in computers and manufacturing techniques have made it easy to collect and store all kinds of data in manufacturing enterprises. The problem of how to enable engineers and managers to understand large amount of data remains. Traditional data analysis methods are no longer the best alternative to be used. Data Mining (DM) approaches have created new intelligent tools for extracting useful information and knowledge automatically. All these will have a profound impact on current practices in manufacturing. In this paper the nature and implications of DM techniques in manufacturing and their implementations on product design and manufacturing are discussed. © 2007 Springer Science+Business Media, LLC.
In order to rapidly and effectively meet the informative demand from commanding decision-making, it is important to build, maintain and mine the intelligence database. The type, structure and maintenance of military intelligence database are discussed. On this condition, a new data-mining arithmetic based on relation intelligence database is presented according to the preference information and the requirement of time limit given by the commander. Furthermore, a simple calculative example is presented to prove the arithmetic with better maneuverability. Lastly, the problem of how to process the intelligence data mined from the intelligence database is discussed. © 2007 The Second Academy of China Aerospace Science & Industry Cooperation.
Business intelligence (BI) can bridge the gap between disconnected silos of information, and BI is also viewed as a power tool to help enterprises survive during the competitive environment. From the viewpoint of system, business process and manufacturing process are also viewed as a system. Hence, manufacturing processes also have hidden intelligence like as the concept with respect to business processes. Those potential intelligences behind manufacturing processes should hold the information about process improvement or product development. Namely, mining such manufacturing intelligence will have positive contribution to enterprise's core competition. In this study, we proposed a procedure based on artificial neural networks (ANNs) to mine the possible manufacturing intelligence for achieving parameter optimization. The rationality and feasibility of the proposed procedure can also be demonstrated well according to the illustrative example in this study.
Accurate and timely prediction of a manufacturing process yield and flow times is often desired as a means of reducing overall production costs. To this end, this paper develops a new decision-theoretic classification framework and applies it to a real-world semiconductor wafer manufacturing line that suffers from constant variations in the characteristics of the chip-manufacturing process. The decision-theoretic framework is based on a model for evaluating classifiers in terms of their value in decision-making. Recognizing that in many practical applications the values of the class probabilities as well as payoffs are neither static nor known exactly, a precise condition under which one classifier 'dominates' another classifier (i.e. achieves higher payoff), regardless of payoff or class distribution information, is presented. Building on the decision-theoretic model, two robust ensemble classification methods are proposed that construct composite classifiers that are at least as good as any of the existing component classifiers for all possible payoff functions and class distributions. It is shown how these two robust ensemble classifiers are put into practice by developing decision rules for effectively monitoring and controlling the real-world semiconductor wafer fabrication line under study.
During wafer fabrication, process data, equipment data, and lot history will be automatically or semi-automatically recorded and accumulated in database for monitoring the process, diagnosing faults, and managing manufacturing. However, in high-tech industry such as semiconductor manufacturing, many factors that are interrelated affect the yield of fabricated wafers. Engineers who rely on personal domain knowledge cannot find possible root causes of defects rapidly and effectively. This study aims to develop a framework for data mining and knowledge discovery from database that consists of a Kruskal-Wallis test, K-means clustering, and the variance reduction splitting criterion to investigate the huge amount of semiconductor manufacturing data and infer possible causes of faults and manufacturing process variations. The extracted information and knowledge is helpful to engineers as a basis for trouble shooting and defect diagnosis. We validated this approach with an empirical study in a semiconductor foundry company in Taiwan and the results demonstrated the practical viability of this approach. © 2006 Elsevier Ltd. All rights reserved.
Semiconductor manufacturing involves lengthy and complex processes, and hence is capital intensive. Companies compete with each other by continuously employing new technologies, increasing yield, and reducing costs. Yield improvement is increasingly important as advanced fabrication technologies are complicated and interrelated. In particular, wafer bin maps (WBM) that present specific failure patterns provide crucial information to track the process problems in semiconductor manufacturing, yet most fabrication facility (fabs) rely on experienced engineers' judgments of the map patterns through eye-ball analysis. Thus, existing studies are subjective, time consuming, and are also restricted by the capability of human recognition. This study proposes a hybrid data mining approach that integrates spatial statistics and adaptive resonance theory neural networks to quickly extract patterns from WBM and associate with manufacturing defects. An empirical study of WBM clustering was conducted in a fab for validation. The results showed practical viability of the proposed approach and now an expert system embedded with the developed algorithm has been implemented in a fab in Taiwan. This study concludes with a discussion on further research. © 2006 Elsevier B.V. All rights reserved.


Chemometric models including data pre-treatment and inspection software are able to evaluate the coarsely grained top down observational data from biological tissues through spectroscopic sensors in a dialogue with fine-grained analytical bottom up data in a sequential exploratory selection strategy. It facilitates interpretation and development of new theoretical concepts. This is demonstrated by a data set of 11 barley mutant endosperm genes and crosses focusing on the extremely high mutant gene discrimination capability by Near Infrared Reflection Spectroscopy (NIRS). It represents patterns of intact chemical bonds from self-organised endosperm tissues, which are validated to prior spectroscopic, chemical and genetic knowledge. Principal Component Analysis (PCA) of spectra could classify different endosperm mutant genes and changed gene backgrounds. A new carbohydrate pathway regulation from starch to ?-glucan was identified. Specific mutant genes were defined visually by directly inspecting and validating spectral patterns from genetically defined barleys. Genetic concepts such as the phenome and pleiotropy were given new definitions, phenomenologically expressed as log1/R MSC (Multiple Scatter Corrected) NIRS fingerprints comparing mutants in an iso-genic background. Chemometric models are efficient in over-viewing genetic and environmental spectral differences but are not able to reproduce and predict in detail the finely tuned spectra from the 
In order to overcome the inherent shortcomings in knowledge acquisition and applicability of traditional engineering decision support systems (EDSS), a new mechanism that integrates data mining (DM) technology with engineering decision support technologies is proposed and an intelligent decision support platform for engineering decision makings named DM-EDSS is introduced. DM-EDSS aims to automatically extract useful knowledge (predictive models or rules) from a large amount of historical engineering data, and provide decision supports to solve new engineering problems by reasoning on the extracted knowledge. The results of case study show that the knowledge extracted by DM-EDSS can provide valuable suggestions for engineering decision makings, and the reasoning results are of acceptable precisions that can meet the requirements of engineering practices.
We explore use of data mining for lead time estimation in make-to-order manufacturing. The regression tree approach is chosen as the specific data mining method. Training and test data are generated from variations of a job shop simulation model. Starting with a large set of job and shop attributes, a reasonably small subset is selected based on their contribution to estimation performance. Data mining with the selected attributes is compared with linear regression and three other lead time estimation methods from the literature. Empirical results indicate that our data mining approach coupled with the attribute selection scheme outperforms these methods. © 2005 Elsevier B.V. All rights reserved.
Because English is the lingua franca of world trade, the language of commerce, finance, and economics is characterized by an ever-increasing use of Anglicisms. Polysemic English loan words are particularly problematic in translation, as their meanings do not always match across donor and receptor languages. An Anglicism may, for example, convey a subset of the senses expressed by the same word in English and/or it may convey meanings typically expressed by a synonymous English word. It is no wonder that translator trainees often get into difficulty when having to decide whether and how to translate an English word with an established Anglicism in Italian. This tutorial presents a corpus-based teaching methodology that draws on the data-learning approach devised by Tim Johns and aims to equip translator trainees with a kit of analytical tools for better understanding Anglicisms in cross- and inter-linguistic professional communication so that they can produce accurate and effective translations. After briefly reviewing recent studies of Anglicisms in Italian, I outline the main features of the proposed educational methodology and illustrate how it has been applied to the analysis of the lemma business in the Language for Specific Purposes (LSP) translation classroom. © 2006 IEEE.
Data mining involves the extraction of meaning contained in information to provide the understanding needed by a user to make the right decision. Tools to perform this process include artificial neural networks, genetic algorithms, decision tress, nearest neighbor method, rule induction and data visualization. When used in the textile industry, data mining provides efficient quality control, textile wet processing, energy conservation and machine maintenance.
Variation reduction of manufacturing processes is an essential objective of process quality improvement. It is highly desirable to develop a methodology of variation source identification that helps quickly identify the variation sources, hence leading to quality improvement and cost reduction in manufacturing systems. This paper presents a variation source identification method based on the analysis of the covariance matrix of process quality measurements. The identification procedure utilizes the fact that the eigenspace of the quality measurement covariance matrix can be decomposed into a subspace due to variation sources and a subspace purely due to system noise. The former subspaces for different samples will be the same if the same variation sources dominate. A testing procedure is presented, which can determine the closeness of the subspaces under sampling uncertainty. A case study is conducted to illustrate the effectiveness of this methodology. © 2006 Wiley Periodicals, Inc.
In this paper, the monthly electric demand prediction approach with dynamic and adaptive mechanism connected to the business environment is proposed with the aim of building the management plan of stable electric power supply and accomodation. The proposed prediction adopts the Kalman-Filter as the basic prediction scheme and possesses two characteristics stated below. One is the state-space built with the principal component time-series integrated with time-series PCA (Principal Component Method) from multi business indices related to the targeted time-series. The other is the self-organised auto-updating of the state-space by structured neural networks. The proposed scheme shows considerably more accurate prediction than any other models with single variable time-series and the obvious effect appears to be the high accuracy achieved by adopting time-series PCA as a Data-Mining technique. Given these results, the proposed prediction scheme might be considered to improve stable electric power supply and accommodation. This prediction scheme can be applied to various management areas, and so it might be considered to be an effective method for decision-making support. Copyright © 2006 Inderscience Enterprises Ltd.
Data mining tools can be very beneficial for discovering interesting and useful patterns in complicated manufacturing processes. These patterns can be used, for example, to improve manufacturing quality. However, data accumulated in manufacturing plants have unique characteristics, such as unbalanced distribution of the target attribute, and a small training set relative to the number of input features. Thus, conventional methods are inaccurate in quality improvement cases. Recent research shows, however, that a decomposition tactic may be appropriate here and this paper presents a new feature set decomposition methodology that is capable of dealing with the data characteristics associated with quality improvement. In order to examine the idea, a new algorithm called (Breadth-Oblivious-Wrapper) BOW has been developed. This algorithm performs a breadth first search while using a new F-measure splitting criterion for multiple oblivious trees. The new algorithm was tested on various real-world manufacturing datasets, specifically the food processing industry and integrated circuit fabrication. The obtained results have been compared to other methods, indicating the superiority of the proposed methodology. © Springer Science+Business Media, LLC 2006.
The complexity of semiconductor manufacturing is increasing due to the smaller feature sizes, greater number of layers, and existing process reentry characteristics. As a result, it is difficult to manage and clarify responsibility for low yields in specific products. This paper presents a comprehensive data mining method for predicting and classifying the product yields in semiconductor manufacturing processes. A genetic programming (GP) approach, capable of constructing a yield prediction system and performing automatic discovery of the significant factors that might cause low yield, is presented. Comparison with the results then is performed using a decision tree induction algorithm. Moreover, this research illustrates the robustness and effectiveness of this method using a well-known DRAM fab's real data set, with discussion of the results. © Springer Science+Business Media, LLC 2006.
The use of data mining technique in knowledge based information system in business intelligence (BI) is discussed. Data mining involves use of techniques to find underlying structures and relationships in a large database. The focal point of knowledge management to make business more valuable and the need to find ways to ensure that this happens. Neutrality is a characteristics of knowledge which consists of accuracy and reliability. Accuracy means that data or information is correct and reliability implies that the information is a true indicator of the variable that it is intended to measure. Effective knowledge management may require going beyond initial knowledge capture, to facilitate strategic decisions about how to extend privously captured knowledge. Market based analysis is useful type of data analysis for marketing as it determines what products customers purchase together, it takes its name from the idea of customers throwing all their purchases into a shopping cart.
In this paper, a new data mining algorithm based on the rough sets theory is presented for manufacturing process control. The algorithm extracts useful knowledge from large data sets obtained from manufacturing processes and represents this knowledge using if/then decision rules. Application of the data mining algorithm developed in this paper is illustrated with an industrial example of rapid tool making (RTM). RTM is a technology that adopts rapid prototyping (RP) techniques, such as spray forming, and applies them to tool and die making. A detailed discussion on how to control the output of the manufacturing process using the results obtained from the data mining algorithm is also presented. Compared to other data mining methods, such decision trees and neural networks, the advantage of the proposed approach is its accuracy, computational efficiency, and ease of use.
Based on synthetically analyzing the distributed application and data-stored environment about a business intelligence system, a business intelligence application oriented distributed data mining system model with Agents as primary components is proposed. The functions of user interface, data mining module, enterprise-applied database system and relevant Agents are discussed. Components of data mining mobile Agent and mobile Agent server in the model are further introduced in Java programming environment. The system is easy to be extended with flexible structure and high security. It is a good reference model to be used to design such a data mining system.
In this paper, the environmental factors surrounding fruit trees are used as the explanatory variables, and the factors concerning the quality of the agricultural produce are used as the explained variables. The observed data are analyzed. The object of analysis is the Onshu orange (mandarin orange). The purposes of this paper are analysis of the effect of environmental factors on product quality, the target value settings for the controllable environmental factors in order to produce high-quality products, and evaluation of the cultivation environment for each item. A system for data analysis is proposed which extracts the useful information for the decision-making such as cultivation management, as well as harvest/shipping planning, from the observed data. The system uses a probabilistic neural network and learns the mapping from the describing variables consisting of observed data to the explained variables. By superposing the acquired knowledge, inverse modeling can be performed. It is shown that by using the proposed system, multivariate nonlinear inverse mapping can be realized instead of the linear inverse regression estimation used in conventional single regression analysis. © 2005 Wiley Periodicals, Inc.
Purpose - Business process redesign (BPR) is undertaken to achieve order-of-magnitude improvements over old forms of the organization. Practitioners in academia and the business world have developed a number of methodologies to support this competitive restructuring that forms the current focus of concern, many of which have not been successful. The purpose of this paper is to suggest the use of data mining (DM) as a technique to support the process of redesigning a business by extracting the much needed knowledge hidden in large volumes of data maintained by the organization through the DM models. Design/methodology/approach - The paper explains how the DM/BPR tool will extract and transfer the much-needed knowledge necessary for implementing the new business. Findings - The process of extracting knowledge hidden from large volumes of data (DM) has proved very successful in solving many business or scientific problems to achieve competitive advantage. As suggested in the DM/BPR framework, the DM model can be deployed on the massive data collected from past business processes of the organization which then yields the previously unknown knowledge and trends needed by top managers or decision makers in the organization for effective business process redesigning. Originality/value - The proposed DM/BPR framework transforms the old business into a new prospect-oriented business organization by carefully re-engineering the old system incorporating the new discovered knowledge which helps the manager to make wise and informed business decisions in the area of accountability, business change management expertise, business process analysis, business model design, business model implementation and others. © Emerald Group Publishing Limited.
It becomes increasingly important to automatically discover business knowledge from large databases in order to drastically reduce operators costs in the areas of CRM (Customer Relationship Management), knowledge management, Web marketing, etc. This paper introduces NEC's technology concept of Knowledge Organization and data mining engines designed for it. They include text mining tool Survey Analyzer, key semantics mining, and topic analysis engine TopicAnalyzer. We briefly overview the principles of these engines and illustrate their applications to real domains.
Purpose - To identify, discuss and provide a solution for a common problem in the mathematical analyses in business analyses, namely, paralysis by analysis. Design/methodology/approach - The paper first discusses the scale and frequency of the paralysis by analysis problem, before discussing it in more depth before addressing a fundamental problem, which is an important root of the paralysis by analysis problem, the indiscriminate usage of central tendency measures. Finally, it discusses how variance can be turned from being a liability into an asset. The approach is conversational but examples and a case study are provided to substantiate the arguments. Findings - The paper provides some recommendations for avoiding paralysis by analysis. Practical implications - Basically, the paper shows by argument and example why practitioners and some researchers need to better understand the limitations and promises of mathematical analyses and to some extent how to incorporate this understanding into their work. Originality/value - There is nothing really new in this paper, but it discusses a problem that for some reason is often ignored by practitioners and some researchers. The true value of the paper therefore lies in making practitioners, in particular, more aware of the limitations as well as the possibilities in the mathematical analyses performed in business analytics so that they can better understand what they are doing and hence get behind the numbers, as it were. © Emerald Group Publishing Limited.
The purpose of this paper is to explain a specific intelligent CIM (computer integrated manufacturing) system by integrating the following five major subject areas: computer integrated manufacturing, data warehouse, online analytical processing (OLAP), data mining and artificial intelligence. The data mining system makes use of the decision tree algorithm and classification model in exploring the meaningful information, which is useful in the process of decision making. Subsequently, the rules discovered by the data mining system are expressed through the rule based knowledge presentation method of the expert system. The intelligent CIM system is applied to semiconductor packing factories and also point at the great fluctuation of DRAM (dynamic random access memory) prices. The results of this paper are the product yield, the manufacturing cycle time and the frequency of holding lot, which have been improved. The contribution can increase business competitiveness, reduce production cost, and promote the rate of available promise for order.
Effective communication between seismic specialists should be facilitated by a shared process model that can be used at different levels of abstraction. In this shared model the seismic work-flow is presented as a value chain, showing the complex interrelationships between the broad range of specialized activities that are needed in today's practice. One of these activities is velocity estimation, providing the relationship between seismic time and geological depth. Excellence in the seismic value chain will depend on the quality of the specialized tools and skills (abilities) involved, as well as on the capability of the organization to combine these abilities in an integrated work-flow to realize maximum value at the end of the chain. © 2004 European Association of Geoscientists & Engineers.
The indexes for semiconductor manufacturing management are complicated and interrelated. Therefore, it is hard to clarify the relationships among the indexes and to derive useful rules for production management. Existing approaches rely on following individual indexes without considering the production system as a whole. This study aims to fill the gap by reviewing the related studies on semiconductor manufacturing management and developing a complete set of performance indexes in hierarchy. In addition, we apply data mining techniques for analyzing production data collected in a semiconductor fab in Taiwan to validate this approach. The empirically derived patterns among the critical indexes were useful for supporting production management decisions. The results demonstrate the practical viability of this approach. This study concludes with results and discussion on future research.
Bibliomining, or data mining for libraries, is the application of data mining and bibliometric tools to data produced from library services. This article outlines the bibliomining process with emphasis on data warehousing issues. Methods for cleaning and anonymizing library data are presented with examples.
The development of an intelligent business analytics platform was described. The aim of the platform was to create a software environment where the latest algorithms and architecture developed within industrial or academic research can be incorporated efficiently into real business applications. As such, the platform removed some of the bottle-necks in the technology transfer process and have enabled teams to builds experimental systems in a fraction of the time required to do so in the past.
Most of the enterprises having finished, one after the other, in the automation of transaction processing, the key issue for the future competitiveness of enterprises shall be the automation of decision-making processing and forecast processing. To complement the deficiency of the forecast flow in enterprise resource planning (ERP) system, we propose here a data searching architecture that can be built on the ERP system while meeting the individual needs of enterprises and providing real time accurate forecasts on changes in the future market. The present paper is based on the transaction flow processing power of ERP. It integrates the database and online analytical processing (OLAP) technologies, transaction and decision making flows of ERP, then uses data searching technology to integrate decision making and forecast flows. The data-searching engine is the actual design of a categorizing module. This research can finish within an hour the analyses and forecasts, which would take the traditional information technology system about three weeks to process the mock analyses. This makes decision making easier for entrepreneurs and thus enables them to handle well the always-changing market and establish themselves firmly. This research applies the ERP data searching system on the IC testing industry. Using the ERP data searching system in this study to investigate the testing status of the testing machine, resolutions are found pinpointing the factors that cause the high frequency of machine down. The results of, according to the actual experiment, the tested testing machine shows an average improvement rate of 83% over the number of machines down.
As firms begin to implement web-based presentation and data mining tools to enhance decision support capability, the firm's knowledge workers must determine how to most effectively use these new web-based tools to deliver competitive advantage. The focus of this study is on evaluating how knowledge workers integrate these tools into their information and knowledge management requirements. The relationship between the independent variables (web-based data mining software tools and business models) and the dependent variable (strategic performance capabilities) is empirically tested in this study. The results from this study demonstrate the positive interaction effect between the tools and models application on strategic performance capability. © 2002 Elsevier Science B.V. All rights reserved.
This paper reports on the findings of a research project that had the objective to build a decision support system to handle customer insolvency for a large telecommunication company. Prediction of customer insolvency, well in advance, and with an accuracy that could make this prediction useful in business terms, was one of the core objectives of the study. In the paper the process of building such a predictive model through knowledge discovery and data mining techniques in vast amounts of heterogeneous as well as noisy data is described. The reported findings are very promising, making the proposed model a useful tool in the decision making process, while some of the discussed problems and limitations are of interest to researchers who intend to use data mining approaches in other similar real-life problems. © 2002 Elsevier Science B.V. All rights reserved.
As the economy has tightened, retailers have been challenged in recent years to be more strategic in their planning. They struggle to find answers to: • Who can I consider a loyal customer? • What kind of marketing strategy is most likely to increase sales? • What can customer-purchasing patterns reveal about improving inventory control? • What is the most effective way to manage customer relations to increase revenues? (Rabinovitch, 1999). With the exponential growth in the amount of data being collected, improvements in technology, and research in machine learning, retailers are now able to reduce the ever growing difficult and complex decision making process by recruiting the efforts of data mining (Barry & Linoff, 1997). Data mining is a computerized technology that uses complicated algorithms to find relationships and trends in large data bases, real or perceived, previously unknown to the retailer, to promote decision support. Currently being utilized by such retail giants as Federated Department Stores, Nordstrom, and Wal-Mart, Inc., data mining is touted to be one of the greatest technologies to hit the retailing industry this decade (Rabinovitch, 1999). The purpose of this study is to critique data mining technology in comparison with more familiar analytical tools for strategic decision making by small to medium size retailers. The context for this study includes current and future industry applications and practices for research performed in data mining applications within the retail sector.
Business performance is an important concern for all profit organizations including surveying firms. Based on the data from 179 UK surveying firms, a data-driven path analysis is conducted to examine the direct and indirect effects of eight independent variables on business performance (the dependent variable). These are organization structure, environmental uncertainty, interconnectedness, size of firm, marketing performance, market orientation, management and strategy. The results suggest that only the marketing performance variable has a strong and significant direct effect on business performance. In turn, marketing performance is largely dependent on the firm's degree of market orientation. The finding in this study reinforces the postulation in existing literature that a market orientation is essential for sustaining competitive advantage in business firms. As such, the implication for surveying firms is that the stronger the market orientation of the firm, the better would be the business performance.

They help identify and predict individual, as well as aggregate, behavior, as illustrated by four application domains: direct mail, retail, automobile insurance, and health care.
The goal is business effectiveness through 'verticalization,' usability, and integration with operational systems.
Mining potentially useful information from large database becomes increasingly important in both research and application in many fields. Because of the complex fabrication processes and the cost resulted from defects, it is critical to identify possible faults through examining the failure patterns of wafer bin-maps. However, little research has been done to develop methods for clustering and classifying wafer bin-maps. We first used spatial statistics to examine the independence among failed dies and thus classified the bin maps into four categories: random failure pattern, systematic repulsion failure pattern, systematic attraction failure pattern, and others. For WBM with systematic attraction failure patterns, we developed an ART neural network for clustering to assist the product engineers in narrowing possible causes of manufacturing defects. We used empirical data from a fab for demonstration and the results showed the practical viability of this approach for its consistency and efficiency. This study concludes with discussions and remarks on future research.
Recent advances in computers and networking technologies and a fast growing internet community have created immense distributed databases located miles away that have the ability to be updated continuously without the knowledge of the possible and prospective users. The ability to collect and store all kinds of data has outpaced the capabilities of an individual to analyze, summarize, and extract knowledge from the data. Traditional methods of data analysis, based mainly on the analysts dealing directly with the data, are no longer the best alternative to be used. Although the database technology provided the basic tools for efficient storage and lookup for large data sets, the issues of how to enable engineers to understand large bodies of data remains a difficult problem. Recently, data mining approaches based on artificial neural networks, fuzzy logic, machine learning, statistics, expert systems, and data visualization have created new intelligent tools for automated data mining and knowledge discovery. All these changes will have a profound impact on current practices used in manufacturing. The way that bills of materials are created, products are designed, and process plans are generated, will definitely be different with the availability of this new technology. In this paper the nature of these changes and their implications on product design and manufacturing are discussed and the basic issues regarding the design of Smart Data Mining Systems are examined with examples.
As global competition continues to strengthen in the semiconductor industry, wafer fabs have been placing increasing importance on increasing die yield and reducing operation costs. Because of automatic manufacturing and information integration technologies, an increasingly large amount of raw data has been accumulated from various sources automatically or semi-automatically from day to day. Mining potentially useful information from large such database becomes very important in both research and application. However, little research has been done on manufacturing data of high-tech industry. In particular, due to the complex fabrication processes and the high cost of defects, using data mining approach to diagnosing defects in semiconductor manufacturing is a critical issue. We constructed a conceptual framework for data mining, proposed two methods for mining WAT data, and then applied them empirically in a fab. The results show the practical viability to assist the domain engineer in narrowing possible causes of manufacturing defects. This study concludes with discussions and remarks on future research directions.
Data Mining, can be defined as the art of extracting non-obvious, useful information from large databases. This emerging field brings a set of powerful techniques which are relevance for companies to focus their efforts in taking advantage of their data warehouses. In this paper we analyse the basic ideas behind Data Mining and we pay special attention to the use of neural networks as an advanced statistical tool. We also present two real world applications of these techniques to stock market predictions and to the study of fire propagation in cables.

Recent advances related to on-line analytical processing (OLAP) have resulted in a significant improvement in data analysis efficiency by virtue of its multidimensional database structure and pre-computing operations of measuring data. However, the research related to the design and implementation of OLAP, particularly in the support of dispersed manufacturing networks in terms of 'intelligent decision making', has yet to be considered as remarkable. Research studies indicate that the level of intelligence of decision support systems can be enhanced with the incorporation of computational intelligence techniques such as case-based reasoning or rule-based reasoning. This paper describes the development of an intelligent data-mining system using a rule-based OLAP approach which can be adopted to support dispersed manufacturing networks in terms of performance enhancement. In this paper, the techniques, methods and infrastructure for the development of such a data-mining system, which possesses certain intelligent features, are presented. To validate the feasibility of this approach, a case example related to the testing of the approach in an emulated industrial environment is covered.
The growing volume of information poses interesting challenges and calls for tools that discover properties of data. Data mining has emerged as a discipline that contributes tools for data analysis, discovery of new knowledge, and autonomous decision-making. In this paper, the basic concepts of rough set theory and other aspects of data mining are introduced. The rough set theory offers a viable approach for extraction of decision rules from data sets. The extracted rules can be used for making predictions in the semiconductor industry and other applications. This contrasts other approaches such as regression analysis and neural networks where a single model is built. One of the goals of data mining is to extract meaningful knowledge. The power, generality, accuracy, and longevity of decision rules can be increased by the application of concepts from systems engineering and evolutionary computation introduced in this paper. A new rule-structuring algorithm is proposed. The concepts presented in the paper are illustrated with examples.
The researchers and practitioners of today create models, algorithms, functions, and other constructs defined in abstract spaces. The research of the future will likely be data driven. Symbolic and numeric data that are becoming available in large volumes will define the need for new data analysis techniques and tools. Data mining is an emerging area of computational intelligence that offers new theories, techniques, and tools for analysis of large data sets. In this paper, a novel approach for autonomous decision-making is developed based on the rough set theory of data mining. The approach has been tested on a medical data set for patients with lung abnormalities referred to as solitary pulmonary nodules (SPNs). The two independent algorithms developed in this paper either generate an accurate diagnosis or make no decision. The methodology discussed in the paper depart from the developments in data mining as well as current medical literature, thus creating a variable approach for autonomous decision-making.

This paper focuses on the strategic role and the implementation of textual data mining (TDM) in government organizations, with special emphasis on TDM to support the management of science and technology (S and T). It begins by defining TDM, and discussing the strategic management process in federal government organizations and the role of TDM as an integral part of this process. The paper then proceeds to describe some of the uses and applications of TDM. The results of a demonstration program by the US Office of Naval Research show some potential benefits from TDM: (1) integration of national and multi-national S and T databases
Data mining is a new buzz word in managed care. More than simply a method of unlocking a vault of useful information in MCO data banks and warehouses, the author believes that it can help steer an organization through the reengineering process, leading to the health system's transformation toward a customer-focused organization.
Efficient mining of data presents a significant challenge due to problems of combinatorial explosion in the space and time often required for such processing. While previous work has focused on improving the efficiency of the mining algorithms, we consider how the representation, organization, and access of the data may significantly affect performance, especially when I/O costs are also considered. By a simple analysis and comparison of the counting stage for the Apriori association rules algorithm, we show that a 'column-wise' approach to data access is often more efficient than the standard row-wise approach. We also provide the results of empirical simulations to validate our analysis. The key idea in our approach is that counting in the Apriori algorithm with data accessed in a column-wise manner significantly reduces the number of disk accesses required to identify itemsets with a minimum support in the database - primarily by reducing the degree to which data and counters need to be repeatedly brought into memory.
Data mining and knowledge discovery offer benefits to manufacturing engineering by extracting strategic and useful knowledge buried in databases. Access to such knowledge adds value to a manufacturing enterprise by rapidly discovering new rules for quality/process control, defect prevention, and safety. This paper provides an overview of data mining and knowledge discovery, their applications to manufacturing engineering, technical and deployment issues, and a case study.
This paper describes a visual model that gives a perceptual distortion measure between an input image and that of reference based on a human-image representational model. We study an approach in which once a few active recognizers tuned to significant orientation and spatial-frequency components of the reference spectrum are obtained, any input image to be compared with the reference one is passed through an operator designated to compare its excitation levels given by the active recognizers, to the corresponding excitation levels for the reference image. Hence, the distortion between a pair of complex images is measured as the weighted sum of the distortion in each filter of a bank of strongly responding recognizers, each tuned to a certain 2D spatial-frequency data in the reference picture, with the weighting of each filter modulating its amplitude response. © 1998 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved.
Many processes in the industrial realm exhibit stochastic and nonlinear behavior. Consequently, an intelligent system must be able to adapt to nonlinear production processes as well as probabilistic phenomena. To this end, an intelligent manufacturing system may draw on techniques from disparate fields, involving knowledge in both explicit and implicit form. In order for a knowledge based system to control a manufacturing process, an important capability is that of prediction: forecasting the future trajectory of a process as well as the consequences of the control action. This paper presents a comparative study of explicit and implicit methods to predict nonlinear chaotic behavior. The evaluated models include statistical procedures as well as neural networks and case based reasoning. The concepts are crystallized through a case study in the prediction of chaotic processes adulterated by various patterns of noise. © 1997 Elsevier Science Ltd.
This paper describes a software tool, DBMine, developed to assist industrial engineers in data mining. This tool implements three common data mining methodologies: Bacon's algorithm, Decision Trees and DB-Learn. Implemented in Microsoft Visual Basic 3.0©, DBMine, can utilize data in Microsoft Access 2.0© and in Watcom SQL© databases. This paper will also present an example session in which job shop sequences produced by a Genetic Algorithm are explored for regularity. © 1997 Elsevier Science Ltd.
Data mining is being touted as having potential for discovering information that will contribute toward resolving business problems or creating business opportunities. However, before this potential can be realized, one needs to be able to relate the strategic objectives of a business to the use of the data mining technology. In this paper we focus on one aspect of that requirement, the translation of managerial goals into parameters of the algorithm being used to analyze the data. The first step in the process is to define a mapping from managerial goals to the performance measures of the algorithm. We then propose and apply an experimental approach using a classification algorithm, Probabilistic Inductive Learning, to develop a mapping between the performance measures and the parameters of the algorithm. The experimental results are analyzed and guidelines are given for setting the bias of the algorithm to meet managerial goals. We conclude by recommending that an experimental analysis similar to what we proposed be conducted for all data mining algorithms and provided along with documentation as an aid to the user. © 1997 Elsevier Science B.V.



This paper describes an architecture and search organization for continuous speech recognition. The recognition module is part of the SPICOS system for the understanding of data base queries spoken in natural language. The recognition is based on statistical decision theory and thus amounts to an integrated approach that combines all available knowledge sources, such as inventory of subword units, pronunciation lexicon, and language model, and attempts to avoid local decisions during the process of acoustic recognition. The recognition decision amounts to a time-synchronous, left-to-right search through a large state space with delayed decisions. The recognized word sequence is then the best interpretation of the observed acoustic data within the constraints as given by the knowledge sources. The organization of the search can be viewed as an extension of the one-pass dynamic programming algorithm for connected word recognition. In continuous speech recognition, however, the search space is much larger, and an efficient organization of the search process is called for in order to keep the organization overhead as small as possible. In this paper, we present such an efficient search organization with the following characteristics. Its computational cost is proportional only to the number of hypotheses actually generated and is independent of the overall size of the potential search space. There is no limit to the number of word hypotheses, there is only a limit to the overall number of hypotheses due to storage constraints. The implementation of the search has been tested on a continuous speech data base comprising up to 4000 words for each of several speakers. In particular, the efficiency and robustness of the search organization has been checked and evaluated along many dimensions, such as different speakers, phoneme models, and language models. © 1992 IEEE
Despite the apparent move toward using data-driven simulators in manufacturing modelling, as opposed to simulation languages and packages that require programming, there have been few rational efforts to evaluate the development and use of these tools. As the number of tools continues to grow, such evaluation is necessary if simulation users are going to make sensible informed choices. This paper presents two specialized data-driven simulators developed to model Flexible Manufacturing Systems called RENSAM (Rensselaer Simulator for Automated Manufacturing) and RENVIS (Rensselaer Visual Interactive Simulator). Experience with these packages leads to consideration of the benefits of using such tools. Advantages include the ease with which models can be developed and the rapid pace of that development, and the enforcement of proper statistics collection

EMERGE is an expert system designed as a medical decision making aid. It is machine-independent, and is implemented in standard Pascal. It has modest memory requirements, and can operate on a microcomputer. EMERGE is rule-based, and its initial application is the analysis of chest pain in the emergency room. The knowledge base is maintained separately from the consultation program. Thus the application area can be changed without any modification to the software. This paper describes the control structures and rule searching procedures used in EMERGE. © 1984 IEEE.
