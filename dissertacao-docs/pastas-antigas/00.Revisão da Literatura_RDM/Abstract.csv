Link,Abstract
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994745056&partnerID=40&md5=64023d71ea668e479a674d89e4cb7521","Probabilistic Risk Assessment (PRA) has proven to be an invaluable tool for evaluating risks in complex engineered systems. However, there is increasing concern that PRA may not be adequate in situations with little underlying knowledge to support probabilistic representation of uncertainties. As analysts and policy makers turn their attention to deeply uncertain hazards such as climate change, a number of alternatives to traditional PRA have been proposed. This paper systematically compares three diverse approaches for risk analysis under deep uncertainty (qualitative uncertainty factors, probability bounds, and robust decision making) in terms of their representation of uncertain quantities, analytical output, and implications for risk management. A simple example problem is used to highlight differences in the way that each method relates to the traditional risk assessment process and fundamental issues associated with risk assessment and description. We find that the implications for decision making are not necessarily consistent between approaches, and that differences in the representation of uncertain quantities and analytical output suggest contexts in which each method may be most appropriate. Finally, each methodology demonstrates how risk assessment can inform decision making in deeply uncertain contexts, informing more effective responses to risk problems characterized by deep uncertainty. © 2016 Elsevier Ltd"
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994779131&partnerID=40&md5=1b669a6ac6050d9b904d02e0005d6578","Ontology-based activity recognition is gaining interest due to its expressiveness and comprehensive reasoning mechanism. An obstacle to its wider use is that the imperfect observations result in failure of recognizing activities. This paper proposes a novel reasoning algorithm for activity recognition in smart environments. The algorithm integrates OWL ontological reasoning mechanism with Dempster–Shafer theory of evidence to provide support for handling uncertainty in activity recognition. It quantifies uncertainty while aggregating contextual information and provides a degree of belief that facilitates more robust decision making in activity recognition. The presented approach has been implemented and evaluated on an internal and public datasets and compared with a data-driven approach that is using hidden Markov model. Results have shown that the proposed reasoning approach can accommodate uncertainties and subsequently infer the activities more accurately in comparison with existing ontology-based recognition and perform comparably well to the data-driven approach. © 2016 Elsevier B.V."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991486853&partnerID=40&md5=26fc79946a7d3716de0496f9626f2cc7","A variety of model-based approaches for supporting decision-making under deep uncertainty have been suggested, but they are rarely compared and contrasted. In this paper, we compare Robust Decision-Making with Dynamic Adaptive Policy Pathways. We apply both to a hypothetical case inspired by a river reach in the Rhine Delta of the Netherlands, and compare them with respect to the required tooling, the resulting decision relevant insights, and the resulting plans. The results indicate that the two approaches are complementary. Robust Decision-Making offers insights into conditions under which problems occur, and makes trade-offs transparent. The Dynamic Adaptive Policy Pathways approach emphasizes dynamic adaptation over time, and thus offers a natural way for handling the vulnerabilities identified through Robust Decision-Making. The application also makes clear that the analytical process of Robust Decision-Making is path-dependent and open ended: an analyst has to make many choices, for which Robust Decision-Making offers no direct guidance. © 2016 The Authors"
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992371589&partnerID=40&md5=029b232a0826b001b8f3907d86f16af8","Each actor evaluating potential management strategies brings her/his own distinct set of objectives to a complex decision space of system uncertainties. The diversity of these objectives and uncertainties requires detailed and rigorous analyses that respond to multifaceted challenges. The utility of this information depends on the accessibility of scientific information to decision makers. This paper demonstrates data visualization tools for presenting scientific results to decision makers in two case studies, La Paz/El Alto, Bolivia, and Yuba County, California. Visualization output from the case studies combines spatiotemporal, multivariate and multirun/multiscenario information to produce information corresponding to the objectives and uncertainties described by key actors. These tools can manage complex data and distill scientific information into accessible formats. Using the visualizations, scientists and decision makers can navigate the decision space and potential objective trade-offs to facilitate discussion and consensus building. These efforts can help identify stable negotiated agreements between different stakeholders. © 2016"
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988493689&partnerID=40&md5=d9a681ca374e627ae1650ec861934c6f","Automated planning and reinforcement learning are characterized by complementary views on decision making: the former relies on previous knowledge and computation, while the latter on interaction with the world, and experience. Planning allows robots to carry out different tasks in the same domain, without the need to acquire knowledge about each one of them, but relies strongly on the accuracy of the model. Reinforcement learning, on the other hand, does not require previous knowledge, and allows robots to robustly adapt to the environment, but often necessitates an infeasible amount of experience. We present Domain Approximation for Reinforcement LearnING (DARLING), a method that takes advantage of planning to constrain the behavior of the agent to reasonable choices, and of reinforcement learning to adapt to the environment, and increase the reliability of the decision making process. We demonstrate the effectiveness of the proposed method on a service robot, carrying out a variety of tasks in an office building. We find that when the robot makes decisions by planning alone on a given model it often fails, and when it makes decisions by reinforcement learning alone it often cannot complete its tasks in a reasonable amount of time. When employing DARLING, even when seeded with the same model that was used for planning alone, however, the robot can quickly learn a behavior to carry out all the tasks, improves over time, and adapts to the environment as it changes. © 2016 Elsevier B.V."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988928468&partnerID=40&md5=ee7ae8588d934c7ca3066abb97a8584b","Uncertainty Quantification (UQ) employing a Monte Carlo Sampling (MCS) method in a building simulation domain has been widely used to account for risks of predicted outputs for robust decision making. However, the stochastic approach for UQ problems requires significant computational burdens compared to the deterministic approach. This paper addresses two surrogate models (Gaussian Process Emulator (GPE) and Polynomial Chaos Expansion (PCE)) which together can be regarded as a meta-model of a Building Performance Simulation (BPS) tool with a high-fidelity model. In the paper, the developed GPE and PCE with different model structures were compared in terms of a prediction capability under different amount of training data and number of inputs. The aim of the comparative study is to identify the relative prediction abilities and model flexibility of GPE and PCE. It was found that the GPE and PCE produce high performance qualities having fast computation speed compared to the developed basis model if new inputs having identical inputs and probability ranges were used. In terms of two-sample Kolmogorov-Smirnov (K-S) hypothesis test, mean values of the minimum p-values of the GPE and PCE were 0.999 and 0.569, respectively, if the number of samplings are over 30 cases. Otherwise, the PCE shows significantly reduced performance quality than the GPE. © 2016"
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991712842&partnerID=40&md5=954fb67088f187ca86bc10602da051d2","Grade Engineering® spans a range of operational techniques that exploits intrinsic grade variability to remove low grade uneconomic material prior to energy intensive and inefficient grinding. Grade Engineering provides an additional level of operational flexibility while also incurring complexity that needs to be managed for an effective operational deployment. An integrated value driven methodology has been developed to manage this complexity by means of stochastic optimisation. This allows the optimum Grade Engineering processing “recipe” to be determined that maximises value per unit of time that can be drawn from a production volume under a set of user defined constraints. The introduction of uncertainty in the stochastic optimisation problem enables the assessment of the risk and operating robustness, both essential in robust decision-making processes. The case study discussed in the paper comprises a large open cut copper porphyry deposit for which two Grade Engineering strategies are assessed: differential blasting for grade, and preferential grade by size response. These size-based coarse separation levers are subsequently exploited through a Grade Engineering circuit. This comprises a set of screens and crushers, with a configuration and operating settings defined by the Grade Engineering recipe. The methodology developed demonstrated that size-based Grade engineering is a robust operating option that can effectively deliver significant improvements in unit metal productivity. © 2016 Elsevier Ltd"
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995745554&partnerID=40&md5=6454fd3bb9b807f3e218477019a202f9","Expansion of today’s underwater scenarios and missions necessitates the requisition for robust decision making of the autonomous underwater vehicle (AUV); hence, design an efficient decision-making framework is essential for maximizing the mission productivity in a restricted time. This paper focuses on developing a deliberative conflict-free-task assignment architecture encompassing a global route planner (GRP) and a local path planner (LPP) to provide consistent motion planning encountering both environmental dynamic changes and a priori knowledge of the terrain, so that the AUV is reactively guided to the target of interest in the context of an uncertain underwater environment. The architecture involves three main modules: The GRP module at the top level deals with the task priority assignment, mission time management, and determination of a feasible route between start and destination point in a large-scale environment. The LPP module at the lower level deals with safety considerations and generates collision-free optimal trajectory between each specific pair of waypoints listed in obtained global route. Re-planning module tends to promote robustness and reactive ability of the AUV with respect to the environmental changes. The experimental results for different simulated missions demonstrate the inherent robustness and drastic efficiency of the proposed scheme in enhancement of the vehicles autonomy in terms of mission productivity, mission time management, and vehicle safety. © 2016 Springer-Verlag Berlin Heidelberg"
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994275147&partnerID=40&md5=885bce95bcdd4e7f6e4ef795867c10ce","In the face of deeply uncertain climate change projections, robust decision frameworks are becoming a popular tool for incorporating climate change uncertainty into water infrastructure planning. These methodologies have the potential to be particularly valuable in developing countries where extensive infrastructure development is still needed and uncertainties can be large. However, many applications of these methodologies have relied on a sophisticated process of climate model downscaling and impact modeling that may be unreliable in data-scarce contexts. In this study, we demonstrate a modified application of the robust decision making (RDM) methodology that is specifically tailored for application in data-scarce situations. This modification includes a novel method for generating transient climate change sequences that account for potential variable dependence but do not rely on detailed GCM projections, and an emphasis on identifying the relative importance of data limitations and uncertainty within an integrated modeling framework. We demonstrate this methodology in the Lake Tana basin in Ethiopia, showing how the approach can highlight the vulnerability of alternative plans across different time scales and identify priorities for research and model refinement. We find that infrastructure performance is particularly sensitive to uncertainty in streamflow model accuracy, irrigation efficiency, and evaporation rates, suggesting that additional research in these areas could provide valuable insights for long-term infrastructure planning. This work demonstrates how tailored application of robust decision frameworks using simple modeling approaches can provide decision support in data-scarce regions where more complex modeling and analysis may be impractical. © 2016 Springer Science+Business Media Dordrecht"
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985990145&partnerID=40&md5=c0117416dcb16b1ad351bb5239b4249e","This article presents an exploratory modelling approach that illustrates how overall transition pathways can emerge from a limited number of underlying change patterns. Pathways describe the temporal development of transitions, they are trajectories of change that carry societal systems such as health care, energy supply or water management into qualitatively different states. Under any given input scenario, a very large number of different pathways may result due to uncertainties such as those related to human agency. Though the pathways all differ in detail, clusters of pathways share enough qualitative similarities to allow identification of a small number of ideal types: many roads to Rome. The input scenario influences how often the various types of futures emerge, not what types emerge. The article explores this using a series of hypothetical cases and compares the results with ideal-typical pathways from the literature. A historical case is simulated for illustration. © 2016 Elsevier Ltd"
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994759269&partnerID=40&md5=14c5c164ac7e35f7db185dc16c78e442","This paper presents a robust satisficing decision-making method for Unmanned Aerial Vehicles (UAVs) executing complex missions in an uncertain environment. Motivated by the info-gap decision theory, we formulate this problem as a novel robust satisficing optimization problem, of which the objective is to maximize the robustness while satisfying some desired mission requirements. Specifically, a new info-gap based Markov Decision Process (IMDP) is constructed to abstract the uncertain UAV system and specify the complex mission requirements with the Linear Temporal Logic (LTL). A robust satisficing policy is obtained to maximize the robustness to the uncertain IMDP while ensuring a desired probability of satisfying the LTL specifications. To this end, we propose a two-stage robust satisficing solution strategy which consists of the construction of a product IMDP and the generation of a robust satisficing policy. In the first stage, a product IMDP is constructed by combining the IMDP with an automaton representing the LTL specifications. In the second, an algorithm based on robust dynamic programming is proposed to generate a robust satisficing policy, while an associated robustness evaluation algorithm is presented to evaluate the robustness. Finally, through Monte Carlo simulation, the effectiveness of our algorithms is demonstrated on an UAV search mission under severe uncertainty so that the resulting policy can maximize the robustness while reaching the desired performance level. Furthermore, by comparing the proposed method with other robust decision-making methods, it can be concluded that our policy can tolerate higher uncertainty so that the desired performance level can be guaranteed, which indicates that the proposed method is much more effective in real applications. © 2016 Ji et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986880428&partnerID=40&md5=b83ca3f0823788083a0ae4921da30bf7","While the shale revolution was largely a US’ affair, it affects the global energy system. In this paper, we look at the effects of this spectacular increase in natural gas, and oil, extraction capacity can have on the mix of primary energy sources, on energy prices, and through that on internal political stability of rentier states. We use two exploratory simulation models to investigate the consequences of the combination of both complexity and uncertainty in relation to the global energy system and state stability. Our simulations show that shale developments could be seen as part of a long term hog-cycle, with a short term drop in oil prices if unconventional supply substitutes demand for oil. These lower oil prices may lead to instability in rentier states neighbouring the EU, especially when dependence on oil and gas income is high, youth bulges are present, or buffers like sovereign wealth funds are too limited to bridge the negative economic effects of temporary low oil prices. © 2016 Elsevier Ltd"
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994474392&partnerID=40&md5=1b0db52b0a87d1cbb71026b1a592a896","Exploratory simulation allows analysts to discover scenarios in which existing or planned water supplies may fail to meet stakeholder objectives. These robustness assessments rely heavily on the choice of plausible future scenarios, which, in the case of drought management, requires sampling or generating a streamflow ensemble that extends beyond the historical record. This study develops a method to modify synthetic streamflow generators by increasing the frequency and severity of droughts for the purpose of exploratory modeling. To support management decisions, these synthetic droughts can be related to recent observed droughts of consequence for regional stakeholders. The method approximately preserves the spatial and temporal correlation of historical streamflow in drought-adjusted scenarios. The approach is demonstrated in a bottom-up planning context using an urban water portfolio design problem in North Carolina, a region whose water supply faces both climate and population pressures. Synthetic scenarios are used to simulate the implications for reliability and cost if events with similar severity to the recent 2007-2008 drought become more frequent under climate change, and in general, the system-level consequences of increasingly frequent and/or severe droughts. Finally, synthetically generated drought extremes are compared with runoff projections derived from downscaled climate model output, serving to support bottom-up robustness methods in water systems planning. © 2016 American Society of Civil Engineers."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978394264&partnerID=40&md5=a05924dada4918d240c774a3d2b54fea","Scenario discovery is a model-based approach to scenario development under deep uncertainty. Scenario discovery relies on the use of statistical machine learning algorithms. The most frequently used algorithm is the Patient Rule Induction Method (PRIM). This algorithm identifies regions in an uncertain model input space that are highly predictive of model outcomes that are of interest. To identify these regions, PRIM uses a hill-climbing optimization procedure. This suggests that PRIM can suffer from the usual defects of hill climbing optimization algorithms, including local optima, plateaus, and ridges and valleys. In case of PRIM, these problems are even more pronounced when dealing with heterogeneously typed data. Drawing inspiration from machine learning research on random forests, we present an improved version of PRIM. This improved version is based on the idea of performing multiple PRIM analyses based on randomly selected features and combining these results using a bagging technique. The efficacy of the approach is demonstrated using three cases. Each of the cases has been published before and used PRIM. We compare the results found using PRIM with the results found using the improved version of PRIM. We find that the improved version is more robust to new data, can better cope with heterogeneously typed data, and is less prone to overfitting. © 2016 The Authors"
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979085712&partnerID=40&md5=d3e424c2afa328a6cc6c5f7706253af8","Computer simulation models can generate large numbers of scenarios, far more than can be effectively utilized in most decision support applications. How can one best select a small number of scenarios to consider? One approach calls for choosing scenarios that illuminate vulnerabilities of proposed policies. Another calls for choosing scenarios that span a diverse range of futures. This paper joins these two approaches for the first time, proposing an optimization-based method for choosing a small number of relevant scenarios that combine both vulnerability and diversity. The paper applies the method to a real case involving climate resilient infrastructure for three African river basins (Volta, Orange and Zambezi). Introducing selection criteria in a stepwise manner helps examine how different criteria influence the choice of scenarios. The results suggest that combining vulnerability- and diversity-based criteria can provide a systematic and transparent method for scenario selection. © 2016"
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995948067&partnerID=40&md5=6b9a3167f83f8b794b8803963df2ffef","Understanding hydrological model predictive capabilities under contrasting climate conditions enables more robust decision making. Using Differential Split Sample Testing (DSST), we analyze the performance of six hydrological models for 37 Irish catchments under climate conditions unlike those used for model training. Additionally, we consider four ensemble averaging techniques when examining interperiod transferability. DSST is conducted using 2/3 year noncontinuous blocks of (i) the wettest/driest years on record based on precipitation totals and (ii) years with a more/less pronounced seasonal precipitation regime. Model transferability between contrasting regimes was found to vary depending on the testing scenario, catchment, and evaluation criteria considered. As expected, the ensemble average outperformed most individual ensemble members. However, averaging techniques differed considerably in the number of times they surpassed the best individual model member. Bayesian Model Averaging (BMA) and the Granger-Ramanathan Averaging (GRA) method were found to outperform the simple arithmetic mean (SAM) and Akaike Information Criteria Averaging (AICA). Here GRA performed better than the best individual model in 51%–86% of cases (according to the Nash-Sutcliffe criterion). When assessing model predictive skill under climate change conditions we recommend (i) setting up DSST to select the best available analogues of expected annual mean and seasonal climate conditions; (ii) applying multiple performance criteria; (iii) testing transferability using a diverse set of catchments; and (iv) using a multimodel ensemble in conjunction with an appropriate averaging technique. Given the computational efficiency and performance of GRA relative to BMA, the former is recommended as the preferred ensemble averaging technique for climate assessment. © 2016. American Geophysical Union. All Rights Reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988723564&partnerID=40&md5=51a965218f49bcaaeea51e396f9895e7","The consideration of climate resilience in power system planning and operations by utilities around the world is very limited to date. This article assimilates some of the initial thoughts developed as part of a World Bank project on climate resilience for Bangladesh. It briefly reflects on the current literature, and focuses on the specific flooding risks faced in Bangladesh to illuminate the way forward to enhance planning practices. © 2016 Elsevier Inc."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988514062&partnerID=40&md5=b0b6fe3253beb5a1dd6120c083c86e46","Electricity systems are constantly exposed to geopolitical, techno-economic and natural uncertainties that may endanger security of supply. Therefore, it is crucial that policy makers concerned about it consider a variety of possible futures — and not only the one that is perceived as the most likely. In particular, they should account for the possibility of sudden shocks in their decisions with the goal of making the system more “robust”. However, long-term power system models which are an important pillar of policy decision making are typically designed to determine the cost-minimal power system for a specific expected future; such a system is not necessarily the most robust one. By combining the classic investment optimization approach with the tools of Robust Decision Making we analyze the viability of different strategies that may potentially increase the robustness of a power system. For the case of the European power system we pursue a dedicated analysis with the European power system model LIMES-EU. Based on a total of more than 40,000 model runs, we find that strategies promoting the ability of countries to always produce at least 95% of their electricity demand domestically significantly help to reduce the loss of load in case of shocks. Such a strategy is not cost-optimal for the expected future without shocks; but the additional costs (about 0.1% of total system costs) are low compared to the benefits of significantly increasing the power system's robustness. © 2016 Elsevier B.V."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951777028&partnerID=40&md5=58165820f1b77b275c931bd2d92afff0","Business process models which are usually constructed by business designers from experience and analysis are the main guidelines for services composition in the service-oriented architecture (SOA) applications development. However, due to the complexity of business models, it is a challenging task for business process designers to optimize the process models dynamically in accordance with changes in business environments. In this paper, a process-mining-based method is proposed to support business process designers to monitor efficiency or capture the changes of a business process. Firstly, we define a scenario model to depict business elements and their relationships which are critical to business process design. Based on the proposed scenario model, process mining algorithms, including control flow mining, roles mining and data flow mining are carried out in a certain sequence synthetically to extract business scenarios from event logs recorded by SOA application systems. Finally, we implement a prototype using a logistic scenario to illustrate the feasibility of our method in SOA applications development. © 2015, Springer-Verlag London."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978252830&partnerID=40&md5=4b72d65f2e9db5a8dc6192fa7da7ce68","Advances in computing technology, new and ongoing restoration initiatives, concerns about climate change's effects, and the increasing interdisciplinarity of research have encouraged the development of landscape-scale mechanistic models of coupled ecological-geophysical systems. However, communication barriers and uneven infiltration of new strategies for data-driven induction persist in the context of simulation model development across disciplines. One challenge is that ecology and the geosciences have embraced different modeling epistemologies, with ecologists historically favoring inductive inference from generalized, phenomenological models and geoscientists favoring deductive inference from detailed first-principles models. Today, many models used for environmental management, particularly for aquatic ecosystems, tend to be highly detailed, with ecological and geophysical components represented in different modules that are linked but often not closely integrated. These observations highlight a need for cross-disciplinary dialogue about landscape-scale modeling objectives and approaches. The philosophies of pattern-oriented modeling in ecology and exploratory modeling in geophysics have yielded advances in theoretical and applied knowledge in both of those disciplines, but they are not comprehensive across all aspects of landscape-scale modeling. Here we define and synthesize the “Appropriate-Complexity Method” (ACME), which builds upon these two philosophies to guide the development of process-oriented models across a spectrum of scientific and management objectives. ACME helps modelers efficiently converge upon an optimal modeling structure through: i) systematic evaluation of the attributes that comprise computational and representational detail, for which we have developed an operational decision tree; ii) iterative adjustment of models based on pattern-oriented model evaluation strategies; and iii) the use of appropriate datasets (where applicable) to build conceptual models and formulate predictions. Decisions about aspects of computational and representational detail are based on the landscape's emergent properties. They are also based on a hierarchy of classes of questions governing model objectives that represent a multi-attribute tradeoff among validation potential, interpretability, tractability, and generality as functions of computational and representational detail. Tradeoff curves, together with model objectives, provide further guidance for determining the “appropriate” level of complexity for representation of processes in models. Once deemed adequate for addressing the original research question of interest, models may be used for projection and scenario testing. They may next undergo expansion that moves them down the hierarchy, where they can then be used to address research questions of higher specificity, detail, and validation potential, though at a cost of lower tractability and interpretability on the tradeoff curves. This practical, systematic procedure provides clear guidance for the design and improvement of landscape models that may be used to address a wide variety of questions relevant to restoration, over a spectrum of scales. © 2016 The Authors"
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991094139&partnerID=40&md5=76fce25f7c6d2726fc90d4490dbb27ff","The rapid socio-economic development and expanding human-induced hydrological alteration have strengthened the interactions between the social and hydrologic systems. To assess regional water supply security under changing water supply and demand condition in strongly human-impacted area, an integrated water resources management model that fully incorporates water demand prediction, optimal water resources allocation and water supply risk analysis is proposed and applied in the mid-lower reach of Hanjiang River basin. The model is run under three scenarios considering increasing water demand and expanding water diversion projects, and then spatial and temporal distributions of water supply reliability and vulnerability are evaluated. Results show that water supply risk in the mid-lower reach of Hanjiang River basin, especially units that take water directly from the mainstream, will be gradually enlarged in the future due to the expansions of both water demand and inter-basin water diversion capacity. The proposed method provides a practical approach towards more robust decision-making of long-term water resources planning and management under changing environment. © 2016 by the authors."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84950970778&partnerID=40&md5=d82473b5bb3e003cc4c1169e3e1dad30","The global container transport system is changing quickly. Ports can be severely affected by these changes; therefore, ports need insight into how the system might change and what the impact of this will be on their competitive position. Given the intrinsic complexity of the container transport system and the presence of a wide range of deeply uncertain factors affecting the system, we use an exploratory modeling approach to study future scenarios for the global container network. Using scenario discovery and worst-case discovery, we assess the implications of various uncertain factors on the competitive position of the port of Rotterdam. It is found that overall the competitive position of Rotterdam is quite robust with respect to the various uncertain factors. The main vulnerability is the quality of the hinterland connections. A modest deterioration of the quality of the hinterland connections, resulting in increased travel time, will result in a loss of throughput for Rotterdam. © 2015 Elsevier Ltd"
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978139041&partnerID=40&md5=04912f04005c135b2429e25c3613c1db","A general framework is introduced for modeling pharmacodynamic processes that are subject to autoregulation, which combines the indirect response (IDR) model approach with methods from classical feedback control of engineered systems. The canonical IDR models are modified to incorporate linear combinations of feedback control terms related to the time course of the difference (the error signal) between the pharmacodynamic response and its basal value. Following the well-established approach of traditional engineering control theory, the proposed feedback control indirect response models incorporate terms proportional to the error signal itself, the integral of the error signal, the derivative of the error signal or combinations thereof. Simulations are presented to illustrate the types of responses produced by the proposed feedback control indirect response model framework, and to illustrate comparisons with other PK/PD modeling approaches incorporating feedback. In addition, four examples from literature are used to illustrate the implementation and applicability of the proposed feedback control framework. The examples reflect each of the four mechanisms of drug action as modeled by each of the four canonical IDR models and include: selective serotonin reuptake inhibitors and extracellular serotonin; histamine H2-receptor antagonists and gastric acid; growth hormone secretagogues and circulating growth hormone; β2-selective adrenergic agonists and potassium. The proposed feedback control indirect response approach may serve as an exploratory modeling tool and may provide a bridge for development of more mechanistic systems pharmacology models. © 2016, Springer Science+Business Media New York."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979939227&partnerID=40&md5=76c3f6ef62677e9d995ac3d263e503f3","IN AUGUST 2015, Professor John H. Holland passed away in Ann Arbor, MI, where he had served on the University of Michigan faculty for more than 50 years. John, as he was known universally to his colleagues and students, leaves behind a long legacy of intellectual achievements. As a descendant of the cybernetics era, he was influenced by the work of John von Neumann, Norbert Wiener, W. Ross Ashby, and Alan Turing, all of whom viewed computation as a broad, interdisciplinary enterprise. Holland thus became an early proponent of interdisciplinary approaches to computer science and an active evangelist of what is now called computational thinking, reaching out enthusiastically to psychologists, economists, physicists, linguists, philosophers, and pretty much anyone he came in contact with. As a result, even though he received what was arguably one of the world's first computer science Ph.D. degrees in 1959,23 his contributions are sometimes better known outside computer science than within. Holland is best known for his invention of genetic algorithms (GAs), a family of search and optimization methods inspired by biological evolution. Since their invention in the 1960s, GAs have inspired many related methods and led to the thriving field of evolutionary computation, with widespread scientific and commercial applications. Although the mechanisms and applications of GAs are well known, they were only one offshoot of Holland's broader motivation-to develop a general theory of adaptation in complex systems. Here, we consider this larger framework, sketching the recurring themes that were central to Holland's theory of adaptive systems: discovery and dynamics in adaptive search; internal models and prediction; exploratory modeling; and universal properties of complex adaptive systems."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978717702&partnerID=40&md5=995ed5931a08c585e860d41330c42d11","UK logistics fleets face increasing competitive pressures due to volatile fuel prices and the small profit margins in the industry. By reducing fuel consumption, operational costs and carbon emissions can be reduced. While there are a number of technologies that can reduce fuel consumption, it is often difficult for logistics companies to identify which would be the most beneficial to adopt over the medium and long terms. With a myriad of possible technology combinations, optimising the vehicle specification for specific duty cycles requires a robust decision-making framework. This paper combines simulated truck and delivery routes with a metaheuristic evolutionary algorithm to select the optimal combination of low-carbon technologies that minimise the greenhouse gas emissions of long-haul heavy goods vehicles during their lifetime cost. The framework presented is applicable to other vehicles, including road haulage, waste collection fleets and buses by using tailored parameters in the heuristics model. © 2016 Informa UK Limited, trading as Taylor & Francis Group"
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978062297&partnerID=40&md5=42b154f6d6c6d2f383c64be327b6f6f9","Purpose: GSK2647544 is a potent and specific inhibitor of lipoprotein-associated phospholipase A2 (Lp-PLA2), which was in development as a potential treatment for Alzheimer’s disease (AD). In order to refine therapeutic dose predictions and confirm brain penetration, a radiolabelled form of the inhibitor, [18F]GSK2647544, was manufactured for use in a positron emission tomography (PET) biodistribution study. Procedures: [18F]GSK2647544 was produced using a novel, copper iodide (Cu(I)) mediated, [18F]trifluoromethylation methodology. Healthy male subjects (n = 4, age range 34–42) received an oral dose of unlabelled GSK2647544 (100 mg) and after 2 h an intravenous (iv) injection of [18F]GSK2647544 (average injected activity and mass were 106 ± 47 MBq and 179 ± 55 μg, respectively) followed by dynamic PET scans for 120 min. Defined regions of interest (ROI) throughout the brain were used to obtain regional time-activity curves (TACs) and compartmental modelling analysis used to estimate the primary outcome measure, whole brain volume of distribution (VT). Secondary PK and safety endpoints were also recorded. Results: PET dynamic data were successfully obtained from all four subjects and there were no clinically significant variations of the safety endpoints. Inspection of the TACs indicated a relatively homogenous uptake of [18F]GSK2647544 across all the ROIs examined. The mean whole brain VT was 0.56 (95 % CI, 0.41–0.72). Secondary PK parameters, Cmax (geometric mean) and Tmax (median), were 354 ng/ml and 1.4 h, respectively. Metabolism of GSK2647544 was relatively consistent across subjects, with 20–40 % of the parent compound [18F]GSK2647544 present after 120 min. Conclusions: The study provides evidence that GSK2647544 is able to cross the blood brain barrier in healthy male subjects leading to a measurable brain exposure. The administered doses of GSK2647544 were well tolerated. Exploratory modelling suggested that a twice-daily dose of 102 mg, at steady state, would provide ~80 % trough inhibition of brain Lp-PLA2 activity. Trial Registration: Clintrials.gov: NCT01924858. © 2016 The Author(s)"
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954289299&partnerID=40&md5=e3360f16e7bdee175c8b4046faf43dfb","Ever since its appearance on the scene in the 1940s, in the then nascent field of statistical decision theory, Wald's maximin paradigm has played a vital role in many disciplines in the treatment of nonprobabilistic uncertainty, both as a tool of thought and as a practical instrument. In fact, in some fields, such as modern robust optimization, Wald's paradigm dominates the scene. It is important to note that this paradigm's preeminence in many fields continues unabated, despite its obvious limitations and the criticism that had been and continues to be leveled at it. So, in this tutorial we examine the methodological aspects of this stalwart of decision theory from the viewpoint of robust decision-making, paying special attention to its obvious and not so obvious limitations and to its relation to other maximin paradigms. Â© 2016 The Authors."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975859162&partnerID=40&md5=cac3ea0191f47f7861a791ae73d12e2d","We adapted a published model to estimate the costs and benefits of screening men who have sex with men for syphilis, including the benefits of preventing syphilis-attributable human immunodeficiency virus. The cost per quality-adjusted life year gained by screeningwas <US $0 (cost-saving) and US $16,100 in the dynamic and static versions of the model, respectively. Copyright © 2016 American Sexually Transmitted Diseases Association. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84980325780&partnerID=40&md5=610078321cd5b616c6ec183daf0156ff","The default mode network (DMN) is a set of regions that is tonically engaged during the resting state and exhibits task-related deactivation that is readily reproducible across a wide range of paradigms and modalities. The DMN has been implicated in numerous disorders of cognition and, in particular, in disorders exhibiting age-related cognitive decline. Despite these observations, investigations of the DMN in normal aging are scant. Here, we used blood oxygen level dependent (BOLD) functional magnetic resonance imaging (fMRI) acquired during rest to investigate age-related changes in functional connectivity of the DMN in 120 healthy normal volunteers comprising six, 20-subject, decade cohorts (from 20-29 to 70-79). Structural equation modeling (SEM) was used to assess age-related changes in inter-regional connectivity within the DMN. SEM was applied both using a previously published, meta-analytically derived, node-and-edge model, and using exploratory modeling searching for connections that optimized model fit improvement. Although the two models were highly similar (only 3 of 13 paths differed), the sample demonstrated significantly better fit with the exploratory model. For this reason, the exploratory model was used to assess age-related changes across the decade cohorts. Progressive, highly significant changes in path weights were found in 8 (of 13) paths: Four rising, and four falling (most changes were significant by the third or fourth decade). In all cases, rising paths and falling paths projected in pairs onto the same nodes, suggesting compensatory increases associated with age-related decreases. This study demonstrates that age-related changes in DMN physiology (inter-regional connectivity) are bidirectional, progressive, of early onset and part of normal aging. © 2016 Li, Laird, Price, McKay, Blangero, Glahn and Fox."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978977087&partnerID=40&md5=108cd14a32ff0f50a4c0c19553ea191e","Developing model-based narratives of society’s response to climate change is challenged by two factors. First, society’s response to possible future climate change is subject to many uncertainties. Second, we argue that society’s mitigation action emerge out of the actions and interactions of the many actors in society. Together, these two factors imply that the overarching dynamics of society’s response to climate change are unpredictable. In contrast to conventional processes of developing scenarios, in this study the emergence of climate change mitigation action by society has been represented in an agent-based model with which we developed two narratives of the emergence of climate change mitigation action by applying exploratory modelling and analysis. The agent-based model represents a two-level game involving governments and citizens changing their emission behaviour in the face of climate change through mitigation action. Insights gained from the exploration on uncertainties pertaining to the system have been used to construct two internally consistent and plausible narratives on the pathways of the emergence of mitigation action, which, as we argue, are a reasonable summary of the uncertainty space. The first narrative highlights how and when strong mitigation action emerges while the second narrative highlights how and when weak mitigation action emerges. In contrast to a conventional scenario development process, these two scenarios have been discovered bottom up rather than being defined top down. They succinctly capture the possible outcomes of the emergence of climate change mitigation by society across a large range of uncertain factors. The narratives therefore help in conveying the consequences of the various uncertainties influencing the emergence of climate change mitigation action by society. © 2016, University of Surrey. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977943161&partnerID=40&md5=5b1d66cea54d67abccfac4a1ae8ffed3","The new scenario framework developed by the climate change research community rests on the fundamental logic that a diversity of socio-economic pathways can lead to the same radiative forcing, and therefore that a given level of radiative forcing can have very different socio-economic impacts. We propose a methodology that implements a ""scenario discovery"" cluster analysis and systematically identifies diverse groups of scenarios that share common outcomes among a database of socio-economic scenarios. We demonstrate the methodology with two examples using the Shared Socio-economic Pathways framework. We find that high emissions scenarios can be associated with either high or low per capita GDP growth, and that high productivity growth and catch-up are not necessarily associated with high per capita GDP and high emissions. © 2016 Elsevier Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978531510&partnerID=40&md5=29079a311a55f0b8307b7fc8c19f1905","Boolean models are gaining an increasing interest for reproducing dynamic behaviours, understanding processes, and predicting emerging properties of cellular signalling networks through in-silico experiments. They are emerging as a valid alternative to the quantitative approaches (i.e., based on ordinary differential equations) for exploratory modelling when little is known about reaction kinetics or equilibrium constants in the context of gene expression or signalling. Even though several approaches and software have been recently proposed for logic modelling of biological systems, they are limited to specific modelling contexts and they lack of automation in analysing biological properties such as complex attractors, molecule vulnerability, dose response. This paper presents a design and verification platform based on SystemC that applies methodologies and tools well established in the electronic-design automation (EDA) field such as assertion-based verification (ABV) and mutation analysis, which allow complex attractors (i.e., protein oscillations) and robustness/sensitivity of the signalling networks to be simulated and analysed. The paper reports the results obtained by applying such verification techniques for the analysis of the intracellular signalling network controlling integrin activation mediating leukocyte recruitment from the blood into the tissues. © 2016 IEEE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974678582&partnerID=40&md5=c01ff300a6f87572cbc40c882aaba650","The physical dimensions of standard cells constrain the dimensions of power networks, affecting the on-chip power noise. An exploratory modeling methodology is presented for estimating power noise in advanced technology nodes. The models are evaluated for 14, 10, and 7 nm technologies to assess the impact on performance. Scaled technologies are shown to be more sensitive to power noise, resulting in potential loss of performance enhancements achieved by scaling. Stripes between local track rails is evaluated as a means to reduce power noise, exhibiting up to 56.5% improvement in power noise for the 7 nm technology node. A strong dependence on the width of a stripe is observed, indicating that fewer wide stripes are more favorable then many thin stripes. As a promising alternative material for power network interconnects, graphene is shown to exhibit good potential in reducing power noise. The effects of different scaling scenarios of local power rails on power noise are also discussed. © 2016 ACM."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969752485&partnerID=40&md5=25a6c69b6ad57569cedfa4ce9b79da0a","In this study we present the structure and implementation of a model-based inquiry teaching–learning sequence (TLS) integrating expressive, experimental and exploratory modelling pedagogies in a cyclic manner, with the aim of enhancing primary education student teachers’ epistemological beliefs about the aspects, nature, purpose and change of models as well as their conceptual understanding of light phenomena related to properties of optical fibres. The subjects were 16 prospective primary teachers involved in modelling activities, employing both hands-on experiments and computer modelling activities, based on the application of the ray model. Student teachers were tested before and after the implementation of the TLS by semi-structured interviews and a written questionnaire. Results show that before the TLS most students adopted epistemologically naïve realistic beliefs about models, whereas after the TLS there was an overall significant transition from naïve to more sophisticated epistemological beliefs, as well as significant improvements in their conceptual knowledge about light phenomena. Nevertheless, the relation between epistemological beliefs and conceptual understanding seems to be aspect-dependent, so our evidence suggests that more educational effort is required in order to establish a coherent relationship between them. © 2016 Informa UK Limited, trading as Taylor & Francis Group."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960118746&partnerID=40&md5=5c704c4b2fc5fe5ec54bdb34371e6482","Scenario discovery is a novel model-based approach to scenario development in the presence of deep uncertainty. Scenario discovery frequently relies on the Patient Rule Induction Method (PRIM). PRIM identifies regions in the model input space that are highly predictive of producing model outcomes that are of interest. To identify these, PRIM uses a lenient hill climbing optimization procedure. PRIM struggles when confronted with cases where the uncertain factors are a mix of data types, and can be used only for binary classifications. We compare two more lenient objective functions which both address the first problem, and an alternative objective function using Gini impurity which addresses the second problem. We assess the efficacy of the modification using previously published cases. Both modifications are effective. The more lenient objective functions produce better descriptions of the data, while the Gini impurity objective function allows PRIM to be used when handling multinomial classified data. © 2015 The Authors."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958522250&partnerID=40&md5=68db05423f7c14b3b2b96104519f1127","Sensitivity Analysis (SA) investigates how the variation in the output of a numerical model can be attributed to variations of its input factors. SA is increasingly being used in environmental modelling for a variety of purposes, including uncertainty assessment, model calibration and diagnostic evaluation, dominant control analysis and robust decision-making. In this paper we review the SA literature with the goal of providing: (i) a comprehensive view of SA approaches also in relation to other methodologies for model identification and application; (ii) a systematic classification of the most commonly used SA methods; (iii) practical guidelines for the application of SA. The paper aims at delivering an introduction to SA for non-specialist readers, as well as practical advice with best practice examples from the literature; and at stimulating the discussion within the community of SA developers and users regarding the setting of good practices and on defining priorities for future research. © 2016 The Authors."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959900245&partnerID=40&md5=83d1973c3e62b25701ece21baca11132","Static input-oriented sampling approaches are often used for generating model-based scenarios. However, for models of deeply uncertain and dynamically complex issues, there is no guarantee that such approaches reveal the total behavioral spectrum that could be generated by simulating them. In this paper, we present an adaptive output-oriented sampling approach for exploring the full behavioral spectrum that could be generated by computational models in view of generating interesting, even previously undiscovered, scenarios. In this paper, we use a resource scarcity model to illustrate the approach, show the difference between static sampling and adaptive sampling, and demonstrate the usefulness for scenario discovery of the latter combined with other methods. We show that this approach can be used for revealing the behavioral spectrum of models, identifying regions of the input space that generate particular behaviors, and selecting (sets of) scenarios that are representative in terms of output and input spaces. © 2015 Elsevier Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968677712&partnerID=40&md5=9d7ab48aca4e3102dfd1b60e0cb45a3b","As the interest in functional connectivity continues to increase among neuroimaging researchers there becomes a greater need to develop an objective method of network identification. The current paper offers a solution to this problem by developing a robust decision making algorithm that can extract a target neural network from an array of spatial maps. We used a probabilistic independent component analysis to generate spatial maps of the Default Mode Network (DMN); however, this adaptive pipeline can be applied to any network of interest. Different template matching algorithms including: Normalized Cross-Correlation, Sum of Squared Differences and Dice Coefficient, were applied to the spatial and frequency domains of the dataset to identify the components that shared the greatest similarity to our DMN template. After identifying components within the resting state, the decision making pipeline selected the components within each method that had the highest matching scores to our DMN template. The final decision of selecting the most prototypical DMN components was made by a comparison between methods. This resulted in a DMN mask that was generated by the components chosen by our decision-making algorithm. To evaluate the accuracy of the decision-maker, a cross-correlation between each final mask and the template was measured. Results indicated that the Normalized Cross Correlation method, using both the spatial and frequency domain, and the Dice Coefficient method, generated the optimal DMN mask. This demonstrates the utility of our algorithm in providing an objective method for network extraction. © 2016 IEEE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957088452&partnerID=40&md5=76d241f04bb8e1dcc4537bdd48f623c4","Vehicle to Grid technologies utilize idle EV battery power as a grid storage tool to meet fluctuating electric power demands. Vehicle to Grid systems are promising substitutes for traditional gas turbine generators, which are relatively inefficient and have high emissions impacts. The purpose of this study is to predict the future net revenue and life cycle emissions savings of Vehicle to Grid technologies for use in ancillary (regulation) services on a regional basis in the United States. In this paper, the emissions savings and net revenue calculations are conducted with respect to five different Independent System Operator/Regional Transmission Organization regions, after which future EV market penetration rates are predicted using an Agent-Based Model designed to account for various uncertainties, including regulation service payments, regulation signal features, and battery degradation. Finally, the concept of Exploratory Modeling and Analysis is used to estimate the future net revenue and emissions savings of integrating Vehicle to Grid technology into the grid, considering the inherent uncertainties of the system. The results indicate that, for a single vehicle, the net revenue of Vehicle to Grid services is highest for the New York region, which is approximately $42,000 per vehicle on average. However, the PJM region has an approximately $97 million overall net revenue potential, given the 38,200 Vehicle to Grid-service-available electric vehicles estimated to be on the road in the future in the PJM region, which is the highest among the studied regions. © 2016 Elsevier Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961288888&partnerID=40&md5=66a5a29b269d2e75269c7ddb386295ee","As cities increasingly engage in climate adaptation planning, many are seeking to promote public participation and facilitate the engagement of different civil society actors. Still, the variations that exist among participatory approaches and the merits and tradeoffs associated with each are not well understood. This article examines the experiences of Quito (Ecuador) and Surat (India) to assess how civil society actors contribute to adaptation planning and implementation. The results showcase two distinct approaches to public engagement. The first emphasizes participation of experts, affected communities, and a wide array of citizens to sustain broadly inclusive programmes that incorporate local needs and concerns into adaptation processes and outcomes. The second approach focuses on building targeted partnerships between key government, private, and civil society actors to institutionalize robust decision-making structures, enhance abilities to raise funds, and increase means to directly engage with local community and international actors. A critical analysis of these approaches suggests more inclusive planning processes correspond to higher climate equity and justice outcomes in the short term, but the results also indicate that an emphasis on building dedicated multi-sector governance institutions may enhance long-term programme stability, while ensuring that diverse civil society actors have an ongoing voice in climate adaptation planning and implementation. Policy relevance Many local governments in the Global South experience severe capacity and resource constraints. Cities are often required to devolve large-scale planning and decision-making responsibilities, such as those critical to climate adaptation, to different civil society actors. As a result, there needs to be more rigorous assessments of how civil society participation contributes to the adaptation policy and planning process and what local social, political, and economic factors dictate the way cities select different approaches to public engagement. Also, since social equity and justice are key indicators for determining the effectiveness and sustainability of adaptation interventions, urban adaptation plans and policies must also be designed according to local institutional strengths and civic capacities in order to account for the needs of the poor and most vulnerable. Inclusivity, therefore, is critical for ensuring equitable planning processes and just adaptation outcomes. © 2015 Taylor & Francis."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954329166&partnerID=40&md5=705d287dd380ee1f7a9331296ca50fc7","Robust decision-making is being increasingly used to support environmental resources decisions and policy analysis under changing climate and society. In this context, a robust decision is a decision that is as much as possible insensitive to a large degree of uncertainty and ensures certain performance across multiple plausible futures. Yet, the concept of robustness is neither unique nor static. Multiple robustness metrics, such as maximin, optimism-pessimism, max regret, have been proposed in the literature, reflecting diverse optimistic/pessimistic attitudes by the decision maker. Further, these attitudes can evolve in time as a response to sequences of favorable (or adverse) events, inducing possible dynamic changes in the robustness metrics. In this paper, we explore the impact of alternative definitions of robustness and their evolution in time for a case of water resources system management under changing climate. We study the decisions of the Lake Como operator, who is called to regulate the lake by balancing irrigation supply and flood control, under an ensemble of climate change scenarios. Results show a considerable variability in the system performance across multiple robustness metrics. In fact, the mis-definition of the actual decision maker’s attitude biases the simulation of its future decisions and produces a general underestimation of the system performance. The analysis of the dynamic evolution of the decision maker’s preferences further confirms the potentially strong impact of changing robustness definition on the decision-making outcomes. Climate change impact assessment studies should therefore include the definition of robustness among the uncertain parameters of the problem in order to analyze future human decisions under uncertainty. © 2016, Springer Science+Business Media Dordrecht."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961912390&partnerID=40&md5=3b5c0300c8ec6dfbe393699641682a9a","Fault tree analysis is a method largely used in probabilistic risk assessment. Uncertainties should be properly handled in fault tree analyses to support a robust decision making. While many sources of uncertainties are considered, dependence uncertainties are not much explored. Such uncertainties can be labeled as 'epistemic' because of the way dependence is modeled. In practice, despite probability theory, alternative mathematical structures, including possibility theory and fuzzy set theory, for the representation of epistemic uncertainty can be used. In this article, a fuzzy β factor is considered to represent the failure dependence uncertainties among basic events. The relationship between β factor and system failure probability is analyzed to support the use of a hybrid probabilistic-possibilistic approach. As a result, a complete hybrid probabilistic-possibilistic framework is constructed. A case study of a high integrity pressure protection system is discussed. The results show that the proposed method provides decision makers a more accurate understanding of the system under analysis when failure dependencies are involved. © 2015 John Wiley & Sons, Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960462047&partnerID=40&md5=73e229d9c8ec1f42e46742fe62ebf1b0","There is a lot of rhetoric suggesting that disaster risk reduction (DRR) pays, yet surprisingly little in the way of hard facts. This review paper examines the evidence regarding the economic efficiency of DRM based on CBA. Specifically, it addresses the following questions: What can be said about current and best practice regarding CBA for DRR including limitations and alternatives? And, what, if at all, can be said in terms of quantitative insight for informing policy and practice? The review compares the documented evidence on the net benefits over a range of disaster management interventions, such as risk reduction, preparedness and risk financing. The review also critically discusses the applicability of cost–benefit analysis as well as other economic decision-supporting tools for assessing the efficiency of certain DRM interventions. Disaster risk is characterized by variability, which requires a risk-based assessment. As a key best practice criterion, and in order to avoid overestimating the benefits and returns on investment, the review focuses on studies that provide a risk-based estimate of benefits. This review shows that for the limited evidence reported the economic case for DRM across a range of hazards is strong and that the benefits of investing in DRM outweigh the costs of doing so on average, by about four times the cost in terms of avoided and reduced losses. In an age of austerity, cost–benefit analysis continues to be an important tool for prioritizing efficient DRM measures but with a shifting emphasis from infrastructure-based options (hard resilience) to preparedness and systemic interventions (soft resilience), other tools such as cost-effectiveness analysis, multi-criteria analysis and robust decision-making approaches deserve more attention. © 2016, The Author(s)."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84965139286&partnerID=40&md5=cb0f485055497d6accf17b58915b42ef","A number of recent studies have investigated differences in human choice behavior depending on task framing, especially comparing economic decision-making to choice behavior in equivalent sensorimotor tasks. Here we test whether decision-making under ambiguity exhibits effects of task framing in motor vs. non-motor context. In a first experiment, we designed an experience-based urn task with varying degrees of ambiguity and an equivalent motor task where subjects chose between hitting partially occluded targets. In a second experiment, we controlled for the different stimulus design in the two tasks by introducing an urn task with bar stimuli matching those in the motor task. We found ambiguity attitudes to be mainly influenced by stimulus design. In particular, we found that the same subjects tended to be ambiguity-preferring when choosing between ambiguous bar stimuli, but ambiguity-avoiding when choosing between ambiguous urn sample stimuli. In contrast, subjects' choice pattern was not affected by changing from a target hitting task to a nonmotor context when keeping the stimulus design unchanged. In both tasks subjects' choice behavior was continuously modulated by the degree of ambiguity. We show that this modulation of behavior can be explained by an information-theoretic model of ambiguity that generalizes Bayes-optimal decision-making by combining Bayesian inference with robust decision-making under model uncertainty. Our results demonstrate the benefits of information-theoretic models of decision-making under varying degrees of ambiguity for a given context, but also demonstrate the sensitivity of ambiguity attitudes across contexts that theoretical models struggle to explain. © 2016, Public Library of Science. All rights reserved. This is an open access article, free of all copyright, and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. The work is made available under the Creative Commons CC0 public domain dedication."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955474448&partnerID=40&md5=73c78aca3634f5a5a875f237ed6e1169","To improve applicability of life cycle assessment (LCA) in supporting direct and robust decision-making, an integrated approach was developed through incorporating operational research and uncertainty analysis methods within a general LCA framework. The methodology can (a) help comprehensive evaluation of environmental impacts at multiple product-service levels, (b) facilitate the reflections of multiple LCA associated uncertainties and transfer them into consequential decision-making process, and (c) identify desired water allocation schemes for minimizing life-cycle environmental impacts. This represented an improvement upon conventional LCA method, as well as water resources allocation. The developed method was then verified in a water-stressed city (i.e., the City of Dalian), northeastern China. The application indicated that the proposed method was effective in generating desired water supply schemes under uncertainties, reflecting the associated life-cycle environmental impacts, and strengthening capabilities of both LCA and operational research methods. The results also indicated that the top three contributors for life-cycle environmental impacts would be districts of Pulandian and Zhuanghe, and Municipal zone of the city. © 2016 Elsevier B.V. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959295395&partnerID=40&md5=1e467b45aff60240234fa4b715ff5cc7","In ecosystems driven by water availability, plant community dynamics depend on complex interactions between vegetation, hydrology, and human water resources use. Along ephemeral rivers-where water availability is erratic-vegetation and people are particularly vulnerable to changes in each other's water use. Sensible management requires that water supply be maintained for people, while preserving ecosystem health. Meeting such requirements is challenging because of the unpredictable water availability. We applied information gap decision theory to an ecohydrological system model of the Kuiseb River environment in Namibia. Our aim was to identify the robustness of ecosystem and water management strategies to uncertainties in future flood regimes along ephemeral rivers. We evaluated the trade-offs between alternative performance criteria and their robustness to uncertainty to account for both (i) human demands for water supply and (ii) reducing the risk of species extinction caused by water mining. Increasing uncertainty of flood regime parameters reduced the performance under both objectives. Remarkably, the ecological objective (species coexistence) was more sensitive to uncertainty than the water supply objective. However, within each objective, the relative performance of different management strategies was insensitive to uncertainty. The 'best' management strategy was one that is tuned to the competitive species interactions in the Kuiseb environment. It regulates the biomass of the strongest competitor and, thus, at the same time decreases transpiration, thereby increasing groundwater storage and reducing pressure on less dominant species. This robust mutually acceptable strategy enables species persistence without markedly reducing the water supply for humans. This study emphasises the utility of ecohydrological models for resource management of water-controlled ecosystems. Although trade-offs were identified between alternative performance criteria and their robustness to uncertain future flood regimes, management strategies were identified that help to secure an ecologically sustainable water supply. © 2016 John Wiley & Sons, Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987736300&partnerID=40&md5=aaad02a46a54a6c4b745caa9dbcfb6a2","This paper describes a new rocky shore profile evolution model. It differs from existing models that focus on shore platform development, through its ability to simulate a wide range of emergent profile shapes. The new model considers a relatively limited number of coastal processes that are represented with simplified mathematical descriptions. The paper describes initial model evaluation against field data from the Japanese coast that were influential in the creation of a widely used conceptual model for rocky shore evolution. The new numerical model produces outputs that are generally consistent with the field observations, but there are some areas of difference which are discussed in the paper. An additional area of focus in the paper considers the new model's capability of producing a wide range of profile types with characteristic geometries, which are also commonly seen in nature, but have been subject to little systematic analysis. Investigation of model outcomes indicates that a broad range of profile shapes in the model requires consideration of the erosive effects of varied wave types (i.e. unbroken, breaking and broken waves). © Coastal Education and Research Foundation, Inc. 2016."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954482916&partnerID=40&md5=71e2d8680b0fa2c154ef24ce83a84d9a","Applying standard decision-making processes such as cost-benefit analysis in an area of high uncertainty such as climate change adaptation is challenging. While the costs of adaptation might be observable and immediate, the benefits are often uncertain. The limitations of traditional decision-making processes in the context of adaptation decisions are recognised, and so-called robust approaches are increasingly explored in the literature. Robust approaches select projects that meet their purpose across a variety of futures by integrating a wide range of climate scenarios, and are thus particularly suited for deep uncertainty. We review real option analysis, portfolio analysis, robust-decision making and no/low regret options as well as reduced decision-making time horizons, describing the underlying concepts and highlighting a number of applications. We discuss the limitations of robust decision-making processes to identify which ones may prove most promising as adaptation planning becomes increasingly critical; namely those that provide a compromise between a meaningful analysis and simple implementation. We introduce a simple framework identifying which method is suited for which application. We conclude that the 'robust decision making' method offers the most potential in adaptation appraisal as it can be applied with various degrees of complexity and to a wide range of options. © 2016 Elsevier B.V."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951862110&partnerID=40&md5=54cc8460f7b1ff8220bfe41d0d951350","In the European Union, the building sector accounts for more than 40% of the total energy consumption and environmental impacts, representing the area with the greatest potential for intervention. In addition to the existing policies that promote energy efficiency in buildings, the embodied energy and the environmental impacts contained in the building materials should be considered. In the case of the construction of insulation façade systems, the environmental implications are different depending on the type of façade system, the insulation materials used and the location of the building. This article aims to provide all of this information for Spain, including not only the production of the components of the façade system but also the installation phase and the transport to the building site. The results show that the most impactful alternative is the ventilated façade combined with the most impactful insulation materials of stone wool and expanded polystyrene. Meanwhile, the most advisable façade in all of the climate zones is the external thermal insulation system combined with any type of insulation. The environmental impacts of insulation materials are very different. Moreover, it is recommended that further studies complete these results with the economic and social implications of the use and maintenance phases for robust decision-making. © 2015 Elsevier Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957076548&partnerID=40&md5=13eac538f2eaa321c8ed037306f6d9a8","One of the most promising strategies recommended for increasing energy security and for mitigating transportation sector emissions is to support alternative fuel technologies, including electric vehicles. However, there is a considerable amount of uncertainty regarding the market penetration of electric vehicles that must be accounted for in order to achieve the current market share goals. This paper aims to address these inherent uncertainties and to identify the possible market share of electric vehicles in the United States for the year 2030, using the developed Electric Vehicle Regional Market Penetration tool. First, considering their respective inherent uncertainties, the vehicle attributes are evaluated for different vehicle types, including internal combustion engine, gasoline hybrid, and three different electric vehicle types. In addition, an agent-based model is developed to identify the market shares of each of the studied vehicles. Finally, market share uncertainties are modeled using the Exploratory Modeling and Analysis approach. The government subsidies play a vital role in the market adoption of electric vehicle and, when combined with the word-of-mouth effect, may achieve electric vehicle market share of up to 30% of new sales in 2030 on average, with all-electric vehicles having the highest market share among the electric vehicle options. © 2015 Elsevier Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975709121&partnerID=40&md5=b908afccb6a473dc56cce3b6209f8f7f","There is increasing concern over deep uncertainty in the risk analysis field as probabilistic models of uncertainty cannot always be confidently determined or agreed upon for many of our most pressing contemporary risk challenges. This is particularly true in the climate change adaptation field, and has prompted the development of a number of frameworks aiming to characterize system vulnerabilities and identify robust alternatives. One such methodology is robust decision making (RDM), which uses simulation models to assess how strategies perform over many plausible conditions and then identifies and characterizes those where the strategy fails in a process termed scenario discovery. While many of the problems to which RDM has been applied are characterized by multiple objectives, research to date has provided little insight into how treatment of multiple criteria impacts the failure scenarios identified. In this research, we compare different methods for incorporating multiple objectives into the scenario discovery process to evaluate how they impact the resulting failure scenarios. We use the Lake Tana basin in Ethiopia as a case study, where climatic and environmental uncertainties could impact multiple planned water infrastructure projects, and find that failure scenarios may vary depending on the method used to aggregate multiple criteria. Common methods used to convert multiple attributes into a single utility score can obscure connections between failure scenarios and system performance, limiting the information provided to support decision making. Applying scenario discovery over each performance metric separately provides more nuanced information regarding the relative sensitivity of the objectives to different uncertain parameters, leading to clearer insights on measures that could be taken to improve system robustness and areas where additional research might prove useful. © 2016 Society for Risk Analysis."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992217822&partnerID=40&md5=4e26e36de16227be3154a64ecc21d207","Climate change adaptation is unavoidable, particularly in developing countries where the adaptation deficit is often larger than in developed countries. Robust Decision Making (RDM) approaches are considered useful for supporting adaptation decision making, yet case study applications in developing countries are rare. This review paper examines the potential to expand the geographical and sectoral foci of RDM as part of the repertoire of approaches to support adaptation. We review adaptation decision problems hitherto relatively unexplored, for which RDM approaches may have value. We discuss the strengths and weaknesses of different approaches, suggest potential sectors for application and comment on future directions. We identify that data requirements, lack of examples of RDM in actual decision-making, limited applicability for surprise events, and resource constraints are likely to constrain successful application of RDM approaches in developing countries. We discuss opportunities for RDM approaches to address decision problems associated with urban socio-environmental and water-energy-food nexus issues, forest resources management, disaster risk management and conservation management issues. We examine potential entry points for RDM approaches through Environmental Impact Assessments and Strategic Environmental Assessments, which are relatively well established in decision making processes in many developing countries. We conclude that despite some barriers, and with modification, RDM approaches show potential for wider application in developing country contexts. © 2016 The Authors"
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981501196&partnerID=40&md5=17a7e80ed26a7972943e5f7ff32b355e","Social media is seen very much as a marketing tool and there is little in the literature that considers its use as a strategic decision making tool. This conceptual paper is an attempt to redress the balance. Social media user-generated content from blogs or consumer feedback are methods that social media can support effective strategic decision making. However, the business and organisational environments are influential on the effective of the data collected and ultimately its analysis. The decision making approach - single or multistage are significant influencers on the quality of the decisions. Multistage decision making is supportive of controversial decision making, which leads to better utilisation of the information and consequently, better decision making. Ultimately, robust decision making is underpinned by the effectiveness of the decision making process. © 2016 by IGI Global. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953350016&partnerID=40&md5=acb3d2ed9a6e09c95f8ea36c3c3a3afa","Adaptation is heterogeneous and relevant for a range of sectors and levels of decision-making. As adaptation moves up the policy agenda, solution-oriented adaptation research requires addressing questions that are salient to stakeholders and decision-makers at various scales and involves applying a wide range of different methods. Yet while solution-oriented adaptation research is being increasingly undertaken, there is to date a lack of synthesis of these experiences in the literature. In this paper, we aim to address this gap by synthesising findings in nine cases from the MEDIATION project (Methodology for Effective Decision-making on Impacts and AdaptaTION), an EC-funded solution-oriented adaptation research project. We do so by, first, describing methods applied for solution-oriented research in Europe and sequences of methods carried out in individual cases. Second, we assess strengths and weaknesses of individual methods in given empirical situations. Third, we analyse patterns observed in the sequences of methods and reflect on their implications for adaptation research. A strength of our approach is that detailed data on choices of research questions and methods were collected through in-depth and iterative interaction with the case study teams. We find that there is no standard recipe for adaptation; that even though social science methods are often indicated, they are often not applied; and that robust decision-making methods, while available, are often constrained because of their resource intensity. Reflecting on the implications of these findings, we argue that greater flexibility and transdisciplinarity are needed in adaptation research and that social science methods should be further supported. Finally, we find that stakeholder engagement is not a panacea and that engagement requires a more differentiated understanding of stakeholders and careful design in order to be effective. © 2015, Springer-Verlag Berlin Heidelberg."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985953092&partnerID=40&md5=342cb6cf8db9f4dfcada708496c67a1c","Commercial air transport has become very safe over the last decades in part, due to the advances made in airplane technology. However, in the particular case of non-normal situations, the flight crew actions are essential for the overall resilience of the system. The European Seventh Framework research project Man4Gen studies the circumstances in which flight crews are more prone to lose situation awareness, which is required for robust decision making in challenging and unexpected situations. The study described in this paper investigates pilot behavior during a challenging and unexpected operational scenario conducted in two different research simulators. Results showed that flight crews with above average performance communicated effectively and exhibited good qualities in several core competencies such as leadership and teamwork, problem solving and decision making, among others. On the other hand, poor-performing crews had difficulties during low-workload situations and revealed poor manual flying skills. The results seem to indicate that competency-based training can be a good methodology to train resiliency in flight crews. This, however, requires validation from future studies. © 2016 by NLR. Published by the American Institute of Aeronautics and Astronautics, Inc."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978204067&partnerID=40&md5=34d8f1e5a09a9faba1feb6f53f15e75d","This chapter proposes a conceptual framework for implementing exemplary academic integrity policy (the elements of which were identified by the Australian Office for Learning and Teaching (OLT)-funded Academic Integrity Standards Project [2010—2012]: access, approach, responsibility, detail, and support) to assist higher education providers improve academic integrity at their institutions. At the center of the framework is a commitment to a culture of academic integrity. The follow-up OLT-funded Exemplary Academic Integrity Project [2012—2013] identified six components which contribute to the development of this culture, including academic integrity champions, academic integrity education for staff and students, robust decision-making systems, record keeping for evaluation, and regular review of policy and process. The framework emphasizes a paradigm shift from misconduct to integrity and recognizes that academic integrity champions initiate and lead change, working with students as partners. It is recommended that the role of academic integrity breach data be broadened to include evaluation for improving educational practice. © Springer Science+Business Media Singapore 2016."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978424178&partnerID=40&md5=793fa54c41ba31e702647e270157ead1","As attested by the prevalence of worst-case-based robustness analysis in many fields,Wald’s maximin paradigm (circa 1940) plays a central role in the broad area of decision-making under uncertainty. The objective of this chapter is therefore twofold. First, to examine the basic conceptual and modeling aspects of this ostensibly intuitive, yet controversial paradigm, so as to clarify some of the issues involved in its deployment in decision-making in the face of a non-probabilistic uncertainty. Second, to elucidate the differences between this paradigm and other maximin paradigms, such as those used in error analysis and game theory. We thereby chart the journey of this paradigm from the field of statistical decision theory to that of modern robust optimization, highlighting its use in the latter, as a tool for dealing with both local and global robustness. We also look briefly at the relationship between probabilistic and worst-case-based robustness analysis. © Springer International Publishing Switzerland 2016."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976906537&partnerID=40&md5=17d431fb4f97d370b4e6dc9e26f0f4a1","Deregulation in power systems has created new uncertainties and increased the previous ones. The presence of these uncertainties causes the transmission network to remain monopoly and the private investors not being interested in investing in this section. This paper presents a new merchant-based transmission expansion planning (TEP) formulation from the viewpoint of private investors. The information-gap decision theory (IGDT) is used to model the inherent uncertainties associated with the estimated investment cost of candidate lines and the forecasted system load and NSGAII is utilized to solve the multi-objective optimization problem. This algorithm helps private investors to select the best lines for investment in the presence of uncertainties. In order to verify the effectiveness of the proposed method, it has been applied to the IEEE RTS 24-bus system and the simplified Iranian 400-kV transmission system. © 2016 John Wiley & Sons, Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978429658&partnerID=40&md5=cbb527241cb67be7c55cbaff6599befb","Nowadays, decision-makers face deep uncertainties from a myriad of external factors such as climate change, population growth, new technologies, and economic developments. The challenge is to develop robust policies, which perform well across all possible resolutions of the uncertainties. One approach for achieving this is to design a policy to be adapted over time in response to how the future actually unfolds. A key determinant for the efficacy of such an adaptive policy is the specification of when and how to adapt it. This specification depends on how robustness is being operationalized. To date, there is little guidance for selecting an appropriate robustness metric. In this chapter we address this problem, using a case study of designing a policy for stimulating the transition of the European energy system towards more sustainable functioning using five different robustness metrics. We compare the policies as identified by each metric and discuss their relative merits. We highlight that the different robustness metrics emphasize different aspects of what makes a policy robust. More specifically, measures that separate dispersion and the mean, effectively doubling the number of objectives, provide very valuable information on the trade-offs between the mean performance of the policy and dispersion around this mean. We also discuss, based on our case, why analysts should use multiple robustness metrics. © Springer International Publishing Switzerland 2016."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995550673&partnerID=40&md5=4c7894d64a5a447d4ffbabb816433aa5","Understanding hydrological model predictive capabilities under contrasting climate conditions enables more robust decision making. Using Differential Split Sample Testing (DSST), we analyze the performance of six hydrological models for 37 Irish catchments under climate conditions unlike those used for model training. Additionally, we consider four ensemble averaging techniques when examining interperiod transferability. DSST is conducted using 2/3 year noncontinuous blocks of (i) the wettest/driest years on record based on precipitation totals and (ii) years with a more/less pronounced seasonal precipitation regime. Model transferability between contrasting regimes was found to vary depending on the testing scenario, catchment, and evaluation criteria considered. As expected, the ensemble average outperformed most individual ensemble members. However, averaging techniques differed considerably in the number of times they surpassed the best individual model member. Bayesian Model Averaging (BMA) and the Granger-Ramanathan Averaging (GRA) method were found to outperform the simple arithmetic mean (SAM) and Akaike Information Criteria Averaging (AICA). Here GRA performed better than the best individual model in 51%-86% of cases (according to the Nash-Sutcliffe criterion). When assessing model predictive skill under climate change conditions we recommend (i) setting up DSST to select the best available analogues of expected annual mean and seasonal climate conditions; (ii) applying multiple performance criteria; (iii) testing transferability using a diverse set of catchments; and (iv) using a multimodel ensemble in conjunction with an appropriate averaging technique. Given the computational efficiency and performance of GRA relative to BMA, the former is recommended as the preferred ensemble averaging technique for climate assessment. © 2016. American Geophysical Union. All Rights Reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976402537&partnerID=40&md5=e3539aa8723851da528d75a40d9348dd","Sea-level rise is one of the most concerning and costly effects of climate change. Resulting sea-level rise impacts may include failure or destruction of infrastructure, immobilization due to transportation system breakdown, and catastrophic saltwater contamination of water supplies. The problem of sea-level rise adaptation is characterized by deep uncertainty that makes it complex to evaluate the economic value of adaptation investments. The key element to evaluate the effectiveness of adaptation strategies is to quantify the long-term cost of physical networks under uncertain sea-level rise scenarios. In this paper, a simulation framework is created and tested to investigate the long-term impacts of sea-level rise on infrastructure systems in order to evaluate the effectiveness of various adaptation strategies. To this end, the transformation of infrastructure systems under various scenarios of sea-level rise and adaptation strategies is modeled using the proposed simulation framework. Then, the impacts of sea-level rise are determined in terms of the life cycle costs of infrastructure networks. These estimated costs are used for evaluating the feasibility of various adaptation strategies under future uncertain sea-level rise scenarios. The application of the proposed simulation framework is shown in a case study of a road network using the sea-level rise scenarios in Southeast Florida. The results of the analysis are threefold: (1) prioritization of infrastructure assets for adaptation investment; (2) identification of the right timing of adaptation investments for different links in an infrastructure network; and (3) evaluation of the present value of adaptation investments for the entire network. The results enable more informed decision-making in order to implement robust adaptation under uncertain sea-level rise scenarios. © ASCE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992364759&partnerID=40&md5=e848aedf03f3f8ac332b85628ec3da83","The development of novel form-active hybrid structures (FAHS) is impeded by a lack of modelling tools that allow for exploratory topology modelling of shaped assemblies. We present a flexible and real-time computational design modelling pipeline developed for the exploratory modelling of FAHS that enables designers and engineers to iteratively construct and manipulate form-active hybrid assembly topology on the fly. The pipeline implements Kangaroo2's projection-based methods for modelling hybrid structures consisting of slender beams and cable networks. A selection of design modelling sketches is presented in which the developed modelling pipeline has been integrated to explore the design space delineated by FAHS. © 2016 The Author(s)."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994568483&partnerID=40&md5=7084a17c4009eec66a2a47cb3aec4427","Software agents, modeling and simulation, and Model-Driven Engineering (MDE) arc fields with their unique body of knowledge. However, developments in one area can provide new avenues for advancing the methodologies and technical strategies in other fields. This paper examines the synergies among these three fields by highlighting selected challenges in each area and delineating how methods developed in one field can mutually be beneficial. We categorize the ways in which the agent paradigm can contribute to critical activities in MDE, and similarly we describe how domain-specific languages and model transformation strategies can be instrumental in the design, implementation, and management of agent systems. © 2016 Society for Modeling & Simulation International (SCS)."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949198542&partnerID=40&md5=5af34adc332b4246102a9b23b57741cd","In recent years, electronic health records (EHRs) have been widely adapted at many healthcare facilities in an attempt to improve the quality of patient care and increase the productivity and efficiency of healthcare delivery. These EHRs can accurately diagnose diseases if utilized appropriately. While the EHRs can potentially resolve many of the existing problems associated with disease diagnosis, one of the main obstacles in effectively using them is the patient privacy and sensitivity of the medical information available in the EHR. Due to these concerns, even if the EHRs are available for storage and retrieval purposes, sharing of the patient records between different healthcare facilities has become a major concern and has hampered some of the effective advantages of using EHRs. Due to this lack of data sharing, most of the facilities aim at building clinical decision support systems using limited amount of patient data from their own EHR systems to provide important diagnosis related decisions. It becomes quite in-feasible for a newly established healthcare facility to build a robust decision making system due to the lack of sufficient patient records. However, to make effective decisions from clinical data, it is indispensable to have large amounts of data to train the decision models. In this regard, there are conflicting objectives of preserving patient privacy and having sufficient data for modeling and decision making. To handle such disparate goals, we develop two adaptive distributed privacy-preserving algorithms based on a distributed ensemble strategy. The basic idea of our approach is to build an elegant model for each participating facility to accurately learn the data distribution, and then transfer the useful healthcare knowledge acquired on their data from these participators in the form of their own decision models without revealing and sharing the patient-level sensitive data, thus protecting patient privacy. We demonstrate that our approach can successfully build accurate and robust prediction models, under privacy constraints, using the healthcare data collected from different geographical locations. We demonstrate the performance of our method using the type-2 diabetes EHRs accumulated from multiple sources from all fifty states in the U.S. Our method was evaluated on diagnosing diabetes in the presence of insufficient number of patient records from certain regions without revealing the actual patient data from other regions. Using the proposed approach, we also discovered the important biomarkers, both universal and region-specific, and validated the selected biomarkers using the biomedical literature. © 2015 Elsevier Inc. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979620079&partnerID=40&md5=ae842c8f71387408d219c19a6e14249e","Visualisation is often presented as a means of simplifying information and helping people understand complex data. In this paper we describe a project designing interactive visualisations to support learner competencies in the broad area of numeracy. The work builds upon: (i) the observation that while spreadsheets are traditional ICT tools, their familiarity means that they are used for exploratory mathematical modelling; (ii) a research theme examining the human factors that influence the ease with which formal notations can be understood and applied appropriately. Our paper describes the iterative design and evaluation of a tool to visualise spreadsheets, with the aim of supporting mid-teen learners based on the premise that spreadsheets serve as a gateway tool for supporting learner experimentation and confidence within numerate subjects. This iterative process is informed by background research into notational design, graphic design as well as learner and tutor feedback. Copyright © 2016 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920169510&partnerID=40&md5=190dc8aa8f17bbf552ce14244d7e54a9","Objective Evaluation of the long-term HPV-16/18 AS04-adjuvanted vaccine immunogenicity persistence in women. Design Multicentre, open-label, long-term follow-up (NCT00947115) of a primary phase-III study (NCT00196937). Setting Six centres in Germany and Poland. Population 488 healthy women (aged 15-55 years, age-stratified into groups: 15-25, 26-45, and 46-55 years) who received three vaccine doses in the primary study. Methods Immune responses were evaluated in serum and cervicovaginal secretion (CVS) samples 6 years after dose 1. Anti-HPV-16/18 geometric mean titres (GMTs) were measured by enzyme-linked immunosorbent assay (ELISA), and were used to fit the modified power-law and piecewise models, predicting long-term immunogenicity. Serious adverse events (SAEs) were recorded. Main outcome measures Anti-HPV-16/18 seropositivity rates and GMTs 6 years after dose 1. Results At 6 years after dose 1, all women were seropositive for anti-HPV-16 and ≥97% were seropositive for anti-HPV-18 antibodies. GMTs ranged from 277.7 to 1344.6 EU/ml, and from 97.6 to 438.2 EU/ml, for anti-HPV-16 and anti-HPV-18, respectively. In all age groups, GMTs were higher (anti-HPV-16, 9.3-45.1-fold; anti-HPV-18, 4.3-19.4-fold) than levels associated with natural infection (29.8 EU/ml). A strong correlation between serum and CVS anti-HPV-16/18 levels was observed, with correlation coefficients of 0.81-0.96 (anti-HPV-16) and 0.69-0.84 (anti-HPV-18). Exploratory modelling based on the 6-year data predicted vaccine-induced anti-HPV-16/18 levels above natural infection levels for at least 20 years, except for anti-HPV-18 in the older age group (piecewise model). One vaccine-related and two fatal SAEs were reported. Conclusions At 6 years after vaccination, immune responses induced by the HPV-16/18 AS04-adjuvanted vaccine were sustained in all age groups. © 2014 Royal College of Obstetricians and Gynaecologists."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951856906&partnerID=40&md5=fef3607c67fca36a9a1166c53ac8da7f","Developing fresh water supply strategies for the long term needs to take into account the fact that the future is deeply uncertain. Not only the extent of climate change and the extent and nature of its impacts are unknown, also socio-economic conditions may change in unpredictable ways, as well as social preferences. Often, it is not possible to find solid ground for estimating probabilities for the relevant range of imaginable possible future developments. Yet, some of these may have profound impacts and consequences for society which could be reduced by timely proactive adaptation. In response to these and similar challenges, various approaches, methods and techniques have been proposed and are being developed to specifically address long-term strategy development under so-called deep uncertainty. This paper, first, offers a brief overview of developments in the field of planning under (deep) uncertainty. Next, we illustrate application of three different approaches to fresh water provision planning under uncertainty in case studies in the Netherlands: a resilience approach, oriented to (re) designing fresh water systems in such a way that they will be less vulnerable, resp. will be able to recover easily from future disturbances; a robustness approach, oriented to quantitative assessment of system performance for various system configurations (adaptation options) under a range of external disturbances, and an exploratory modeling approach, developed to explore policy effectiveness and system operation under a very wide set of assumptions about future conditions. © 2015 The Author(s)"
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939535868&partnerID=40&md5=ac493b140ec32171879ec3994c2a272e","This study introduces a new open source software framework to support bottom-up environmental systems planning under deep uncertainty with a focus on many-objective robust decision making (MORDM), called OpenMORDM. OpenMORDM contains two complementary components: (1) a software application programming interface (API) for connecting planning models to computational exploration tools for many-objective optimization and sensitivity-based discovery of critical deeply uncertain factors; and (2) a web-based visualization toolkit for exploring high-dimensional datasets to better understand system trade-offs, vulnerabilities, and dependencies. We demonstrate the OpenMORDM framework on a challenging environmental management test case termed the ""lake problem"". The lake problem has been used extensively in the prior environmental decision science literature and, in this study, captures the challenges posed by conflicting economic and environmental objectives, a water quality ""tipping point"" beyond which the lake may become irreversibly polluted, and multiple deeply uncertain factors that may undermine the robustness of pollution management policies. The OpenMORDM software framework enables decision makers to identify policy-relevant scenarios, quantify the trade-offs between alternative strategies in different scenarios, flexibly explore alternative definitions of robustness, and identify key system factors that should be monitored as triggers for future actions or additional planning. The web-based OpenMORDM visualization toolkit allows decision makers to easily share and visualize their datasets, with the option for analysts to extend the framework with customized scripts in the R programming language. OpenMORDM provides a platform for constructive decision support, allowing analysts and decision makers to interactively discover promising alternatives and potential vulnerabilities while balancing conflicting objectives. © 2015 The Authors."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960465254&partnerID=40&md5=5fe09431832588e2bd4876c5d1a31cec","We present a new model for reliability analysis that is able to distinguish the latent internal vulnerability state of the equipment from the vulnerability caused by temporary external sources. Consider a wind farm where each turbine is running under the external effects of temperature, wind speed and direction, etc. The turbine might fail because of the external effects of a spike in temperature. If it does not fail during the temperature spike, it could still fail due to internal degradation, and the spike could cause (or be an indication of) this degradation. The ability to identify the underlying latent state can help better understand the effects of external sources and thus lead to more robust decision-making. We present an experimental study using SCADA sensor measurements from wind turbines in Italy. © Institute of Mathematical Statistics, 2015."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941061686&partnerID=40&md5=87d6780badde378672c6f0d655a72599","Wang et al. (2015) employed driving force-pressure-state-impact-response (DPSIR) framework to provide a robust decision-making structure for carbon emission reduction by use of wolfberry plantation in the Jingtai oasis, China. DPSIR appropriately identified the causes of problem along with adopting the responses to the barriers associated with wolfberry plantation. However, the discusser argues that, the paper could have prepared more viable outcomes, if the authors had used causal network rather than causal chains through the DPSIR framework. Furthermore, they could have quantified the mutual relationship among the relevant factors to provide a detailed economic assessment. With this knowledge in hand, the current discussion letter suggests eDPSIR and meDPSIR to address drawbacks regarding conventional DPSIR framework. Due to proper performance of eDPSIR and meDPSIR, they are recommended as practical tools in the future environmental studies. © 2015 Elsevier Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925707964&partnerID=40&md5=cd791b2a773887b03d52ea22817fcf83","The future of the American West depends on sustainable water resource governance. A variety of uncertainties associated with limited freshwater supplies, population growth, land use change, drought, and climate change impacts present substantial challenges. To inform decision making, managers are adopting new techniques such as scenario planning to understand how water resources might change and what practices can support economic, environmental, and social sustainability. Scenario planning can be informed by understanding the normative future preferences of a variety of stakeholders, including decision makers, who influence water governance. This article presents a survey of central Arizona decision makers to understand their visions for a desirable future for the water system in terms of supply, delivery, demand, outflow, and crosscutting activities. Principle components analysis is used to identify patterns underlying responses about preferences for each domain of the system and correlation analysis is used to evaluate associations between themes across the domains. The results reveal two distinct visions for water in central Arizona - one in which water experts and policy makers pursue supply augmentation to serve metropolitan development, and another in which broadened public engagement is used in conjunction with policy tools to reduce water consumption, restore ecosystem services, and limit metropolitan expansion. The results of this survey will inform the development of a set of normative scenarios for use in exploratory modeling and anticipatory governance activities. © National Association of Environmental Professionals 2015."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941283807&partnerID=40&md5=fd2f7265a95ea4ed36c1d98951858648","We propose a systematic framework for the selection of optimal processing pathways for a microalgae-based biorefinery under techno-economic uncertainty. The proposed framework promotes robust decision making by taking into account the uncertainties that arise due to inconsistencies among and shortage in the available technical information. A stochastic mixed integer nonlinear programming (sMINLP) problem is formulated for determining the optimal biorefinery configurations based on a superstructure model where parameter uncertainties are modeled and included as sampled scenarios. The solution to the sMINLP problem determines the processing technologies, material flows, and product portfolio that are optimal with respect to all the sampled scenarios. The developed framework is implemented and tested on a specific case study. The optimal processing pathways selected with and without the accounting of uncertainty are compared with respect to different objectives. © 2015 Elsevier Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946563494&partnerID=40&md5=2ab285430d11a3ff467df5bf02bef772","As unmanned aerial vehicles (UAVs) are used more and more in military operations, increasing their level of autonomous decision making becomes necessary. In uncertain battlefield environments, when making sovereign decisions, UAVs must choose low-risk options. An integrated framework is proposed for UAV robust decision making in air-to-ground attack missions under severe uncertainty. In the offline part of the framework, the battlefield scenarios are analyzed and an influence diagram is built to represent the decision situation. In the online part, the UAV evaluates the alternative actions for every scenario, and then the optimal robust action is chosen, using the robust decision model. Results of simulation show that the proposed approach is feasible and effective. The framework can support UAVs in making independent robust decisions under circumstances which require immediate responses under severe uncertainty, and it can also be extended to applications in more complex situations. © 2015, Central South University Press and Springer-Verlag Berlin Heidelberg."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956789526&partnerID=40&md5=a07cd181dc21a059669e6ba6093bbddc","Various uncertainties are involved in the representation of processes that characterize interactions among societal needs, ecosystem functioning, and hydrological conditions. Here we develop an empirical uncertainty assessment of water security indicators that characterize scarcity and vulnerability, based on a multimodel and resampling framework. We consider several uncertainty sources including those related to (i) observed streamflow data; (ii) hydrological model structure; (iii) residual analysis; (iv) the method for defining Environmental Flow Requirement; (v) the definition of critical conditions for water provision; and (vi) the critical demand imposed by human activities. We estimate the overall hydrological model uncertainty by means of a residual bootstrap resampling approach, and by uncertainty propagation through different methodological arrangements applied to a 291 km2 agricultural basin within the Cantareira water supply system in Brazil. Together, the two-component hydrograph residual analysis and the block bootstrap resampling approach result in a more accurate and precise estimate of the uncertainty (95% confidence intervals) in the simulated time series. We then compare the uncertainty estimates associated with water security indicators using a multimodel framework and the uncertainty estimates provided by each model uncertainty estimation approach. The range of values obtained for the water security indicators suggests that the models/methods are robust and performs well in a range of plausible situations. The method is general and can be easily extended, thereby forming the basis for meaningful support to end-users facing water resource challenges by enabling them to incorporate a viable uncertainty analysis into a robust decision-making process. Key Points: Uncertainty analysis of scarcity and vulnerability indicators This multimodel/resampling-based framework includes several uncertainty sources Viable uncertainty analysis to be included into a robust decision-making process. © 2015. American Geophysical Union. All Rights Reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995755407&partnerID=40&md5=98af5b7b7e86f965b5039913b30b1049","A shape grammar defines a procedural shape space containing a variety of models of the same class, e.g. buildings, trees, furniture, airplanes, bikes, etc. We present a framework that enables a user to interactively design a probability density function (pdf) over such a shape space and to sample models according to the designed pdf. First, we propose a user interface that enables a user to quickly provide preference scores for selected shapes and suggest sampling strategies to decide which models to present to the user to evaluate. Second, we propose a novel kernel function to encode the similarity between two procedural models. Third, we propose a framework to interpolate user preference scores by combining multiple techniques: function factorization, Gaussian process regression, autorelevance detection, and l1 regularization. Fourth, we modify the original grammars to generate models with a pdf proportional to the user preference scores. Finally, we provide evaluations of our user interface and framework parameters and a comparison to other exploratory modeling techniques using modeling tasks in five example shape spaces: furniture, low-rise buildings, skyscrapers, airplanes, and vegetation. Copyright 2015 ACM."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929582555&partnerID=40&md5=1b06fcf3223c9df52680cb9735f5c0bb","A diverse range of response options is available for decision-makers to manage environmental change and meet sustainability objectives. These can include inter alia: top-down statutory regulation and levies; bottom-up initiatives including quality assurance networks or community-based partnerships; formal incentives; and voluntary market-based schemes such as 'payments for ecosystem services' or offsetting. Each type of response option has a distinct set of characteristics, which suggests that they may be best suited to different contexts rather than presumed to be effective in all circumstances. These attributes are used to develop a working typology to help understand the strengths and weaknesses of different response types, particularly regarding adaptation to long-term change and handling of uncertainty. To facilitate this, response types are referenced from a socio-ecological systems perspective using a refined version of the DPSIR integrated assessment framework to incorporate ecosystem functions and services. This shows that some responses are more clearly associated with maintaining resilience of natural functions, whilst others are directed at human-defined services. The role of stakeholders in response options can also be distinguished such as comparing mandatory or voluntary initiatives. Polluter-pays approaches are therefore distinguished from beneficiary-pays approaches. The typology can therefore provide a working reference framework for recognising complementary rather than conflicting interventions, as guided by the holistic principles of the Ecosystem Approach. © 2015 Elsevier Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954305975&partnerID=40&md5=979e1017b1c42bb298ad6a3236591938","This paper proposes a four-step approach based on technology roadmapping and scenario-based roadmapping. The objective is to evaluate the relevance of new products and technologies and its variation under a range of possible future conditions or scenarios. A case study on rail automation for passenger transport systems is conducted to demonstrate the applicability of the proposed method. Market drivers, new systems, products and technologies are identified in a literature review and then verified and linked by expert judgments. Analyzing the resulting graphical representation of relevance and robustness from the proposed approach leads to a periodization of products and technologies for future development and an evaluation of the most influential market driver. The proposed approach for scenario-based technology roadmapping facilitates robust decision making under future uncertainties. © 2016 Elsevier Inc."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941170354&partnerID=40&md5=dac93c4f4c6bc0194f07944dd0769d16","Uncertainty is a pervasive characteristic of all research addressed at the International Institute for Applied Systems Analysis (IIASA) which is at the core of this Special Issue. The role of science in better coping with uncertainty is twofold. First, to describe uncertainties as comprehensively and well as possible, both quantitatively and qualitatively; second, to develop methods that can lead to improved decision-making under uncertainty. Here increasingly the concept of ""optimal"" is replaced by one of ""robust"" decisions, i.e. decisions that make sense vis à vis multiple uncertainties. This paper illustrates selective examples from IIASA research that contribute to the twin objectives of a better description of uncertainty and improved decision-making under uncertainty, drawing from research in the fields of technology dynamics, climate change policy, as well as catastrophic risk management and portfolio analysis. The conclusions emphasize the need for a basic research strategy aimed at elucidating uncertainties in parameters as well as in alternative model representations, and in developing improved models for robust decision-making. Models of robust decision making emphasize risk hedging spatially and through a portfolio of policies and technology options. © 2015 Elsevier Inc."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955170921&partnerID=40&md5=0f921b9dd830ca4537713842d89ac2f1","In this paper, we propose a model where binary games with many players are implemented at two subsequent dates. An external authority sets incentives to maximize the gain deriving from the project. We show that the interplay between the optimal participation shares at the two subsequent dates makes the optimal strategy nontrivial and, to some extent, unexpected. As an application, in the context of an insurgence muting into an armed rebellion, we study the emergence of escalation effects when many actors interact taking into account social recognition. © 2015 World Scientific Publishing Company."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938569181&partnerID=40&md5=36ee35026b4ccf33e2522c2e5ceccd98","In this paper we discuss the importance of framing the question of public acceptance of sustainable energy transitions in terms of values and a 'whole-system' lens. This assertion is based on findings arising from a major research project examining public values, attitudes and acceptability with regards to whole energy system change using a mixed-method (six deliberative workshops, n=. 68, and a nationally representative survey, n=. 2441), interdisciplinary approach. Through the research we identify a set of social values associated with desirable energy futures in the UK, where the values represent identifiable cultural resources people draw on to guide their preference formation about particular aspects of energy system change. As such, we characterise public perspectives as being underpinned by six value clusters relating to efficiency and wastefulness, environment and nature, security and stability, social justice and fairness, autonomy and power, and processes and change. We argue that this 'value system' provides a basis for understanding core reasons for public acceptance or rejection of different energy system aspects and processes. We conclude that a focus on values that underpin more specific preferences for energy system change brings insights that could provide a basis for improved dialogue, more robust decision-making, and for anticipating likely points of conflict in energy transitions. © 2015."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938059213&partnerID=40&md5=ab2bc4569352e5e1c0ea688ae3c98c60","To successfully limit climate change, today's greenhouse gas mitigation policies should encourage reductions that will continue for decades. History suggests, however, that some policy reforms lead to societal changes that persist over the long-term while others fade without long-term effect. Current climate policy literature provides little guidance on how today's policy choices can successfully shape long-term emission reduction paths. To address such questions, this paper introduces a new agent-based, game theoretic model designed to compare how near-term choices regarding alternative policy architectures influence long-term emission reduction trajectories. Drawing on political science literature that identifies the characteristics of policies that persist over time, this simulation for the first time integrates the co-evolution of an industry sector, its technology base, and the shifting political coalitions that influence the future stringency of the government's emission reduction policies-all as influenced by the initial choice of policy architecture. An exploratory modeling analysis that represents deeply uncertain phenomena such as the future potential for innovation and the behavior of future governments draws policy-relevant conclusions from this model. The analysis finds that near-term choices regarding the architecture of a carbon pricing policy may affect long-term decarbonization rates significantly. In particular, such rates are higher if program revenues are returned to firms in proportion to their market share, thus, creating a political constituency for continuing the carbon pricing policy. More generally, the analysis provides a framework for considering how near-term policy choices can affect long-term emission transformation pathways within integrated assessment models. © 2015 Elsevier Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937828100&partnerID=40&md5=8533e49bee68c8d27e03e9bd8bdf2db3","Abstract The main goal of this research is to specify the best assessment indexes for irreversible refrigeration cycles. These approaches available in the previous works are the ecological coefficient of performance and exergetic performance coefficient. Irreversible Carnot refrigerator is defined as a system. External and internal irreversibilities are included in the thermodynamic analysis. In this work, two scenarios of optimization are defined. The outcomes of each scenario are evaluated distinctly. In first scenario, in order to maximize the ecological coefficient of performance (ECOP) and exergy input to the system (Ex) and cooling load (QL), multi-objective optimization algorithms have been utilized. Also, in second scenario, three objective functions comprising the ecological coefficient of performance (ECOP) and exergy input to the system (Ex) and exergetic performance criteria (EPC) are maximized concurrently via multi-objective optimization approach. Multi-objective evolutionary algorithms (MOEAs) joined with NSGA-II approach are employed throughout this paper. Three robust decision making methods including LINAMP, TOPSIS and FUZZY are employed to ascertain final solutions. Finally, error analyses of the outputs gained via decision making methods are determined. © 2015 Elsevier Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937510436&partnerID=40&md5=27e1b068a934553e418c7b331816ebcc","The robustness of a range of watershed-scale green and gray drainage strategies in the future is explored through comprehensive modeling of a fully integrated urban wastewater system case. Four socio-economic future scenarios, defined by parameters affecting the environmental performance of the system, are proposed to account for the uncertain variability of conditions in the year 2050. A regret-based approach is applied to assess the relative performance of strategies in multiple impact categories (environmental, economic, and social) as well as to evaluate their robustness across future scenarios. The concept of regret proves useful in identifying performance trade-offs and recognizing states of the world most critical to decisions. The study highlights the robustness of green strategies (particularly rain gardens, resulting in half the regret of most options) over end-of-pipe gray alternatives (surface water separation or sewer and storage rehabilitation), which may be costly (on average, 25% of the total regret of these options) and tend to focus on sewer flooding and CSO alleviation while compromising on downstream system performance (this accounts for around 50% of their total regret). Trade-offs and scenario regrets observed in the analysis suggest that the combination of green and gray strategies may still offer further potential for robustness. © 2015 American Chemical Society."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937116207&partnerID=40&md5=319cc7d9e26a1181fd758682af8efc78","Climate change and its impacts already pose considerable challenges for societies that will further increase with global warming (IPCC, 2014a, b). Uncertainties of the climatic response to greenhouse gas emissions include the potential passing of large-scale tipping points (e.g. Lenton et al., 2008; Levermann et al., 2012; Schellnhuber, 2010) and changes in extreme meteorological events (Field et al., 2012) with complex impacts on societies (Hallegatte et al., 2013). Thus climate change mitigation is considered a necessary societal response for avoiding uncontrollable impacts (Conference of the Parties, 2010). On the other hand, large-scale climate change mitigation itself implies fundamental changes in, for example, the global energy system. The associated challenges come on top of others that derive from equally important ethical imperatives like the fulfilment of increasing food demand that may draw on the same resources. For example, ensuring food security for a growing population may require an expansion of cropland, thereby reducing natural carbon sinks or the area available for bio-energy production. So far, available studies addressing this problem have relied on individual impact models, ignoring uncertainty in crop model and biome model projections. Here, we propose a probabilistic decision framework that allows for an evaluation of agricultural management and mitigation options in a multi-impact-model setting. Based on simulations generated within the Inter-Sectoral Impact Model Intercomparison Project (ISI-MIP), we outline how cross-sectorally consistent multi-model impact simulations could be used to generate the information required for robust decision making. Using an illustrative future land use pattern, we discuss the trade-off between potential gains in crop production and associated losses in natural carbon sinks in the new multiple crop- and biome-model setting. In addition, crop and water model simulations are combined to explore irrigation increases as one possible measure of agricultural intensification that could limit the expansion of cropland required in response to climate change and growing food demand. This example shows that current impact model uncertainties pose an important challenge to long-term mitigation planning and must not be ignored in long-term strategic decision making. © Author(s) 2015."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931031051&partnerID=40&md5=30a72b2c887b47e735c7989bdd2f6131","With hard-to-predict changes in future demand, climate, supply options, technological opportunities, and budgetary constraints, water agency plans should be flexible and robust, designed to meet agency goals over a wide range of plausible future conditions. But current state-of-the-art approaches to water planning make it difficult to craft flexible and robust plans to guide resource allocation and facilitate discussions with the agency's constituents and ratepayers. This paper describes an innovative effort by one agency, the Metropolitan Water District of Southern California (termed Metropolitan), to begin to address these challenges. Metropolitan's year 2010 integrated resources plan (IRP) update specifies resource allocations over 25 years and calls for an adaptive management approach to revisit these allocations over time. Using a quantitative decision support approach called robust decision-making (RDM), an enhanced version of Metropolitan's main planning model was run, over many thousands of cases representing different combinations of assumptions about future demand, conditions in the bay/delta, climate conditions, local resource yields, and implementation challenges. Statistical cluster analysis on the resulting database of model runs identifies scenarios that succinctly summarize the types of future conditions in which the IRP core resources strategy does and does not meet its goals. These scenarios inform early warning indicators that can guide the adaptive management component of the IRP. The robust decision methods presented in this paper should prove broadly useful for Metropolitan in addition to other water agencies seeking to develop robust and adaptive plans in the face of uncertain future conditions. © 2014 American Society of Civil Engineers."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939971051&partnerID=40&md5=eaf74e614e6c491a61f0c8a747994ecc","The stochastic location-allocation p-hub median problems are related to long-term decisions made in risky situations. Due to the importance of this type of problems in real-world applications, the authors were motivated to propose an approach to obtain more reliable policies in stochastic environments considering the decision makers’ preferences. Therefore, a systematic approach to make robust decisions for the single location-allocation p-hub median problem based on mean-variance theory and two-stage stochastic programming was developed. The approach involves three main phases, namely location modeling, risk modeling, and decision making, each including several steps. In the first phase, the pertinent location-allocation model of the problem is developed in the form of a two-stage stochastic model based on its deterministic version. A risk measure, based on total cost function and mean-variance theory, is derived in the second phase. Furthermore, two heterogeneous terms of the risk measure have been normalized and an innovative procedure has been proposed to significantly improve the calculation efficiency. In the third phase, the Pareto solution is obtained, the frontier curve is depicted to determine the decision maker’s risk aversion coefficient, and a robust policy is obtained through optimization based on decision makers’ preferences. Finally, a case study of an automobile part distribution system with stochastic demand is described to further illustrate our risk management and analysis approach. © 2014, Springer-Verlag London."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925340714&partnerID=40&md5=b05548783051bfbe949133850897ff83","A procedure for the objective reduction of a rain gauge network is developed and applied for the Susquehanna River Basin (SRB) in the United States. The procedure utilizes evaluation of the theoretical error variance associated with precipitation analysis using a variant of the National Weather Service's (NWS) Multisensor Precipitation Estimator (MPE). The uncertainty analysis is carried out for 16 different combinations of the precipitation accumulation period, season, magnitude, and areal extent, and use or nonuse of the Flash Flood Potential Index (FFPI), which is used as a proxy for the spatially varying runoff ratio. To estimate the statistical parameters of the procedure, the historical archive of the MPE products operationally produced by the Middle Atlantic River Forecast Center (MARFC) was used. The marginal value of each rain gauge in the Susquehanna Flood Forecasting and Warning System (SFFWS) network to the parent network is assessed by calculating the increase in the theoretical error variance in radar-gauge precipitation analysis over the SRB following hypothetical removal of the gauge. The parent network is made of 73 gauges in the SFFWS network plus 120 high-quality hourly and subhourly rain gauges within and in the vicinity of the SRB. The results show that significant variability exists in the rankings of the marginal value of the SFFWS gauges among the 16 cases considered. The most significant source of this variability is the seasonal variation in the spatial structure of precipitation. The second most significant source is the use or nonuse of the FFPI. Given the significant sensitivity to these and possibly other factors, one may not expect a unique solution for optimal network reduction. For robust decision making, an ensemble of solutions should be considered that reflects the range of variability in such factors. © 2014 American Society of Civil Engineers."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963594613&partnerID=40&md5=1cf9874aa4091e05f4bcd10cabb47693","European sea bass, Dicentrarchus labrax, is a highly valuable species in Europe, both for aquaculture in the Mediterranean Sea and for commercial and recreational fisheries in the North East Atlantic Ocean. Subjected to increasing fishing pressure, the wild population has recently experienced significant recruitment fluctuation as well as a northward extension of its distribution area in the North Sea. While the nature of the ecological and/or physiological processes involved remains unresolved, ontogenetic habitat shifts and adult site fidelity could increase the species' vulnerability to climate change and overfishing. As managers look for expert information to propose management scenarios leading to sustainable exploitation, exploratory modelling appears to be a cost-efficient approach to enhance the understanding of recruitment dynamics and the spatio-temporal scales over which fish populations function. A conceptual modelling framework and its specific data requirements are discussed to tackle some sound ecological questions regarding this species. We consequently provide an updated review of current knowledge on bass population structure, biology and ecology. This paper will hence be particularly valuable to develop spatially-explicit models of European sea bass dynamics under environmental and anthropogenic forcing. Knowledge gaps requiring further research efforts are also reported. © EDP Sciences 2015."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920189724&partnerID=40&md5=0d8835a00f79c78cea52f13180dac6f1","Scenario planning traditionally relies on qualitative methods to choose its scenarios. Recently, quantitative decision support tools have also begun to facilitate such choices. This study uses behavioral experiments and structured decision-maker interviews to evaluate the results of ""scenario discovery,"" a quantitative method that defines scenarios as sets of future states of the world in which proposed policies fail to meet their goals. Statistical cluster-finding and principal component algorithms applied to large databases of computer simulation model results then help users to identify such scenarios. The two experiments examine the results of this process and demonstrate a user preference for increased accuracy and simplicity achieved through rotating the space of uncertain model input parameters, but primarily when the rotated parameters are conceptually similar. Interviews with experts suggest utility for both qualitatively- and quantitatively-derived scenarios. The former were easier to understand and had the most utility for scoping. The latter were perceived as containing more relevant information and having more utility for understanding tradeoffs and making choices among them. Overall, this study suggests the value of quantitative tools for facilitating scenario choice, while also highlighting the importance of formal evaluation in judging the utility of new methods for decision support. © 2014 Elsevier Inc.."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948447379&partnerID=40&md5=1a90d713db4fde68d02da90413d4e465","The proceedings contain 45 papers. The topics discussed include: addressing the adaptive customization of timber prefabricated housing through axiomatic design; on the computation of the information content of a coupled design with two functional requirements; design of changeable production units within the automotive sector with axiomatic design; axiomatic design based guidelines for the design of a lean product development process; robust decision making for agile systems development part 2: a decomposition and analysis; robust decision making for agile systems development part 1: exploring the paradigm; design progress of the DEMO divertor locking system according to IPADeP methodology; systematic design of SME manufacturing and assembly systems based on axiomatic design; dynamic axiomatic design (DAD): applying the independence axiom in the design of social systems; using axiomatic design and entropy to measure complexity in mass customization; and re-design of an interoperable buyer-seller automotive relationship aided by computer simulation."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84935139870&partnerID=40&md5=afbe01cb982b08ae582534941664a5a1","Societal ageing is a messy problem with diverging stakeholder views regarding the desirability of policy measures. In this paper, we use a System Dynamics model representing the Dutch demographic and social security system to investigate if and when Dutch governmental retirement and healthcare contributions become unaffordable. Following the Robust Decision Making approach, we then design and test policies for reducing the societal costs of ageing, taking into account societal support for these policies. We find that unaffordable societal ageing costs are mainly caused by declining productivity levels and increasing life expectancy: permanent increases in productivity are required to sustain the current social security level. We also find that the recently adopted Dutch retirement age policy is insufficiently robust and that focusing on increasing the actual instead of the formal retirement age may generate more public support. © 2015 John Wiley & Sons, Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84965147247&partnerID=40&md5=b07f388a4ffdd6ad48b1d6546f891a54","In this paper we address the problem of decision making within a Markov decision process (MDP) framework where risk and modeling errors are taken into account. Our approach is to minimize a risk-sensitive conditional-value-at-risk (CVaR) objective, as opposed to a standard risk-neutral expectation. We refer to such problem as CVaR MDP. Our first contribution is to show that a CVaR objective, besides capturing risk sensitivity, has an alternative interpretation as expected cost under worst-case modeling errors, for a given error budget. This result, which is of independent interest, motivates CVaR MDPs as a unifying framework for risk-sensitive and robust decision making. Our second contribution is to present an approximate value-iteration algorithm for CVaR MDPs and analyze its convergence rate. To our knowledge, this is the first solution algorithm for CVaR MDPs that enjoys error guarantees. Finally, we present results from numerical experiments that corroborate our theoretical findings and show the practicality of our approach."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946067358&partnerID=40&md5=45680d11c67aec130af6ad85b269d885","This paper focusses on robust decision-making in disaster response where pre-existing logistical structures have not been destructed yet but where a great risk of delayed consequences exists if the functioning of these structures is not strengthened. Responsible decision-makers are companies as operators of the logistical structures themselves, particularly those whose businesses refer to the critical infrastructure sectors food, water, health care, and energy. This paper outlines a conception of a simulation model which combines approaches of scenario-based optimization, stress testing, and robustness measurement.The conception is developed for a decision problem of a food retail company where a society must be prevented from threatening food shortages due to a flu epidemic in Berlin, Germany. © 2015 The Authors. Published by Elsevier Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944524446&partnerID=40&md5=710f23c02d715dbb3538b38532144345","Starting from the state-of-the-art and recent evolutions in the field of system dynamics modeling and simulation, this chapter sketches a plausible near term future of the broader field of systems modeling and simulation. In the near term future, different systems modeling schools are expected to further integrate and accelerate the adoption of methods and techniques from related fields like policy analysis, data science, machine learning, and computer science. The resulting future state of the art of the modeling field is illustrated by three recent pilot projects. Each of these projects required further integration of different modeling and simulation approaches and related disciplines as discussed in this chapter. These examples also illustrate which gaps need to be filled in order to meet the expectations of real decision makers facing complex uncertain issues. © Springer International Publishing Switzerland 2015."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939499206&partnerID=40&md5=e3b034b3354a8af8acabc189dda3be1b","Water systems planners have long recognized the need for robust solutions capable of withstanding deviations from the conditions for which they were designed. Robustness analyses have shifted from expected utility to exploratory bottom-up approaches which identify vulnerable scenarios prior to assigning likelihoods. Examples include Robust Decision Making (RDM), Decision Scaling, Info-Gap, and Many-Objective Robust Decision Making (MORDM). We propose a taxonomy of robustness frameworks to compare and contrast these approaches based on their methods of (1) alternative generation, (2) sampling of states of the world, (3) quantification of robustness measures, and (4) sensitivity analysis to identify important uncertainties. Building from the proposed taxonomy, we use a regional urban water supply case study in the Research Triangle region of North Carolina to illustrate the decision-relevant consequences that emerge from each of these choices. Results indicate that the methodological choices in the taxonomy lead to the selection of substantially different planning alternatives, underscoring the importance of an informed definition of robustness. Moreover, the results show that some commonly employed methodological choices and definitions of robustness can have undesired consequences when ranking decision alternatives. For the demonstrated test case, recommendations for overcoming these issues include: (1) decision alternatives should be searched rather than prespecified, (2) dominant uncertainties should be discovered through sensitivity analysis rather than assumed, and (3) a carefully elicited multivariate satisficing measure of robustness allows stakeholders to achieve their problem-specific performance requirements. This work emphasizes the importance of an informed problem formulation for systems facing challenging performance tradeoffs and provides a common vocabulary to link the robustness frameworks widely used in the field of water systems planning. © 2015 American Society of Civil Engineers."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939545496&partnerID=40&md5=57d63ab39323aaffc6f38e01da4a1147","Managing ecosystems with deeply uncertain threshold responses and multiple decision makers poses nontrivial decision analytical challenges. The problem is imbued with deep uncertainties because decision makers do not know or cannot converge on a single probability density function for each key parameter, a perfect model structure, or a single adequate objective. The existing literature on managing multistate ecosystems has generally followed a normative decision-making approach based on expected utility maximization (MEU). This approach has simple and intuitive axiomatic foundations, but faces at least two limitations. First, a prespecified utility function is often unable to capture the preferences of diverse decision makers. Second, decision makers’ preferences depart from MEU in the presence of deep uncertainty. Here, we introduce a framework that allows decision makers to pose multiple objectives, explore the trade-offs between potentially conflicting preferences of diverse decision makers, and to identify strategies that are robust to deep uncertainties. The framework, referred to as many-objective robust decision making (MORDM), employs multiobjective evolutionary search to identify trade-offs between strategies, re-evaluates their performance under deep uncertainty, and uses interactive visual analytics to support the selection of robust management strategies. We demonstrate MORDM on a stylized decision problem posed by the management of a lake in which surpassing a pollution threshold causes eutrophication. Our results illustrate how framing the lake problem in terms of MEU can fail to represent key trade-offs between phosphorus levels in the lake and expected economic benefits. Moreover, the MEU strategy deteriorates severely in performance for all objectives under deep uncertainties. Alternatively, the MORDM framework enables the discovery of strategies that balance multiple preferences and perform well under deep uncertainty. This decision analytic framework allows the decision makers to select strategies with a better understanding of their expected trade-offs (traditional uncertainty) as well as their robustness (deep uncertainty). © 2015 by the author(s)."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926387585&partnerID=40&md5=347ec641608b4fb4a723a03604a03286","The robustness analysis is a substantial and debatable issue in the multi-criteria decision support systems. The multi-criteria decision support systems always depend on preference information from a decision maker. Although it is evident that elicited preference information has a great impact on the result, research on the preference robustness in multi-criteria decision support system rather narrow in contrast to the well-established research robust decision making. This paper focuses on the multi-criteria decision support systems based on the multiple criteria sorting methods. The paper proposes an analyzing approach based on the concept of the preference robustness. The approach involved in the multi-criteria decision support system can provide valuable insight to a decision maker on how preferences influence the choice among alternatives. Moreover, the approach can be combined with other decision aiding methods as an additional technique for deriving a consensus when there are multiple preferences. © Springer International Publishing Switzerland 2015."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907783083&partnerID=40&md5=0bae1b585b7ed1cdf60be2f6f0b2d5aa","This paper presents a novel Primal-Dual Interior Point Method (PDIPM) based sensitivity approach for efficient assessment of the impact of uncertainties in Multi-objective Optimization (MO). This shall aid in robust decision making. The MO problem considered, in this paper, is the Environmental-Economic dispatch (EED) problem. The two objectives, i.e. the emission and economic cost, are continuous convex functions. The uncertainties in the system parameters such as loads (or injections) and limits on line flows and voltage magnitudes, are assumed to be of fuzzy type, more specifically in an interval. Results for the IEEE 30 bus system have been obtained using the proposed approach and compared with those obtained by Monte Carlo Simulations (MCS) and Particle Swarm Optimization (PSO) based on Harmony Search (HSPSO). The results obtained provide interesting insights on how uncertainties in input data can affect decision making in MO. © 2014 Elsevier Ltd. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921966538&partnerID=40&md5=cb252413f7078340d0ef4fe415dbbb77","Multiple-positive feedback circuits are ubiquitous regulatory motifs in complex bio-molecular networks. A popular topic is why multiple-positive feedback mechanisms have been evolved and selected by organisms. To this end, a two-component dual-positive feedback genetic circuit is investigated, which consists of an auto-activation loop and a double negative feedback circuit. The auto-activation loop acts as an additional positive feedback loop (APFL), and our aim is to explore the functional characteristics of the APFL. Investigations reveal that the APFL can regulate the size of bistable region and the robust attractiveness of stable steady states. It is also found that the APFL can regulate global relative input–output sensitivities of the system. Furthermore, the APFL can tune the response speed, noise resistance and stochastic switch behavior of the system, which makes it easy to realize functional tunability and robust decision-making. Therefore, rationalizing why multiple-positive feedback circuits so frequently appear in real-world biological systems. Potential applications of the associated investigations include the design of artificial genetic circuits, the modeling and model reduction for large-scale bio-molecular networks. © 2014, Springer Science+Business Media Dordrecht."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944874118&partnerID=40&md5=ab05651c676fa75a02a43086647cda67","Progress in sustainability science is hindered by challenges in creating and managing complex data acquisition, processing, simulation, post-processing, and intercomparison pipelines. To address these challenges, we developed the Framework to Advance Climate, Economic, and Impact Investigations with Information Technology (FACE-IT) for crop and climate impact assessments. This integrated data processing and simulation framework enables data ingest from geospatial archives; data regridding, aggregation, and other processing prior to simulation; large-scale climate impact simulations with agricultural and other models, leveraging high-performance and cloud computing; and post-processing to produce aggregated yields and ensemble variables needed for statistics, for model intercomparison, and to connect biophysical models to global and regional economic models. FACE-IT leverages the capabilities of the Globus Galaxies platform to enable the capture of workflows and outputs in well-defined, reusable, and comparable forms. We describe FACE-IT and applications within the Agricultural Model Intercomparison and Improvement Project and the Center for Robust Decision-making on Climate and Energy Policy. © 2015 John Wiley & Sons, Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958983941&partnerID=40&md5=ca3b65f1a7eeedb701c87d63489ddf2e","Probabilistic Risk Assessment (PRA) has proven to be an invaluable tool for evaluating risks in complex engineered systems. However, there is increasing concern that PRA may not be adequate in situations with little underlying knowledge to support probabilistic representation of uncertainties. As analysts and policy makers turn their attention to deeply uncertain hazards such as climate change, a number of alternatives to traditional PRA have been proposed. These include the use of qualitative descriptions of underlying knowledge that supports a PRA, non-probabilistic representations of uncertainty, and robustness-based approaches. However, these techniques have largely been developed in isolation from each other, and few comparisons between them exist. This paper compares three approaches for risk analysis under deep uncertainty: qualitative uncertainty factors, probability bounds analysis, and robust decision making. Each method is applied to a simple example problem, allowing for systematic comparison of the requirements for each approach, the procedures by which they develop risk descriptions, and the information provided to decision makers. This systematic comparison is aimed at evaluating how each approach relates to the traditional risk assessment process and fundamental issues associated with risk assessment and description. © 2015 Taylor & Francis Group, London."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922684275&partnerID=40&md5=c5871778dccce209e5006645d1fc7155","In many situations, decision-makers need to exceed a random target or make decisions using expected utilities. These two situations are equivalent when a decision-makers utility function is increasing and bounded. This article focuses on the problem where the random target has a concave cumulative distribution function (cdf) or a risk-averse decision-makers utility is concave (alternatively, the probability density function (pdf) of the random target or the decision-maker marginal utility is decreasing) and the concave cdf or utility can only be specified by an uncertainty set. Specifically, a robust (maximin) framework is studied to facilitate decision making in such situations. Functional bounds on the random targets cdf and pdf are used. Additional general auxiliary requirements may also be used to describe the uncertainty set. It is shown that a discretized version of the problem may be formulated as a linear program. A result showing the convergence of discretized models for uncertainty sets specified using continuous functions is also proved. A portfolio investment decision problem is used to illustrate the construction and usefulness of the proposed decision-making framework. © 2015 ""IIE""."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938236112&partnerID=40&md5=ae1ea0c7b082109dab325c4ffb30b135","This paper presents an intention-aware online planning approach for autonomous driving amid many pedestrians. To drive near pedestrians safely, efficiently, and smoothly, autonomous vehicles must estimate unknown pedestrian intentions and hedge against the uncertainty in intention estimates in order to choose actions that are effective and robust. A key feature of our approach is to use the partially observable Markov decision process (POMDP) for systematic, robust decision making under uncertainty. Although there are concerns about the potentially high computational complexity of POMDP planning, experiments show that our POMDP-based planner runs in near real time, at 3 Hz, on a robot golf cart in a complex, dynamic environment. This indicates that POMDP planning is improving fast in computational efficiency and becoming increasingly practical as a tool for robot planning under uncertainty. © 2015 IEEE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84912558988&partnerID=40&md5=a344559702d00aed22aa1a231de62c40","National infrastructure systems (energy, transport, digital communications, water, and waste) provide essential services to society. Although for the most part these systems developed in a piecemeal way, they are now an integrated and highly interdependent ""system of systems."" However, understanding the long-term performance trajectory of national infrastructure has proved to be very difficult because of the complexity of these systems (in physical and institutional terms) and because there is little tradition of thinking cross-sectorally about infrastructure system performance. Here, a methodology is proposed for analyzing national multisectoral infrastructure systems performance in the context of uncertain futures, incorporating interdependencies in demand across sectors. Three contrasting strategies are considered for infrastructure provision (capacity intensive, capacity constrained, and decentralized) and multiattribute performance metrics are analyzed in the context of low, medium, and high demographic and economic growth scenarios. The approach is illustrated using Great Britain and provides the basis for the development and testing of long-term strategies for national infrastructure provision. It is especially applicable to mature industrial economics with a large stock of existing infrastructure and challenges of future infrastructure provision. © 2014 American Society of Civil Engineers."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944519930&partnerID=40&md5=feddf114f20f6be3a47e1018256ba22d","This chapter highlights the benefits of framing adaptation to climate change as an issue of climate risk management and describes a number of methods and approaches that may be applied in the process of developing adaptation strategies. A key consideration when developing adaptation strategies is to have a sound understanding of how a given system functions in response to changes in both climate and non-climate factors, and thus the need for a causal model which represents this understanding. There are a range of methods and tools that may be applied to assist with developing this system understanding in a climate risk assessment, and a number of these are described here. Moreover, given that adaptation planning is to a large degree about forward planning, all adaptation strategies will need to appropriately consider the implications of uncertainty on their likely effectiveness. This chapter provides a discussion of the ways in which adaptation strategies can be developed and decisions made when appraising different adaptation strategies. As such, it provides a basis upon which users can assess how they may approach adaptation as an issue of climate risk management and select and apply suitable methods. It provides a useful accompaniment to any practitioner or organization, as they proceed on their adaptation journey. © Springer-Verlag Berlin Heidelberg 2015."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947041776&partnerID=40&md5=f993f594c8b33efc21799475ac4bbc14","Coastal areas provide a wealth of resources and represent the most developed spaces worldwide. As pressures on these resources increase, managers and policymakers require understanding that facilitates comprehensive valuation of ecosystem services, including social–ecological dynamics and the coupling of social and natural systems. Adoption of ecosystem management approaches has created a need for increased knowledge of social systems commensurate to baseline ecological understanding; that is, managers benefit from integrating stakeholder values into decision-making processes. This study, using the Delphi technique, elicited knowledge and opinions from international coastal experts regarding relevant social values of ecosystem services, facilitating integration of multiple perspectives into an informed consensus typology of 16 social values. Agencies across the board see inclusive evaluation of ecosystem services, including social values, as critical to robust decision making regarding relevant resources. The next step for research findings is application of the typology in resource assessment initiatives within research and practice-based efforts. © 2015, Copyright © Taylor & Francis Group, LLC."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940544036&partnerID=40&md5=4bb36bf79aa18b9a27756fc7ee145150","The research in the field of microalgae-based biofuels and chemicals is in early phase of the development, and therefore a wide range of uncertainties exist due to inconsistencies among and shortage of technical information. In order to handle and address these uncertainties to ensure robust decision making, we propose a systematic framework for the synthesis and optimal design of microalgae-based processing network under uncertainty. By incorporating major uncertainties into the biorefinery superstructure model we developed previously, a stochastic mixed integer nonlinear programming (sMINLP) problem is formulated for determining the optimal biorefinery structure under given parameter uncertainties modelled as sampled scenarios. The solution to the sMINLP problem determines the optimal decisions with respect to processing technologies, material flows, and product portfolio in the presence of uncertain parameters. The developed framework is implemented and tested on a specific case study, to identify the promising processing pathway for the production of biofuels from microalgae while accounting for modelled uncertainties. © 2015 Elsevier B.V."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940514818&partnerID=40&md5=0d9d8d403f89b42bd290ecc36eedeea9","Effective uncertainty evaluation is a critical step toward real-time and robust decision-making for complex systems in uncertain environments. A Multivariate Probabilistic Collocation Method (M-PCM) was developed to effectively evaluate system uncertainty. The method smartly chooses a limited number of simulations to produce a low-order mapping, which precisely predicts the mean output of the original system mapping up to certain degrees. While the M-PCM significantly reduces the number of simulations, it does not scale with the number of uncertain parameters, making it difficult to use for large-scale applications that typically involve a large number of uncertain parameters. In this paper, we develop a method to break the curse of dimensionality. The method integrates M-PCM and Orthogonal Fractional Factorial Designs (OFFDs) to maximally reduce the number of simulations from 22m to 2⌈log2(m+1)⌉ for a system mapping of m parameters. The integrated M-PCM-OFFD predicts the correct mean of the original system mapping, and is the most robust to numerical errors among all possible designs of the same number of simulations. The analysis also provides new insightful formal interpretations on the optimality of OFFDs. © 2014 IEEE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945537843&partnerID=40&md5=669a10d9ec56221d4700898882512c3f","Policy goals to transition national energy systems to meet decarbonisation and security goals must contend with multiple overlapping uncertainties. These uncertainties are pervasive through the complex nature of the system, the long term consequences of decisions, and in the models and analytical approaches used. These greatly increase the challenges of informing robust decision making. Energy system studies have tended not to address uncertainty in a systematic manner, relying on simple scenario or sensitivity analysis. This paper utilises an innovative UK energy system model, ESME, which characterises multiple uncertainties via probability distributions and propagates these uncertainties to explore trade-offs in cost effective energy transition scenarios. A linked global sensitivity analysis is used to explore the uncertainties that have most impact on the transition. The analysis highlights the strong impact of uncertainty on delivering the required emission reductions, and the need for an appropriate carbon price. Biomass availability, gas prices and nuclear capital costs emerge as critical uncertainties in delivering emission reductions. Further developing this approach for policy requires an iterative process to ensure a complete understanding and representation of different uncertainties in meeting mitigation policy objectives. © 2015 The Authors."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940051202&partnerID=40&md5=b68022c3fd1fb2a49f81bb19b435bb54","The provision of an adequate network of urban infrastructures is essential to create clean and energy-efficient urban mobility systems. However, the urban infrastructure to support sustainable mobility can produce a substantial environmental burden if no life cycle environmental criteria are applied in its design and management. This paper demonstrates the potential to support energy-efficient and CO<inf>2</inf>-free pedestrian and electric bike (e-bike) mobility through the ecological design (eco-design) of urban elements. An eco-design approach is applied to reconceptualize a conventional pergola toward an eco-product (solar pergola). The solar pergola generates surplus photovoltaic electricity that provides a multifunctional character. According to the end-use of this energy, different scenarios are analyzed for robust decision-making. The deployment of solar pergolas can contribute to save from 2,080kg to over 47,185kg of CO<inf>2</inf> eq. and from 350,390MJ to over 692,760MJ eq. in 10 years, depending on the geographic emplacement (solar radiation and electricity grid system). These savings are equivalent to charging 2-9 e-bikes per day using clean energy. Instead of maximizing infrastructure deployment to shift to environmentally friendly modes of mobility, the implementation of multifunctional urban elements represents a key area of action in the context of smart city development. © 2015 Elsevier Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922422529&partnerID=40&md5=1b19e721c83d2d08d00394f45af70536","This paper addresses the NASA Langley Research Center's Multidisciplinary Uncertainty Quantification Challenge problem, which is intended to pose challenges to the uncertainty quantification and robust design communities. The goals of the Multidisciplinary Uncertainty Quantification Challenge problem can be formulated into four main topics that are commonly encountered in the model development process: calibration, sensitivity analysis, extreme-case analysis, and robust design. The analysis presented herein places a particular emphasis on the use of info-gap decision theory to address the goals of the Multidisciplinary Uncertainty Quantification Challenge problem. Info-gap decision theory provides a convenient framework to quantify the effect of uncertainty, herein referred to as robustness, when using simulation models for decision making. Robustness, as defined in the context of info-gap decision theory, is used to pursue calibration, uncertainty propagation, and robust design. Calibration is performed using info-gap decision theory to address the situation whereby deterministic calibration might result in nonunique solutions, meaning that different sets of calibration variables are able to replicate experiments with comparable fidelity. Extreme-case analysis is performed such that the worst-case and best-case performances of the model output are conditioned on the level of uncertainty that ispermitted in the simulations.To pursue robust design, the robustness criterion is used to establish whether the amount of uncertainty tolerable in the optimized design is an improvement over the baseline design. Our analysis demonstrates that improving the robustness of the model requires different knowledge than improving the performance of the model. The main conclusion is that the info-gap decision theory provides a sound theoretical basis, and practical implementation, to meet the goals of the NASA Multidisciplinary Uncertainty Quantification Challenge problem without formulating simplifying assumptions."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948461462&partnerID=40&md5=19f7febb908f323494762ced324de0a0","The need for agility in operational systems within the defence enterprise and procurement domains has been identified by many authors, and over time, there have been a number of initiatives and programmes that have sought to identify the nature of agility, and the means by which it can be defined and employed within individual cases and scenarios. These have identified impediments to the successful realization of agile practices and methods, particularly the resilience of agile decision making throughout the conceptual understanding, design and implementation of the operational system. To further investigate the extent to which this process can be implemented in a robust and reliable manner, Cranfield University created the 'Robust Enterprise-based Approach for Agility in Capability Through-life (REA2CT)' framework, which provides a number of functional steps to institute a systems development lifecycle approach to producing agile solutions for use in networked systems and systems-of-systems. This paper briefly examines the Customer Need (CN) for the enterprise-based delivery of system (of systems) agility into the operational domain. Axiomatic Design (AD) theory is used to describe the REA2CT framework, identifying Functional Requirements (FRs) which might satisfy the CN for agility. Initial Design Parameters (DPs) are proposed to satisfy the FRs. © 2015 The Authors. Published by Elsevier B.V."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948451855&partnerID=40&md5=e008b75ec6a206edd5a4352d78142df3","The need for agility in operational systems within the defence enterprise and procurement domains has been identified by many authors, and over time, there have been a number of initiatives and programmes that have sought to identify the nature of agility, and the means by which it can be defined and employed within individual cases and scenarios. These have identified impediments to the successful realization of agile practices and methods, particularly the resilience of agile decision making throughout the conceptual understanding, design and implementation of the operational system. To further investigate the extent to which this process can be implemented in a robust and reliable manner, Cranfield University created the 'Robust Enterprise-based Approach for Agility in Capability Through-life (REA2CT)' framework, which provides a number of functional steps to institute a systems development lifecycle approach to producing agile solutions for use in networked systems and systems-of-systems. This paper builds upon the description of the framework[1] by applying the Axiomatic Design (AD) theory to identify where complexity exists within the requirements and design activities that underpin the framework. Using this analysis, this paper identifies 'pain points' within the REA2CT framework, and suggests necessary improvements to facilitate the implementation of agility throughout the systems development lifecycle. © 2015 The Authors. Published by Elsevier B.V."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947571814&partnerID=40&md5=77f613bdabc4325f269af6299d3fcd46","This paper presents an evidential reasoning (ER)-based method of fault diagnosis by combining uncertain information of various fault features collected from multiple sources for fault decision-making. A normalization approach is applied to acquire diagnosis evidence from the likelihood function of fault feature samples gathered from information sources (sensors). A novel method is proposed to calculate evidence reliability according to sensor accuracy specifications and the differences of capabilities in recognizing fault modes through different fault features. A bi-objective optimization model is presented to train evidence weights to reflect the relative importance of evidence. The ER rule is then applied to combine multiple pieces of diagnosis evidence, which are regulated by their weights and reliability factors, and fault decision-making can thus be conducted on the basis of the combined results. The proposed ER-based fault diagnosis method inherits the main features of Dempster-Shafer evidence theory in uncertainty modelling, while providing a systematic process for explicitly taking into account the reliability and importance of evidence, thereby enabling rigorous inference and robust decision making. Finally, a diagnosis experiment on a rotor test bed is conducted to show the effectiveness of the proposed ER-based fault diagnosis method. ©, 2015, South China University of Technology. All right reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937636330&partnerID=40&md5=dd9043c70902218556417e36775cce54","Coastal cities are growing at a very rapid pace, in terms of both population and physical assets, and great uncertainty surrounds the future evolution of hurricane intensity and sea-level rise. The combination of these trends will contribute to large financial losses due to property damage in the absence of specific protections. Southeast Florida represents a clear hot spot of coastal flood exposure: more than 5 million inhabitants live in the counties of Miami Dade, Broward, and Palm Beach, and the population is still growing. It is also a low-lying area where tropical hurricanes hit frequently. This article illustrates a methodology to assess coastal flood damage in urban settlements and it aims to determine more general lessons useful for all coastal cities. We consider the impact of different storm surges predicted by the SLOSH model and investigate flood risk considering different types of hurricanes. For each event, we apply a specific damage function and determine whether the considered storm surges potentially lead to an asset loss, considering both properties and their contents. The results show that, in the absence of protections, losses will be very high for large storm surges, reaching up to tens of billions of US dollars. In the second part of the analysis, we demonstrate how economic impact changes when protections are built up, considering different heights of protections. These results could be used as inputs into a robust decision-making process to determine the future of coastal protection in southern Florida. © 2014 Taylor & Francis."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84935053870&partnerID=40&md5=464bb12f8fc8f4029d8742e2c6c18a38","Although precise severity of climate change remains highly uncertain, its impact on quantity and quality of water resources is certain. In such an uncertain environment, planners and water resource specialists are faced with the challenge of properly allocating pollutants to water bodies and resulting discharge permits. In their investigations, specialists have to either explicitly or implicitly account for the uncertainty of climate change scenarios to plan for the most appropriate load allocations. Due to the large number of feasible combinations of load reductions and prevailing uncertainties, the use of an efficient optimization approach may prove to be quite beneficial. This paper presents a robust decision making and modeling approach to develop pollutant load allocation with Total Maximum Daily Load (TMDL) approach realizing the uncertainties involved in climate changes. Models provide a linkage between loads and receiving water conditions while maximizing the allowable total daily load. The approach employs the non-probabilistic regret approach to deal with the uncertainty involved in different climate change scenarios. Water quality responses are simulated using the QUAL2K model. The charged system search algorithms are utilized for maximizing the total daily load under various climate change scenarios to be used in regret analysis. To illustrate the performance and applicability of the methodology, it is applied to the New River system from the International Boundary to the outlet at the Salton Sea. © 2015 ASCE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938930709&partnerID=40&md5=42862c36e62302c3aa6f04c03ba640fc","This paper aims to discuss a decision support model for automated railway level crossing (LC) using fuzzy logic control (FLC) for providing robust decision making at unmanned railway level crossings, to save the overall operation time, to avoid any accidental fatalities, and to eliminate human errors. The decision support model proposed here provides intelligent decisive action signals as similar to a human brain (e.g. during arrival and departure of trains at railway level crossing). FLC model is designed which recognizes the events (i.e. arrival and departure of trains) and accordingly output action signals are generated (i.e. to warning siren, control actions for opening and closing of gates). This type of model can be implemented in unmanned railway level where the chances of accidents are higher and reliable control operation is required. Three primary inputs to the specified model are considered based on visual, acoustic, and vibration. This novel system makes use of all these three parameters as input for its decision taking parameters, which increases the robustness of this model as compared with previously proposed models where the input is dependent on a single event. The FLC structure implemented to generate this model is multiple input multiple output (MIMO) system. © 2015 The Authors. Published by Elsevier B.V."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923320918&partnerID=40&md5=2eb463c9f5d80165e27101dc77fea271","Impairment of an individual's ability to communicate is a major hurdle for active participation in education and social life. A lot of individuals with cerebral palsy (CP) have normal intelligence, however, due to their inability to communicate, they fall behind. Non-invasive electroencephalogram (EEG) based brain-computer interfaces (BCIs) have been proposed as potential assistive devices for individuals with CP. BCIs translate brain signals directly into action. Motor activity is no longer required. However, translation of EEG signals may be unreliable and requires months of training. Moreover, individuals with CP may exhibit high levels of spontaneous and uncontrolled movement, which has a large impact on EEG signal quality and results in incorrect translations. We introduce a novel thought-based row-column scanning communication board that was developed following user-centered design principles. Key features include an automatic online artifact reduction method and an evidence accumulation procedure for decision making. The latter allows robust decision making with unreliable BCI input. Fourteen users with CP participated in a supporting online study and helped to evaluate the performance of the developed system. Users were asked to select target items with the row-column scanning communication board. The results suggest that seven among eleven remaining users performed better than chance and were consequently able to communicate by using the developed system. Three users were excluded because of insufficient EEG signal quality. These results are very encouraging and represent a good foundation for the development of real-world BCI-based communication devices for users with CP. © 2015 Elsevier Masson SAS."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958559452&partnerID=40&md5=7327e384a297478e6acab0393a1bdc1b","Critical decision making with limited information and associated uncertainties is a challenge at every level of the hydrocarbon value chain. This is even more so at early stages of field development projects when geosciences and engineering data are sparse and the understanding of the geological complexities is still limited. Thus, the need for building multiple realizations of representative reservoir models that captures the full range of subsurface uncertainties is crucial to ensure a robust decision-making process. From previous studies, the Reservoir Complex, X is highlighted as a stack of two gas-bearing reservoirs that merge to form a doublet as observed on seismic and share a common contact towards the flank of the structure. This interpretation informed the modelling of both reservoirs as a single mega unit. An Integrated Reservoir Modeling approach rather than a discipline-focused one was adopted to evaluate the range of uncertainties in the reservoirs. For this study, a multidisciplinary subsurface team built a scenario-based model and performed Sensitivity analysis to identify key uncertainties on input parameters that are most impactful on in-place volumes and recoverables. This paper discusses techniques employed in building the static/dynamic models, generating estimates for the various uncertainties and how these were analyzed to identify the 'heavy hitters'. Results from this study identified Structure, Net-to-Gross and Porosity as the top three uncertainties with most impact on static volumes while Structure, PVT and Aquifer size have most impact on recoveries. Deterministic low, base and high case In-place volumes computed are 509, 627 and 769 BScf while recoveries were 182, 339 and 643BScf respectively. Probabilistically, the in-place volumes were 336, 480 and 634 Bscf while the recoverables were 205, 348 and 550 Bscf respectively. Copyright 2015, Society of Petroleum Engineers."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946059614&partnerID=40&md5=3182de082612c84b2a977b7f162590d6","Nowadays hazard identification and risk assessment play an established and fundamental role for the prevention of major accidents in the process industries. Despite their proved effectiveness, many hazard identification and risk assessment techniques lack the dynamic dimension, which is the ability to learn from new risk notions, experience and early warnings. Nevertheless, recent major disasters have raised the need to go beyond the limits of conventional static methods for hazard identification and risk assessment. The necessity to address risk issues in a continuously evolving environment, coupled with improved information and communication technologies, led in the last few years to the development of several advanced dynamic techniques for hazard identification and risk assessment in process systems. Eventually dynamic approaches to risk have proved to be capable of identifying and assessing emerging and increasing risks throughout the lifetime of the process. Recent applications have shown the effectiveness of dynamic approaches to major accidents, as well as to maintenance activities. Despite the relevant differences among the mentioned approaches, all these dynamic methods aim at dealing with uncertainties, system complexity, real-time changing environments and real-time information from different sources with enhanced flexibility, in respect to conventional approaches. The present study addresses dynamic approaches to hazard identification and risk assessment in the process industry. These novel methods will be inserted in the broader framework of dynamic risk management. These techniques will be joined with representative applications based on real events. The results of the mentioned applications are used to show how risk can be assessed by means of continuous activities of monitoring and review, coupled with real time risk evaluation. The ability of such dynamic approaches to capture general failures and risk management deficits demonstrate their effectiveness, both in risk management and in the prevention of major accidents, providing a more robust decision-making within the process industry context. Copyright © 2015, AIDIC Servizi S.r.l."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942015585&partnerID=40&md5=41b8eb01a61bd91507e5802911829d64","The life cycle cost and environmental impacts of electric vehicles are very uncertain, but extremely important for making policy decisions. This study presents a new model, called the Electric Vehicles Regional Optimizer, to model this uncertainty and predict the optimal combination of drivetrains in different U.S. regions for the year 2030. First, the life cycle cost and life cycle environmental emissions of internal combustion engine vehicles, gasoline hybrid electric vehicles, and three different Electric Vehicle types (gasoline plug-in hybrid electric vehicles, gasoline extended range electric vehicle, and all-electric vehicle) are evaluated considering their inherent uncertainties. Then, the environmental damage costs and the water footprint of the studied drivetrains are estimated. Additionally, using an Exploratory Modeling and Analysis method, the uncertainties in the life cycle cost, environmental damage cost, and water footprint of studied vehicle types are modeled for different U.S. electricity grid regions. Finally, an optimization model is coupled with Exploratory Modeling and Analysis to find the ideal combination of different vehicle types in each U.S. region for the year 2030. The findings of this research will help policy makers and transportation planners to prepare our nation's transportation system for the influx of electric vehicles. © 2015 Elsevier Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923841945&partnerID=40&md5=24537ccf579f769499a28ad88734cee3","In the 2014 Ebola outbreak in West Africa, sociocultural, psychological and higher-order disease-related dynamics play an important role in the speed of virus transmission. Although such effects may strongly affect outbreaks, they are often not included in transmission models. Here, we include different combinations of these effects in an extended system dynamics transmission model to generate and explore ensembles of plausible future dynamics of the Ebola outbreak and test the effectiveness of sets of policies in the presence of these effects under deep uncertainty. Accounting for these effects, it seems that policies currently being implemented to curb the ongoing Ebola epidemic are, or could be made, sufficient to curb the epidemic by early 2015 unless psychological and sociocultural effects remain adverse. © 2015 John Wiley & Sons, Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84932600124&partnerID=40&md5=96ff82b9a39766cfe9c3a41e1bd59254","Biomethane is a renewable alternative to natural gas. It has the potential to increase the sustainability of the energy system and to help deal with supply problems. However, several factors make the future of biomethane production complex and uncertain, such as resource availability, demand, capacity installation, profitability and the competition between the biomethane and electricity sectors for sharing the available biogas and biomass resources. In this research, we study the dynamics of the Dutch biomethane production and analyze the effects of subsidization policy with a system dynamics model. The policy is tested under uncertainty with respect to three conflicting objectives, namely maximizing production and emission reduction, and minimizing costs. According to the results, the subsidization is crucial to develop biomethane production, and the performance of the policy is enhanced in terms of robustness and of meeting all three objectives satisfactorily when the policy is implemented for a long time, with relatively low subsidy prices. Besides, the subsidization policy is found to be most vulnerable to the producers' uncertain investment response to profitability. In future research, different policy options such as subsidizing other biomass-based renewable energy options and policies affecting the biomethane demand can be tested. © 2015 Elsevier Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923511359&partnerID=40&md5=9553bad84a1a994771c627be50d5f9a4","This study showcases the dynamic simulation capabilities of the Urban Biophysical Environments And Technologies Simulator (UrbanBEATS) on a Melbourne catchment. UrbanBEATS simulates the planning, design and implementation of water sensitive urban design (WSUD) infrastructure in urban environments. It considers explicitly the interaction between urban and water infrastructure planning through time. The model generates a large number of realizations of different WSUD interventions and their evolution through time based on a user-defined scenario. UrbanBEATS' dynamics was tested for the first time on a historical case study of Scotchman's Creek catchment and was trained using historical data (e.g. planning documents, narratives, urban development and societal information) to adequately reproduce patterns of uptake of specific WSUD technologies. The trained model was also used to explore the implications of more stringent future water management objectives. Results highlighted the challenges of meeting this legislation and the opportunities that can be created through the mix of multiple spatial scales. © IWA Publishing 2015."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922990374&partnerID=40&md5=acd1b151587e3295589cdbd09fd5cff0","The current review examined modeling literature in top science education journals to better understand the pedagogical functions of modeling instruction reported over the last decade. Additionally, the review sought to understand the extent to which different modeling pedagogies were employed, the discursive acts that were identified as important, and the technology leveraged in the pursuit of engaging students in developing and using models. After narrowing from 783 articles originally identified with an abstract keyword search, the literature review included a database of 81 research articles whose abstracts revealed a focus on modeling as an instructional intervention and contained learner modeling. A multistage process was then completed whereby each article was read and information from the articles were identified and discussed among a group of five researchers. The most salient findings identified in the research included (a) conceptual understanding was the most common pedagogical function identified for modeling, while developing facility and understanding of science practices was identified least often, (b) Expressive modeling was the most frequently used and sequences which connected Exploratory and Experimental modeling were the most frequently observed combination of modeling pedagogies, (c) the most important discursive acts identified as important were scientific reasoning, explanation, and peer-to-peer collaborative/cooperative learning, and (d) technology was used in approximately one-half of the research reviewed, with Expressive and Exploratory modeling pedagogies found most often supported or mediated by technology. © 2014 by iSER, International Society of Educational Research."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937677686&partnerID=40&md5=f353e43abb1113289f4cf9f25cef3371","Purpose: To accurately hypothesize the optimal frequency of psychosocial distress screening in patients undergoing radiation therapy using exploratory modeling of prospective data. Materials and Methods: Between October 2010 and May 2011, 71 RT patients underwent daily screening with the Distress Thermometer. Prevalences of Distress Thermometer scores ≥ 4 were recorded. Optimal screening frequency was evaluated by planned post hoc comparison of prevalence rates and required screening events estimated by numerical modeling, consisting of data point omission to mimic weekly, every-other-week, monthly, and one-time screening intervals. Dependence on clinical variables and chronologic trends were assessed as secondary end points. Results: A total of 2,028 daily screening events identified that 37% of patients reported distress at least once during the course of treatment. Weekly, every-other-week, monthly, and one-time screening models estimated distress prevalences of 32%, 31%, 23%, and 17%, respectively, but required only 21%, 12%, 7%, and 4% of the assessments required for daily screening. No clinical parameter significantly predicted distress in univariable analysis, but ""alone"" living situation trended toward significance (P = .06). Physician-reported grade 3 toxicity predicted distress with 98% specificity, but only 19% sensitivity. Conclusion: Thirty-seven percent of radiation oncology patients reported distress at least once during treatment. Screening at every-other-week intervals optimized efficiency and frequency, identifying nearly 90% of distressed patients with 12% of the screening events compared with daily screening. Copyright © 2015 by American Society of Clinical Oncology."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907663311&partnerID=40&md5=ee303e0fb622797ae8a2e877404852d2","Long-term projections for key drivers needed in urban water infrastructure planning such as climate change, population growth, and socio-economic changes are deeply uncertain. Traditional planning approaches heavily rely on these projections, which, if a projection stays unfulfilled, can lead to problematic infrastructure decisions causing high operational costs and/or lock-in effects. New approaches based on exploratory modelling take a fundamentally different view. Aim of these is, to identify an adaptation strategy that performs well under many future scenarios, instead of optimising a strategy for a handful. However, a modelling tool to support strategic planning to test the implication of adaptation strategies under deeply uncertain conditions for urban water management does not exist yet. This paper presents a first step towards a new generation of such strategic planning tools, by combing innovative modelling tools, which coevolve the urban environment and urban water infrastructure under many different future scenarios, with robust decision making. The developed approach is applied to the city of Innsbruck, Austria, which is spatially explicitly evolved 20 years into the future under 1000 scenarios to test the robustness of different adaptation strategies. Key findings of this paper show that: (1) Such an approach can be used to successfully identify parameter ranges of key drivers in which a desired performance criterion is not fulfilled, which is an important indicator for the robustness of an adaptation strategy; and (2) Analysis of the rich dataset gives new insights into the adaptive responses of agents to key drivers in the urban system by modifying a strategy. © 2014 Elsevier Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922433050&partnerID=40&md5=4273b7b1f0f5a4f7d5916828cf3bc5de","Chemical regulation is challenged by the large number of chemicals requiring assessment for potential human health and environmental impacts. Current approaches are too resource intensive in terms of time, money and animal use to evaluate all chemicals under development or already on the market. The need for timely and robust decision making demands that regulatory toxicity testing becomes more cost-effective and efficient. One way to realize this goal is by being more strategic in directing testing resources; focusing on chemicals of highest concern, limiting testing to the most probable hazards, or targeting the most vulnerable species. Hypothesis driven Integrated Approaches to Testing and Assessment (IATA) have been proposed as practical solutions to such strategic testing. In parallel, the development of the Adverse Outcome Pathway (AOP) framework, which provides information on the causal links between a molecular initiating event (MIE), intermediate key events (KEs) and an adverse outcome (AO) of regulatory concern, offers the biological context to facilitate development of IATA for regulatory decision making. This manuscript summarizes discussions at the Workshop entitled ""Advancing AOPs for Integrated Toxicology and Regulatory Applications"" with particular focus on the role AOPs play in informing the development of IATA for different regulatory purposes. © 2014 Elsevier Inc."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907954728&partnerID=40&md5=7de6011eb7a0d3f79c4d661a71b6b65f","Purpose: In Hot Mix Asphalt (HMA) overlays, the existing cracks in the underlying pavements can propagate upward to the new added overlay and may cause Reflective Cracks (RC). These cracks allow water infiltration to the underlying layers and causes further moisture damage as well as weakening the unbound layers. Over the years, several methods have been developed for mitigating the RCs. This study aims to investigate the current reflective cracking mitigation methods and develop a methodology for the selection of appropriate mitigation technique. The developed model is then applied to a case study in the state of Florida. Method: To accomplish this goal, a nationwide literature review was conducted to better understand the current in practice methods in the United States. Moreover, a life cycle cost analysis (LCCA) in five different road types was performed to find the annuity of roadway rehabilitation for each of the mitigation methods. The uncertainty in the LCCA results is represented using Exploratory Modeling and Analysis (EMA) method. Then through a Multi Criteria Decision Making (MCDM) model, a stochastic optimization model was developed to find the appropriate reflective cracking mitigation solution under Florida's climate and road conditions, based on different cost and performance weights. Results: Based on the available data for the state of Florida, the LCCA results indicate that the annuity of maintaining the roadway with Fabrics and ISAC are lower compared to other methods. However, the results of stochastic optimization model reveal that while looking at the performance and cost at the same time, different methods would be more feasible. For instance, while the cost of the used method does not matter at all and only performance matters, STRATA® is more probable to be the appropriate mitigation technique. The findings of this research are critical for decision makers to better understand the most cost-effective mitigation technique in different conditions. © 2014 Elsevier Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942363035&partnerID=40&md5=53b1ba1f7e464fbc6372032fe1e892e1","There is a growing focus on the economics of adaptation as policy moves from theory to practice. However, the techniques commonly used in economic appraisal have limitations in coping with climate change uncertainty. While decision making under uncertainty has gained prominence, economic appraisal of adaptation still uses approaches such as deterministic cost-benefit analysis. Against this background, this paper provides a critical review and assessment of existing economic decision support tools (cost-benefit analysis and cost-effectiveness analysis) an uncertainty framework (iterative risk management) and alternative tools that more fully incorporate uncertainty (real options analysis, robust decision making and portfolio analysis). The paper summarises each method, provides examples, and assesses their strengths and weaknesses for adaptation. The tools are then compared to identify key differences, and to identify when these approaches might be appropriate for specific applications in adaptation decision making. © 2014, Springer Science+Business Media Dordrecht."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910594225&partnerID=40&md5=f89f50b6bb6667f9433675dcb7b7868a","Tongariro volcano, New Zealand, lies wholly within the Tongariro National Park (TNP), one of New Zealand's major tourist destinations. Two small eruptions of the Te Maari vents on the northern flanks of Tongariro on 6 August 2012 and 21 November 2012 each produced a small ash cloud to <. 8. km height accompanied by pyroclastic density currents and ballistic projectiles. The most popular day hike in New Zealand, the Tongariro Alpine Crossing (TAC), runs within 2. km of the Te Maari vents. The larger of the two eruptions (6 August 2012) severely impacted the TAC and resulted in its closure, impacting the local economic and potentially influencing national tourism. In this paper, we document the science and risk management response to the eruption, and detail how quantitative risk assessments were applied in a rapidly evolving situation to inform robust decision-making for when the TAC would be re-opened. The volcanologist and risk manager partnership highlights the value of open communication between scientists and stakeholders during a response to, and subsequent recovery from, a volcanic eruption. © 2014 Elsevier B.V."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942364143&partnerID=40&md5=5ab0880789dd936a41e867a5a3892b69","Conventional forecast driven approaches to climate change adaptation create a cascade of uncertainties that can overwhelm decision makers and delay proactive adaptation responses. Robust Decision Making inverts the analytical steps associated with forecast-led methodologies, reframing adaptation in the context of a specific decision maker’s capacities and vulnerabilities. In adopting this bottom-up approach, the aim is to determine adaptation solutions which are insensitive to uncertainty. Yet despite the increased use of the approach in large-scale adaptation projects in developed countries, there is little empirical evidence to test whether or not it can be successfully applied in developing countries. The complex realities of decision making processes, the need to combine quantitative data with qualitative understanding and competing environmental, socio-economic and political factors all pose significant obstacles to adaptation. In developing countries, these considerations are particularly relevant and additional pressures exist which may limit the uptake and utility of the Robust Decision Making approach. In this paper, we investigate the claim that the approach can be deemed valuable in developing countries. Challenges and opportunities associated with Robust Decision Making, as a heuristic decision framework, are discussed with insights from a case study of adapting coastal infrastructure to changing environmental risks in South Africa. Lessons are extracted about the ability of this framework to improve the handling of uncertainty in adaptation decisions in developing countries. © 2014, Springer Science+Business Media Dordrecht."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908424259&partnerID=40&md5=7dc8307958559899c886f2b85be91159","Chemical regulation is challenged by the large number of chemicals requiring assessment for potential human health and environmental impacts. Current approaches are too resource intensive in terms of time, money and animal use to evaluate all chemicals under development or already on the market. The need for timely and robust decision making demands that regulatory toxicity testing becomes more cost-effective and efficient. One way to realize this goal is by being more strategic in directing testing resources; focusing on chemicals of highest concern, limiting testing to the most probable hazards, or targeting the most vulnerable species. Hypothesis driven Integrated Approaches to Testing and Assessment (IATA) have been proposed as practical solutions to such strategic testing. In parallel, the development of the Adverse Outcome Pathway (AOP) framework, which provides information on the causal links between a molecular initiating event (MIE), intermediate key events (KEs) and an adverse outcome (AO) of regulatory concern, offers the biological context to facilitate development of IATA for regulatory decision making. This manuscript summarizes discussions at the Workshop entitled ""Advancing AOPs for Integrated Toxicology and Regulatory Applications"" with particular focus on the role AOPs play in informing the development of IATA for different regulatory purposes. © 2014 Elsevier Inc."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922465409&partnerID=40&md5=c3a70f76df23d0259bd74a370c7ed6a4","The need to adapt to climate change is now widely recognised as evidence of its impacts on social and natural systems grows and greenhouse gas emissions continue unabated. Yet efforts to adapt to climate change, as reported in the literature over the last decade and in selected case studies, have not led to substantial rates of implementation of adaptation actions despite substantial investments in adaptation science. Moreover, implemented actions have been mostly incremental and focused on proximate causes; there are far fewer reports of more systemic or transformative actions. We found that the nature and effectiveness of responses was strongly influenced by framing. Recent decision-oriented approaches that aim to overcome this situation are framed within a ""pathways"" metaphor to emphasise the need for robust decision making within adaptive processes in the face of uncertainty and inter-temporal complexity. However, to date, such ""adaptation pathways"" approaches have mostly focused on contexts with clearly identified decision-makers and unambiguous goals; as a result, they generally assume prevailing governance regimes are conducive for adaptation and hence constrain responses to proximate causes of vulnerability. In this paper, we explore a broader conceptualisation of ""adaptation pathways"" that draws on 'pathways thinking' in the sustainable development domain to consider the implications of path dependency, interactions between adaptation plans, vested interests and global change, and situations where values, interests, or institutions constrain societal responses to change. This re-conceptualisation of adaptation pathways aims to inform decision makers about integrating incremental actions on proximate causes with the transformative aspects of societal change. Case studies illustrate what this might entail. The paper ends with a call for further exploration of theory, methods and procedures to operationalise this broader conceptualisation of adaptation. © 2013 The Authors."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905983125&partnerID=40&md5=93006c7ce2b5241a28b16df166aabba8","On 22 May 2011 a massive tornado tore through Joplin, Mo., killing 158 people. With winds blowing faster than 200 miles per hour, the tornado was the most deadly in the United States since modern record keeping began in the 1950s. ©2014. American Geophysical Union. All Rights Reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927511422&partnerID=40&md5=ab9c52996b034d7ca099948a562b58b2","Purpose - The purpose of this paper is to propose a set of process capability indices (PCIs) which are based on robust and agile statistics such that they may be applicable irrespective of the process status. Design/methodology/approach - The four popular PCIs - Cp, Cpk, Cpm and Cpmk - are reconstructed to improve location and dispersion predictions by introducing robust estimators such as the median and the interquartile range. The proposed PCIs are sequentially evaluated in partitioned regions where fluctuations are inspected to be not significant. The runs test playing the role of a detector permits marking those regions between two consecutive appearances of causes that disrupt data randomness. Wilcoxon's one-sample test is utilized to approximate PCI's central tendency and its confidence interval across all formed partitions. Findings - The Cpmk depicted the most conservative view of the process status when tracking the magnesium content ina showcased aluminum manufacturing paradigm. Cp and Cpk were benchmarked with controlled random data. It was found that the proposed set of robust PCIs are substantially less prone to false alarm in predicting non-conforming units in comparison to the regular PCIs. Originality/value - The recommended method for estimating PCIs is purely distribution-free and thus deployable at any process maturity level. The advantageous approach defends vigorously against the influence of intruding sources of unknown and unknowable variability. Therefore, the predicament here is to protect the monitoring indicators from unforeseen data instability and breakdown, which are conspicuous in wreaking havoc in managerial decisions. © Emerald Group Publishing Limited."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904658611&partnerID=40&md5=8ef413e4f576335dc786060f0f3f7a38","Vicarious trial-and-error (VTE) is a behavior observed in rat experiments that seems to suggest self-conflict. This behavior is seen mainly when the rats are uncertain about making a decision. The presence of VTE is regarded as an indicator of a deliberative decision-making process, that is, searching, predicting, and evaluating outcomes. This process is slower than automated decision-making processes, such as reflex or habituation, but it allows for flexible and ongoing control of behavior. In this study, we propose for the first time a robotic model of VTE to see if VTE can emerge just from a body-environment interaction and to show the underlying mechanism responsible for the observation of VTE and the advantages provided by it. We tried several robots with different parameters, and we have found that they showed three different types of VTE: high numbers of VTE at the beginning of learning, decreasing numbers afterward (similar VTE pattern to experiments with rats), low during the whole learning period, and high numbers all the time. Therefore, we were able to reproduce the phenomenon of VTE in a model robot using only a simple dynamical neural network with Hebbian learning, which suggests that VTE is an emergent property of a plastic and embodied neural network. From a comparison of the three types of VTE, we demonstrated that 1) VTE is associated with chaotic activity of neurons in our model and 2) VTE-showing robots were robust to environmental perturbations. We suggest that the instability of neuronal activity found in VTE allows ongoing learning to rebuild its strategy continuously, which creates robust behavior. Based on these results, we suggest that VTE is caused by a similar mechanism in biology and leads to robust decision making in an analogous way. Copyright: © 2014 Matsuda et al."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953249929&partnerID=40&md5=b147f66516b395715a81f0de15c5d4ce","Introduction: Advanced appendicitis (perforation, mass, or abscess) is a significant cause of morbidity in children. This chapter reviews the risk factors for and the management of children with advanced appendicitis and associated complications. Methods: A search of the literature was conducted and manual cross-referencing was performed. Results: The incidence of perforation and outcomes vary according to age, gender, and geographical region. Advanced appendicitis is unlikely in the presence of a normal white blood cell (WBC) or C-reactive protein (CRP) measurement. The presence of fever, symptom duration > 24h, generalized abdominal tenderness, rebound tenderness and or rigidity, hypoactive and/or absent bowel sounds, right lower quadrant mass, leukocytosis, and fecalith on CT scans may suggest advanced appendicitis. Age, increased BMI, diarrhea, inadequate antibiotic therapy, and certain microbial isolates may predispose an individual to an increased risk of post-appendectomy complications. Discussion: Non-operative, operative, and postoperative management strategies in the treatment of pediatric advanced appendicitis are discussed. The key to reducing complications is early diagnosis of advanced appendicitis, which is aided by robust decision-making, biomarker analysis, and the judicious use of imaging. Conclusion: An up-to-date review of the risk factors for and management of children with advanced appendicitis and complications is presented. © 2014 by Nova Science Publishers, Inc. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894435128&partnerID=40&md5=1e0696b725ac74314bb0edeb985fa86c","This paper provides an overview of developments in robust optimization since 2007. It seeks to give a representative picture of the research topics most explored in recent years, highlight common themes in the investigations of independent research teams and highlight the contributions of rising as well as established researchers both to the theory of robust optimization and its practice. With respect to the theory of robust optimization, this paper reviews recent results on the cases without and with recourse, i.e., the static and dynamic settings, as well as the connection with stochastic optimization and risk theory, the concept of distributionally robust optimization, and findings in robust nonlinear optimization. With respect to the practice of robust optimization, we consider a broad spectrum of applications, in particular inventory and logistics, finance, revenue management, but also queueing networks, machine learning, energy systems and the public good. Key developments in the period from 2007 to present include: (i) an extensive body of work on robust decision-making under uncertainty with uncertain distributions, i.e., ""robustifying"" stochastic optimization, (ii) a greater connection with decision sciences by linking uncertainty sets to risk theory, (iii) further results on nonlinear optimization and sequential decision-making and (iv) besides more work on established families of examples such as robust inventory and revenue management, the addition to the robust optimization literature of new application areas, especially energy systems and the public good. © 2014 Elsevier B.V. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930334579&partnerID=40&md5=936efa738089a3f0650739682d4fd52f","This book explores economic concepts related to disaster losses, describes mechanisms that determine the economic consequences of a disaster, and reviews methodologies for making decisions regarding risk management and adaptation. The author addresses the need for better understanding of the consequences of disasters and reviews and analyzes three scientific debates on linkage between disaster risk management and adaptation to climate change. The first involves the existence and magnitude of long-term economic impact of natural disasters on development. The second is the disagreement over whether any development is the proper solution to high vulnerability to disaster risk. The third debate involves the difficulty of drawing connections between natural disasters and climate change and the challenge in managing them through an integrated strategy. The introduction describes economic views of disaster, including direct and indirect costs, output and welfare losses, and use of econometric tools to measure losses. The next section defines disaster risk, delineates between ""good"" and ""bad"" risk-taking, and discusses a pathway to balanced growth. A section entitled ""Trends in Hazards and the Role of Climate Change"" sets scenarios for climate change analysis, discusses statistical and physical models for downscaling global climate scenarios to extreme event scenarios, and considers how to consider extremes of hot and cold, storms, wind, drought and flood. Another section analyzes case studies on hurricanes and the US coastline; sea-level rises and storm surge in Copenhagen; and heavy precipitation in Mumbai. A section on Methodologies for disaster risk management includes a study on cost-benefit analysis of coastal protections in New Orleans, and one on early-warning systems in developing countries. The next section outlines decision-making in disaster risk management, including robust decision-making, No-regret and No-risk strategies; and strategies that reduce time horizons for decision-making. Among the conclusions is the assertion that risk management policies must recognize the benefits of risk-taking and avoid suppressing it entirely. The main message is that a combination of disaster-risk-reduction, resilience-building and adaptation policies can yield large potential gains and synergies. © 2014 Springer International Publishing Switzerland. All rights are reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901084295&partnerID=40&md5=bd4ef4e13e3fc32b9b5e08bf2f35703d","Structural equation modeling is used in statistical applications as both confirmatory and exploratory modeling to test models and to suggest the most plausible explanation for a relationship between the independent and the dependent variables. Although structural analysis cannot prove causation, it can suggest the most plausible set of factors that influence the observed variable. We apply structural model analysis to the annual mean Arctic surface air temperature from 1900 to 2012 to find the most effective set of predictors and to isolate the anthropogenic component of the recent Arctic warming by subtracting the effects of natural forcing and variability from the observed temperature. We find that anthropogenic greenhouse gases and aerosols radiative forcing and the Atlantic Multidecadal Oscillation internal mode dominate Arctic temperature variability. Our structural model analysis of observational data suggests that about half of the recent Arctic warming of 0.64K/decade may have anthropogenic causes.© 2014. American Geophysical Union. All Rights Reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949928262&partnerID=40&md5=3eb2fdcc6b77dac21037467b3cbe06e4","In this paper we propose one IDTMC (Interval Discrete-Time Markov Chain) algorithm to improve simulation when significant variabilities exist. The IDTMC models takes into account the effects of variabilities in transition probabilities represented by intervals. IDTMC simulation incorporates variabilities and uncertainties based on imprecise probabilities, where the statistical distribution parameters in the simulation are intervals instead of precise real numbers. Interval arithmetic is used to simulate a set of scenarios simultaneously in each simulation run. A case study is presented for processing uncertainty from an ISPN (Interval Stochastic Petri Net) model. This simulation procedure can be applied to support robust decision making. © 2014 IEEE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983146809&partnerID=40&md5=339287e5ea739c7aa5aa8e12587ad970","This paper presents a robust-based procedure to evaluate the impact of uncertainty on optimal sizing of stochastic Renewable Energy Sources (RES) such as photovoltaic panels (PVs) and small-scale wind power plants (WPPs). The feasibility study of an isolated system in the midterm planning horizon is aimed at determining whether it can adequately satisfy the demand. Accordingly, the objective of the model is to provide a convenient decision making framework for supplying the demand of a typical smart research institute merely by RES such that it would guarantee the minimum investment cost. However, due to RES inherent uncertainty, these generators are often unable to meet demand reliably. Given this, smartly operating Energy Storage Systems (ESS) are being advocated as a solution for increasing the reliability of such intermittent renewable sources. An Iranian smart research center is chosen as a case study to illustrate the effectiveness of the proposed model. © 2014 IEEE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899809681&partnerID=40&md5=477654572ad55573627eba4d1a41acd4","When making decisions under uncertainty, the optimal choices are often difficult to discern, especially if not enough information has been gathered. Two key questions in this regard relate to whether one should stop the information gathering process and commit to a decision (stopping criterion), and if not, what information to gather next (selection criterion). In this paper, we show that the recently introduced notion, Same-Decision Probability (SDP), can be useful as both a stopping and a selection criterion, as it can pro- vide additional insight and allow for robust decision making in a variety of scenarios. This query has been shown to be highly intractable, being PP PP-complete, and is exemplary of a class of queries which correspond to the computation of certain expectations. We pro- pose the first exact algorithm for computing the SDP, and demonstrate its effectiveness on several real and synthetic networks. Finally, we present new complexity results, such as the complexity of computing the SDP on models with a Naive Bayes structure. Additionally, we prove that computing the non-myopic value of information is complete for the same complexity class as computing the SDP. © 2014 AI Access Foundation. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890960177&partnerID=40&md5=cc8a5611a70089d6b742d862248e8370","Model predictive control (MPC) is a promising solution for the effective control of process supply chains. This paper presents an optimization-based decision support tool for supply chain management, by means of a robust MPC strategy. The proposed formulation: (i) captures uncertainty in model parameters and demand by stochastic programming, (ii) accommodates hybrid process systems with decisions governed by logical conditions/rulesets, and (iii) addresses multiple supply chain performance metrics including customer service and economics, within an integrated optimization framework. Two mechanisms for uncertainty propagation are presented - an open-loop approach, and an approximate closed-loop strategy. The performance of the robust MPC framework is analyzed through its application to two process supply chain case studies. The proposed approach is shown to provide a substantial reduction in the occurrence of back orders when compared to a nominal MPC implementation. © 2013 Elsevier Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890863739&partnerID=40&md5=3c2ae80ae1a16f2eeb345cbd14f0c580","This study developed an index-based robust decision making framework for watershed management dealing with water quantity and quality issues in a changing climate. It consists of two parts of management alternative development and analysis. The first part for alternative development consists of six steps: 1) to understand the watershed components and process using HSPF model, 2) to identify the spatial vulnerability ranking using two indices: potential streamflow depletion (PSD) and potential water quality deterioration (PWQD), 3) to quantify the residents' preferences on water management demands and calculate the watershed evaluation index which is the weighted combinations of PSD and PWQD, 4) to set the quantitative targets for water quantity and quality, 5) to develop a list of feasible alternatives and 6) to eliminate the unacceptable alternatives. The second part for alternative analysis has three steps: 7) to analyze all selected alternatives with a hydrologic simulation model considering various climate change scenarios, 8) to quantify the alternative evaluation index including social and hydrologic criteria with utilizing multi-criteria decision analysis methods and 9) to prioritize all options based on a minimax regret strategy for robust decision. This framework considers the uncertainty inherent in climate models and climate change scenarios with utilizing the minimax regret strategy, a decision making strategy under deep uncertainty and thus this procedure derives the robust prioritization based on the multiple utilities of alternatives from various scenarios. In this study, the proposed procedure was applied to the Korean urban watershed, which has suffered from streamflow depletion and water quality deterioration. Our application shows that the framework provides a useful watershed management tool for incorporating quantitative and qualitative information into the evaluation of various policies with regard to water resource planning and management. © 2013 Elsevier B.V."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888120151&partnerID=40&md5=0a243a58f5b2c51f51a2bfa2d2f82add","An airport is the gateway which facilitates access to air transport. As a reaction to very diverse attacks on the air transport system during the last decades a broad range of security measures has been introduced to mitigate possible threats. The challenge to provide a trouble free experience for the passenger and, at the same time, to operate more efficiently calls for a proactive approach. This requires the definition of future requirements that allow an adaptation of the security system. When dealing with uncertainty that future-oriented decisions inevitably display, it is important to gain as much knowledge as possible about a system's general structure. The approach described in this paper systematically documents elements and relationships of the airport security system. It consists of threat scenario elements as well as security measures. The development of a software tool, the so-called Scenario Builder, is described and its application for the identification of possible future threats explained. The presented approach offers intuitive access to the underlying structure of the airport security system. It provides decision makers with a possibility to interact with the system and anticipate effects of threat development, thereby enabling robust, future-oriented decisions. © 2013 Elsevier Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894457576&partnerID=40&md5=e39e4cfbca4a304a6b67108bd1468e14","This paper addresses the NASA Langley Multidisciplinary Uncertainty Quantification Challenge (MUQC) Problem, which is intended to pose challenges to the uncertainty quantification and robust design communities. The goals of the MUQC problem can be formulated into four main topics that are commonly encountered in the model development process: calibration, sensitivity analysis, uncertainty propagation, and robust design. Our analysis places a particular emphasis on the use of info-gap decision theory (IGDT) to address the goals of the MUQC problem. IGDT provides a convenient framework to treat epistemic uncertainty when using simulation models for decision-making. We utilize a robustness criterion, defined in the context of IGDT, to pursue calibration, uncertainty propagation, and robust design. Herein, our calibration utilizes IGDT to address the situation whereby traditional calibration techniques might result in non-unique results where different sets of calibration variables are able to replicate experiments with comparable fidelity. Uncertainty propagation is performed such that the worst-case and best-case performances of the model output are conditioned on the level of uncertainty that is permitted in the simulations. To pursue robust design, we utilize the robustness criterion to establish whether the amount of uncertainty tolerable in our optimized design is an improvement over the baseline design. We demonstrate that improving the robustness of the model requires different knowledge than improving performance of the model. The main conclusion is that IGDT provides a sound theoretical basis, and practical implementation, to meet the goals of the NASA MUQC problem without formulating simplifying assumptions."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894453004&partnerID=40&md5=c0a3e55b05c268f6999b58b25fab7294","The proceedings contain 40 papers. The special focus in this conference is on Non-Deterministic Approaches. The topics include: Evaluation of model validation techniques in the presence of uncertainty; a simple probabilistic validation metric for the comparison of uncertain model and test results; options for the inclusion of model discrepancy in Bayesian calibration; Bayesian calibration of coupled aerothermal models using time-dependent data; uncertainty quantification and output prediction in multi-level problems; a mixed uncertainty quantification approach with evidence theory and stochastic expansions; uncertainty propagation with monotonicity preserving robustness; quantification of margins and mixed uncertainties using evidence theory and stochastic expansions; an uncertainty quantification framework for prognostics and condition-based monitoring; statistical aspects in neural network for the purpose of prognostics; probabilistic design of smart sensing functions for failure diagnostics and prognostics; probabilistic prognosis using dynamic Bayesian networks; a novel Bayesian imaging method for probabilistic delamination detection of composite materials; probability of failure analysis and design using an efficient sequential sampling approach; ignoring dependence between failure modes is reasonable for low probabilities of failure; integration of system reliability analysis and FMECA to efficiently identify structural hot spots; reliability estimation using guided tail modeling with adaptive sampling; a pre-validation study on supersonic wind tunnel data collected from legacy aerothermal experiments; uncertainty quantification of hypersonic reentry flows using sparse sampling and stochastic expansions; uncertainty analysis of corrugated skin with random elastic parameters and surface topology; aggressive design under uncertainty; optimal design and tolerancing of compressor blades subject to manufacturing variability; utilizing an adjustment factor to scale between multiple fidelities within a design process; a dynamic data driven approach to online flight envelope updating for self aware aerospace vehicles; decomposed multilevel optimization under epistemic uncertainty; data assimilation for turbulent flows; computational effort vs. accuracy tradeoff in uncertainty quantification; the NASA Langley multidisciplinary uncertainty quantification challenge; a probabilistic approach to the NASA Langley multidisciplinary uncertainty quantification challenge problem; a hybrid Bayesian solution to NASA uncertainty quantification challenge; robust decision making applied to the NASA multidisciplinary uncertainty quantification challenge problem; uncertainty quantification methods for model calibration, validation, and risk analysis; Bayesian framework for multidisciplinary uncertainty quantification and optimization; a probabilistic treatment of multiple uncertainty types; an integrated and efficient numerical framework for uncertainty quantification and a Bayesian multilevel framework for uncertainty characterization and the NASA Langley multidisciplinary UQ challenge."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988273448&partnerID=40&md5=fb730d1945af9d6d3be7d3e9f0dcafff","To be robust, decision-making process must take account the imperfection associated with models. The identification, understanding and propagation of imperfection sources are important. In general, the imperfection in land cover change (LCC) prediction process can be categorized as both aleatory and epistemic. This imperfection, which can be subdivided into parameter and structural model imperfection, is recognized to have an important impact on actual results. Previously, it has been shown that evidence theory can be applied to model aleatory and epistemic imperfection. The objective of this study is to introduce an efficient methodology for the propagation of imperfection using evidence theory in LCC prediction model, which include both parameter and structural model imperfection sources. © 2014 IEEE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893382552&partnerID=40&md5=aa0ab80d5d55f709c1d192f614402edd","Uncertainties of climate change and other nonprobabilistic and structural uncertainties need to be addressed in strategic planning and priority setting for infrastructure systems. Traditional economic analysis and risk analysis of particular uncertainties can be prohibitive due to sparse data, complex models, and unforeseen interactions of climate change with other stressors. Nevertheless, planners need to proceed in the near term and may be asked to allocate resources to these deep uncertainties. This paper identifies and quantifies the influence of climate change combining with other sources of uncertainty to the priority order of projects in a portfolio of infrastructure investments. A demonstration for the Hampton Roads region of Virginia proceeds as follows. First, we apply traditional multicriteria analysis to generate a baseline prioritization of over 93 transportation projects. Next, we identify the following scenarios: climate conditions combined with economic conditions, wear and tear, ecological conditions, and traffic-demand conditions, and climate conditions alone. Next, we adjust a multicriteria value function for each scenario. We then quantify the sensitivity of the priority order of projects to the scenarios. Last, we identify the scenarios that are disruptive to the baseline prioritization. This approach is widely applicable to strategic planning for infrastructure systems that are subject to uncertainties of emergent and future conditions. © 2013 IEEE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894262363&partnerID=40&md5=0cccdb9ce40b46bb7d468c00be74eb87","Summary: Life cycle assessment (LCA) is generally described as a tool for environmental decision making. Results from attributional LCA (ALCA), the most commonly used LCA method, often are presented in a way that suggests that policy decisions based on these results will yield the quantitative benefits estimated by ALCA. For example, ALCAs of biofuels are routinely used to suggest that the implementation of one alternative (say, a biofuel) will cause an X% change in greenhouse gas emissions, compared with a baseline (typically gasoline). However, because of several simplifications inherent in ALCA, the method, in fact, is not predictive of real-world impacts on climate change, and hence the usual quantitative interpretation of ALCA results is not valid. A conceptually superior approach, consequential LCA (CLCA), avoids many of the limitations of ALCA, but because it is meant to model actual changes in the real world, CLCA results are scenario dependent and uncertain. These limitations mean that even the best practical CLCAs cannot produce definitive quantitative estimates of actual environmental outcomes. Both forms of LCA, however, can yield valuable insights about potential environmental effects, and CLCA can support robust decision making. By openly recognizing the limitations and understanding the appropriate uses of LCA as discussed here, practitioners and researchers can help policy makers implement policies that are less likely to have perverse effects and more likely to lead to effective environmental policies, including climate mitigation strategies. © 2013 by Yale University."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893668815&partnerID=40&md5=3fdcdbb67cc960a9da4e6898f48539b7","Objective To investigate the short-term trajectory of recovery from mechanical neck pain, and predictors of trajectory. Design Prospective, longitudinal cohort study with 5 repeated measurements over 4 weeks. Setting Community-based physical therapy clinics. Participants Convenience sample of community-dwelling adults (N=50) with uncomplicated mechanical neck disorders of any duration. Interventions Usual physical therapy care. Main Outcome Measures Neck Disability Index (NDI), numeric rating scale (NRS) of pain intensity. Results A total of 50 consecutive subjects provided 5 data points over 4 weeks. Exploratory modeling using latent class growth analysis revealed a linear trend in improvement, at a mean of 1.5 NDI points and 0.5 NRS points per week. Within the NDI trajectory, 3 latent classes were identified, each with a unique trend: worsening (14.5%), rapid improvement (19.6%), and slow improvement (65.8%). Within the NRS trajectory, 2 unique trends were identified: stable (48.0%) and improving (52.0%). Predictors of trajectory class suggest that it may be possible to predict the trajectory. Results are described in view of the sample size. Conclusions The mean trajectory of improvement in neck pain adequately fits a linear model and suggests slow but stable improvement over the short term. However, up to 3 different trajectories have been identified that suggest neck pain, and recovery thereof, is not homogenous. This may hold value for the design of clinical trials. © 2014 by the American Congress of Rehabilitation Medicine."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908179647&partnerID=40&md5=7bfc5d1de5c2ac09db15d876107fbcc7",[No abstract available]
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911449269&partnerID=40&md5=7f33153b059eac3296b2056f119d27bd","We propose a logic-based mechanism for robot action decisions that is robust over the environmental noise of the real world and has a formal way to reason the possibility of achieving the robot's goal. Our experimental demos show that a robot can eventually reach its destination even if its actions are not that accurate. Copyright © 2014, International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904788790&partnerID=40&md5=363b55ec9ffb2d3d552b0e530b051710","Dynamic Adaptive Policy Pathways has been developed as an approach to deal with deep uncertainties and support robust decision-making for long-term planning. Given the unpredictable and uncertain futures, implementation of the resulting adaptive policies needs to be informed by regular monitoring. However, monitoring implementation in practice is complicated by the need to coordinate activities and share information among multiple actors. Here we present a first outline for an approach to organise collaborative monitoring to support adaptive implementation of long-term water policies. The analytical basis rests on an extension of Dynamic Adaptive Policy Pathways with actor analysis principles. Monitoring is to be organised around adaptation tipping points, for which a set of questions needs to be addressed that put societal actors central. Examples from two water management cases in the Netherlands suggest the usefulness of this approach. Copyright © 2014 IAHS Press."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930149546&partnerID=40&md5=ed7fd79a034f58ce88f573928880e50d","This survey gives a review of recent artificial intelligence-related research directions that are considered priority areas by the U.S. Air Force and targeted for basic research funding by Air Force Office of Scientific Research. These research areas include space situational awareness, autonomous systems, sensing and information fusion, surveillance, navigation, robust decision making, human-computer interfaces, and computational and machine intelligence. The possible contributions of artificial intelligence to these topics will be described and illustrated whenever possible by recently awarded grants. © 2014 World Scientific Publishing Company."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908258559&partnerID=40&md5=72c8752241bf8af4fc895451a31df954","This paper considers a robust decision-making problem associated with supplies of parts and deliveries of finished products in a customer driven supply chain under disruption risks. The robustness refers to an equitably efficient performance of a supply chain in average-case as well as in the worst-case, which reflects the decision-makers common requirement to maintain an equally good performance of a supply chain under different conditions. Given a set of customer orders for products, the decision-maker needs to select suppliers of parts required to complete the orders, allocate the demand for parts among the selected suppliers and schedule the orders over the planning horizon, to equitably optimise average and worst-case performance of the supply chain. The supplies are subject to independent random local and regional disruptions. The obtained combinatorial stochastic optimisation problem is formulated as a mixed-integer program with conditional value-at-risk as a risk measure. The ordered weighted averaging aggregation of the expected value and the conditional value-at-risk of the selected optimality criterion is applied to obtain a robust solution. The risk-neutral, risk-averse and robust solutions that optimise, respectively average, worst-case and equitable average and worst-case performance of a supply chain are determined and compared for cost and customer service level objective functions. Numerical examples and computational results, in particular comparison with the mean-risk approach, are presented and some managerial insights are reported. © 2014 Taylor and Francis."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84895416415&partnerID=40&md5=306b837159288d3b28b87cf812210892","This article presents a methodology for planning new water resources infrastructure investments and operating strategies in a world of climate change uncertainty. It combines a real options (e.g., options to defer, expand, contract, abandon, switch use, or otherwise alter a capital investment) approach with principles drawn from robust decision-making (RDM). RDM comprises a class of methods that are used to identify investment strategies that perform relatively well, compared to the alternatives, across a wide range of plausible future scenarios. Our proposed framework relies on a simulation model that includes linkages between climate change and system hydrology, combined with sensitivity analyses that explore how economic outcomes of investments in new dams vary with forecasts of changing runoff and other uncertainties. To demonstrate the framework, we consider the case of new multipurpose dams along the Blue Nile in Ethiopia. We model flexibility in design and operating decisions - the selection, sizing, and sequencing of new dams, and reservoir operating rules. Results show that there is no single investment plan that performs best across a range of plausible future runoff conditions. The decision-analytic framework is then used to identify dam configurations that are both robust to poor outcomes and sufficiently flexible to capture high upside benefits if favorable future climate and hydrological conditions should arise. The approach could be extended to explore design and operating features of development and adaptation projects other than dams. Key Points No planning alternative is likely to dominate across plausible future conditions We present a method for generating information for the selection of robust planning alternatives Downside and upside metrics can assist enhanced decision making © 2014. American Geophysical Union. All Rights Reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929148011&partnerID=40&md5=c9a503ec78faef65c04fec2bdbbad74a","Several major accidents caused by metal dusts were recorded in the past few years. For instance, in 2011, three accidents caused by iron dust killed five workers at the Hoeganaes Corp. facility in Gallatin, Tennessee (USA). In order to prevent such accidents, a dynamic approach to risk management was defined in this study. The method is able to take into account new risk notions and early warnings and to systematically update the related risk. It may be applied not only in the design phase of a system, but also throughout the system lifetime as a support to a more precise and robust decision making process. The synergy of two specific techniques for hazard identification and risk assessment wasobtained: the Dynamic Procedure for Atypical Scenarios Identification (Dy PASI) and the Dynamic Risk Assessment (DRA) methods. To demonstrate its effectiveness, this approach was applied to the analysis of Gallatin metal dust accidents. The application allowed collecting a number of risk notions related to the plant, equipment and materials used. The analysis of risk notions by means of this dynamic approach could have led to enhanced hazard identification and dynamic real-time risk assessment. However, the approach described is effective only if associated to a proper safety culture, in order to produce an appropriate and robust decision making response to emerging risk issues. © 2013 The Institution of Chemical Engineers. Published by Elsevier B.V. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910119475&partnerID=40&md5=95133754d8d63f5cc27059d750d3b385","Scenario discovery is a novel participatory modelbased approach to scenario development in the presence of deep uncertainty. Scenario discovery relies on the use of statistical machine-learning algorithms. The most frequently used algorithm is the Patient Rule Induction Method. This algorithm identifies regions in the uncertain model input space that are highly predictive of producing model outcomes that are of interest. To identify these regions, PRIM in essence uses a hill climbing optimization procedure. This suggests that PRIM can suffer from the usual defects of hill climbing optimization algorithms, including local optima, plateaus, and ridges and alleys. In case of PRIM, these problems are even more pronounced when dealing with heterogeneously typed data. Drawing inspiration from machine learning research on random forests, we present an improved version of PRIM. This improved version is based on the idea of performing multiple PRIM analyses based on randomly selected features and combining these results using a bagging technique. The efficacy of the approach is demonstrated through a case study of scenario discovery for the transition of the European energy system towards more sustainable functioning, focusing on identifying scenarios where the transition fails. © 2014 PICMET."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911408081&partnerID=40&md5=c0717e6021c1c8634ae7b4939e0a67d1","While optimality is a foundational mathematical concept in water resources planning and management, ""optimal"" solutions may be vulnerable to failure if deeply uncertain future conditions deviate from those assumed during optimization. These vulnerabilities may produce severely asymmetric impacts across a region, making it vital to evaluate the robustness of management strategies as well as their impacts for regional stakeholders. In this study, we contribute a multistakeholder many-objective robust decision making (MORDM) framework that blends many-objective search and uncertainty analysis tools to discover key tradeoffs between water supply alternatives and their robustness to deep uncertainties (e.g., population pressures, climate change, and financial risks). The proposed framework is demonstrated for four interconnected water utilities representing major stakeholders in the ""Research Triangle"" region of North Carolina, U.S. The utilities supply well over one million customers and have the ability to collectively manage drought via transfer agreements and shared infrastructure. We show that water portfolios for this region that compose optimal tradeoffs (i.e., Pareto-approximate solutions) under expected future conditions may suffer significantly degraded performance with only modest changes in deeply uncertain hydrologic and economic factors. We then use the Patient Rule Induction Method (PRIM) to identify which uncertain factors drive the individual and collective vulnerabilities for the four cooperating utilities. Our framework identifies key stakeholder dependencies and robustness tradeoffs associated with cooperative regional planning, which are critical to understanding the tensions between individual versus regional water supply goals. Cooperative demand management was found to be the key factor controlling the robustness of regional water supply planning, dominating other hydroclimatic and economic uncertainties through the 2025 planning horizon. Results suggest that a modest reduction in the projected rate of demand growth (from approximately 3% per year to 2.4%) will substantially improve the utilities' robustness to future uncertainty and reduce the potential for regional tensions. The proposed multistakeholder MORDM framework offers critical insights into the risks and challenges posed by rising water demands and hydrological uncertainties, providing a planning template for regions now forced to confront rapidly evolving water scarcity risks. Key Points We advance many-objective robust decision making for multiple stakeholders Stakeholders' robustness exhibits dependencies, vulnerabilities, and tradeoffs A modest reduction in demand growth rate insulates against future uncertainty © 2014. American Geophysical Union. All Rights Reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900505883&partnerID=40&md5=39da4d0c00f4cf4331a6c8f9efdadb71","In this paper, we address the cloud service trustworthiness evaluation problem, which in essence is a multi-attribute decision-making problem, by proposing a novel evaluation model based on the fuzzy gap measurement and the evidential reasoning approach. There are many sources of uncertainties in the process of cloud service trustworthiness evaluation. In addition to the intrinsic uncertainties, cloud service providers face the problem of discrepant evaluation information given by different users from different perspectives. To address these problems, we develop a novel fuzzy gap evaluation approach to assess cloud service trustworthiness and to provide evaluation values from different perspectives. From the evaluation values, the perception-importance, delivery-importance, and perception-delivery gaps are generated. These three gaps reflect the discrepancy evaluation of cloud service trustworthiness in terms of perception utility, delivery utility, and importance utility, respectively. Finally, the gap measurement of each perspective is represented by a belief structure and aggregated using the evidential reasoning approach to generate final evaluation results for informative and robust decision making. From this hybrid two-stage evaluation process, cloud service providers can get improvement suggestions from intermediate information derived from the gap measurement, which is the main advantage of this evaluation process. © 2013 Wiley Publishing Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928939431&partnerID=40&md5=2bc54aa327df19135adbcd0bd35e6958","Southwestern United States paleoclimate reconstructions feature droughts that are longer and higher in magnitude than any water shortage during the twentieth century, and thus could aid water managers in planning for future severe droughts. This research used the robust decision-making (RDM) analytical framework to incorporate paleoclimate information into an analysis of long-range water management for a Southern California water agency. The analysis leverages a water management model to identify near-term management actions that may help mitigate water shortages over a wide range of future conditions reflecting various assumptions about climate, costs, and planning. Results indicate that a regional urban water management plan for 2005 is vulnerable to extended droughts and that enhancing water management actions in the near term reduces the risk of future unmet demand and shortage costs. Comparing results with previous work with the IEUA using climate model projections indicates some differences in outcomes related to climate data characteristics such as mean and maximum temperature. This work highlights some problems and potential benefits associated with using paleoclimate data for water management modeling. © 2014 American Society of Civil Engineers."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903890546&partnerID=40&md5=a5b78ce7f1908f2cf581b35a9f6c6b47","The classical newsvendor model under complete information is the crucial controlling method of single period inventory management, the jointly pricing and ordering decisions are the primary means to improve the operational efficiency of newsvendor model. Scarf first addressed the robust newsvendor problem under partial information, but the robust jointly pricing and ordering problem, due to its difficulty, has long plagued operation and management scholars. We establish the worst-case robust jointly pricing and ordering model where only the mean and the variance of the demand are known, and provide a closed-loop optimal solution of the joint decision making under appropriate conditions. Numerical experiments confirm that the jointly pricing and ordering decisions can improve the operational efficiency of Scarf's robust ordering model greatly. Our model is suitable to new product operation management, the characteristic of which is the operation environment under limited information."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904355042&partnerID=40&md5=60cbce3186d16087441890d506cdebbd","In the past few years, research around (big) data management has begun to intertwine with research around predictive modeling and simulation in novel and interesting ways. Driving this trend is an increasing recognition that information contained in real-world data must be combined with information from domain experts, as embodied in simulation models, in order to enable robust decision making under uncertainty. Simulation models of large, complex systems (traffic, biology, population well-being) consume and produce massive amounts of data and compound the challenges of traditional information management. We survey some challenges, mathematical tools, and future directions in the emerging research area of model-data ecosystems. Topics include (i) methods for enabling data-intensive simulation, (ii) simulation and information integration, and (iii) simulation metamodeling for guiding the generation of simulated data and the collection of real-world data. Copyright is held by the owner/author(s)."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911014861&partnerID=40&md5=66edaafcc72d819c3fb7bf8cf184889c","In this work, we propose to explicitly include group decision making strategies into an architecting phase, so to clearly document not only the architectural decisions that may lead to the success or failure of a system, but also group decision making factors driving the way architecture design decisions are made. In this regard, this work defines a group design decision metamodel (for representing group design decisions and their relationships), together with ways to trace group design decisions towards other system life-cycle artifacts, and a change impact analysis engine for supporting evolving design decisions.In order to build resilient systems, robust architectures are needed. The software architecture community clearly recognizes that robust architectures come from a robust decision-making process. The community also acknowledges that software architecture decision making is not an individual activity but a group process where architectural design decisions are made by groups of heterogeneous and dispersed stakeholders. The decision-making process is not just data driven, but also people driven, and group decision making methodologies have been studied from multiple perspectives (e.g., psychology, organizational behavior, economics) with the clear understanding that a poor-quality decision making process is more likely than a high-quality process leading to undesirable outcomes (including disastrous fiascoes). © Springer International Publishing Switzerland 2014."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904789799&partnerID=40&md5=47eccf22a63d7b090b8934f9fd5476a3","The conventional methods used to solve multi-criteria multi-stakeholder problems are less strongly formulated, as they normally incorporate only homogeneous information at a time and suggest aggregating objectives of different decision-makers avoiding water-society interactions. In this contribution, Multi-Criteria Group Decision Analysis (MCGDA) using a fuzzy-stochastic approach has been proposed to rank a set of alternatives in water management decisions incorporating heterogeneous information under uncertainty. The decision making framework takes hydrologically, environmentally, and socio-economically motivated conflicting objectives into consideration. The criteria related to the performance of the physical system are optimized using multi-criteria simulation-based optimization, and fuzzy linguistic quantifiers have been used to evaluate subjective criteria and to assess stakeholders' degree of optimism. The proposed methodology is applied to find effective and robust intervention strategies for the management of a coastal hydrosystem affected by saltwater intrusion due to excessive groundwater extraction for irrigated agriculture and municipal use. Preliminary results show that the MCGDA based on a fuzzy-stochastic approach gives useful support for robust decision-making and is sensitive to the decision makers' degree of optimism. Copyright © 2014 IAHS Press."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903146969&partnerID=40&md5=1dd601ba14d1e0def99e11741f968875","Modern societies are increasingly threatened by disasters that require rapid response through ad-hoc collaboration among a variety of actors and organizations. The complexity within and across today's societal, economic and environmental systems defies accurate predictions and assessments of damages, humanitarian needs, and the impact of aid. Yet, decision-makers need to plan, manage and execute aid response under conditions of high uncertainty while being prepared for further disruptions and failures. This paper argues that these challenges require a paradigm shift: instead of seeking optimality and full efficiency of procedures and plans, strategies should be developed that enable an acceptable level of aid under all foreseeable eventualities. We propose a decision- and goal-oriented approach that uses scenarios to systematically explore future developments that may have a major impact on the outcome of a decision. We discuss to what extent this approach supports robust decision-making, particularly if time is short and the availability of experts is limited. We interlace our theoretical findings with insights from experienced humanitarian decision makers we interviewed during a field research trip to the Philippines in the aftermath of Typhoon Haiyan. © 2014 Springer International Publishing."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958550027&partnerID=40&md5=f1481c031cc845ecc797fbc01d7f6612","In this paper, we study a robust multi modal compass for a vision based navigation system. The model mimics several aspects of the head direction cells found in the postsubiculum of the rat. Idiothetic information is recalibrated according to the learning of visual stimuli associated to robust landmarks. The model is based on dynamic neural fields allowing building attractors associated to the compass direction. The novelty of the model relies in the way the decision of the sensor fusion is re-injected in the visual compass allowing a robust decision-making. Robotics experiments show the capability of the model to merge different sources of information when their predictions are coherent. When the information become incoherent because the inputs propose quite different directions, the system is able to bifurcate on one coherent solution in order to maintain the temporal coherency of its behavior. © 2014 Springer International Publishing Switzerland."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84895107835&partnerID=40&md5=a386a07cae4003338d8976360f47fd59","Public health agencies face difficult decisions when allocating scarce resources to control the spread of HIV/AIDS. Decisions are often made with few local empirical data. We demonstrated the use of the robust decision making approach in Los Angeles County, an approach that is data driven and allows decision makers to compare the performance of various intervention strategies across thousands of simulated future scenarios. We found that the prevailing strategy of emphasizing behavioral risk reduction interventions was unlikely to achieve the policy goals of the national HIV/AIDS strategy. Of the alternative strategies we examined, those that invested most heavily in interventions to initiate antiretroviral treatment and support treatment adherence were the most likely to achieve policy objectives. By employing similar methods, other public health agencies can identify robust strategies and invest in interventions more likely to achieve HIV/AIDS policy goals © 2014 Project HOPE-The People-to-People Health Foundation, Inc."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930989213&partnerID=40&md5=a4ddcef7af4c839579340864b669c768","Fossil fuel gencos are subject to influence of multiple uncertain but interactive energy and emission markets. It procures production resources from fuel and emission market and sells its generation through multiple contracts in electricity market. With increasing volatility and unpredictability in energy markets, a genco needs to make prudent decision to manage its trading in all involved markets, to guarantee minimum profit. Considering the existing market uncertainties and associated information gap, this paper proposes a robust decision making approach for gencos trading portfolio selection in all three involved markets, based on Information Gap Decision Theory (IGDT). Results from a realistic case study provides a range of decisions for a risk averse genco, appropriate to its nature, and based on the trade-off existing between robustness and targeted profit. © 2014 IEEE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925484561&partnerID=40&md5=fc0f858e581ed542092ab9180f8361b1","Over the last years, it has been observed an increasing interest of the finance and business communities in any application tool related to the prediction of credit and bankruptcy risk, probably due to the need of more robust decision-making systems capable of managing and analyzing complex data. As a result, plentiful techniques have been developed with the aim of producing accurate prediction models that are able to tackle these issues. However, the design of experiments to assess and compare these models has attracted little attention so far, even though it plays an important role in validating and supporting the theoretical evidence of performance. The experimental design should be done carefully for the results to hold significance; otherwise, it might be a potential source of misleading and contradictory conclusions about the benefits of using a particular prediction system. In this work, we review more than 140 papers published in refereed journals within the period 2000–2013, putting the emphasis on the bases of the experimental design in credit scoring and bankruptcy prediction applications. We provide some caveats and guidelines for the usage of databases, data splitting methods, performance evaluation metrics and hypothesis testing procedures in order to converge on a systematic, consistent validation standard. © 2014, Springer Science+Business Media New York."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898795502&partnerID=40&md5=f662f6a5cabe3a1175d9944118e6fd9e","Adapting to climate change in the water sector requires abandoning two crucial assumptions. First, that the climate represented in the instrumental record is representative of the future. Instead, future water resource planning cannot be based on old measurements (or sequences derived from attaching change factors to instrumental data) and it should be recognized that stationarity is no longer viable, and, second, that climate modelling can be expected to give precise and certain predictions of the future. Instead, probabilistic projections of the future that take into account the full range of uncertainty should form the basis of robust climate change adaptation plans. As a response to the first assumption, it is suggested that stochastic weather generators represent a particularly useful approach to understanding the impacts of future climate change on water resources at a catchment scale, particularly given the recent release of 'science-hidden' tools such as the UKCP09 weather generator. With regards to the second assumption, it is suggested that modelling activity should identify the range of plausible futures to develop probabilities of risk, using those robust decision-making techniques which can gauge the performance of potential adaptation strategies. The best practice for delivering a replicable and practical hydroclimatological impact assessment for UK water resources at a catchment scale is identified, and an hypothetical example is outlined. It is suggested that although augmenting the resilience of water resources to climate change on a catchment scale is dependent on using the correct modelling tools, the robustness of the method with which that information is used to make adaptation decisions is equally as important. © 2012 Royal Meteorological Society."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904764030&partnerID=40&md5=17a3c1ca7287cf4779af07a39fdf8f29","Waste management in construction is critical for the sustainable treatment of building-related construction and demolition (C&D) waste materials, and recycling of these wastes has been considered as one of the best strategies in minimization of C&D debris. However, recycling of C&D materials may not always be a feasible strategy for every waste type and therefore recycling and other waste treatment strategies should be supported by robust decision-making models. With the aim of assessing the net carbon, energy, and water footprints of C&D recycling and other waste management alternatives, a comprehensive economic input-output-based hybrid life-cycle assessment model is developed by tracing all of the economy-wide supply-chain impacts of three waste management strategies: recycling, landfilling, and incineration. Analysis results showed that only the recycling of construction materials provided positive environmental footprint savings in terms of carbon, energy, and water footprints. Incineration is a better option as a secondary strategy after recycling for water and energy footprint categories, whereas landfilling is found to be as slightly better strategy when carbon footprint is considered as the main focus of comparison. In terms of construction materials' environmental footprint, nonferrous metals are found to have a significant environmental footprint reduction potential if recycled. © The Author(s) 2014."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902160375&partnerID=40&md5=b85f31ee5e57609c11df3c1081bf78c2","Magnetic transducers have been applied in impedance-based damage detection recently. Owing to the magneto-mechanical coupling characteristics between a magnetic transducer and the underneath metallic structure, a magnetic transducer can excite the host structure by means of the Lorenz force, and its electrical impedance is directly related to the host structure's mechanical impedance. Therefore, the change of electrical impedance before and after damage occurrence can be used as damage indicator. Since there is no direct contact between the magnetic transducer and the host structure, it appears that the magnetic transducer has advantage in online health monitoring of many structures with complex geometries and boundaries. However, one key issue is that the coupling between the magnetic transducer and the host structure is strongly influenced by the lift-off distance (i.e. the distance from the transducer to the host structure) which changes as the structure is inevitably subject to oscillation/movement due to environment disturbance. In this research, we propose a new approach of transformed impedance that can explicitly take the lift-off distance change into consideration to facilitate efficient and robust decision making. This algorithm takes advantage of the lift-off distance embedded in the impedance measurement, and is capable of removing the lift-off variation without explicitly measuring the lift-off variation. Numerical simulations and experimental validations are carried out to demonstrate the effectiveness. © 2014 SPIE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904644707&partnerID=40&md5=0ac9e74c4283664d8b606e60bbc30a4d","Applications running in today's data centers show high workload variability. While seasonal patterns, trends and expected events may help building proactive resource allocation policies, this approach has to be complemented with adaptive strategies which should address unexpected events such as flash crowds and volume spikes. Additionally, the limitations of current I/O infrastructures in the face of dramatic increase of data generation require, the ability to build novel abstractions and models for robust decision making regarding data layout and data locality. In this work, we present CONDESA (CONtrolling Data distribution on Elastic Server Architectures), a framework for exploring adaptive data distribution strategies for elastic server architectures. To the best of our knowledge CONDESA is the first platform that permits to systematically study the interplay between five data related strategies: workload prediction, adaptive control of data distribution and server provisioning, adaptive data grouping, adaptive data placement, and adaptive system sizing. We demonstrate how CONDESA can be used for browsing the design space of adaptive data distribution policies. We show how prediction models can be compared in terms of overhead and accuracy. We evaluate the impact of change detection on prediction accuracy and how CONDESA can be used for choosing an adequate prediction horizon. We demonstrate how adaptive prediction can be used for sizing a server system. Finally, we show how prediction models, change detection strategies, and data placement policies can be combined and compared based on server utilization, load balance, data locality, over-and underprovisioning. © 2013 IEEE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901496137&partnerID=40&md5=05ea21d767f9df826d3364421b80332a","Despite information on the benefits of climate change adaptation planning being widely available and well documented, in the UK at least relatively few real-world cases of scenario led adaptation have been documented. This limited uptake has been attributed to a variety of factors including the vast uncertainties faced, a lack of resources and potentially the absence of probabilities assigned to current climate change projections, thereby hampering conventional approaches to decision making under risk. Decision criteria for problems of uncertainty have been criticised for being too restrictive, crude, overly pessimistic, and data intensive. Furthermore, many cannot be reproduced reliably from sub-samples of the UKCP09 probabilistic dataset. This study critically compares current decision criteria for problems of uncertainty and subsequently outlines an improved criterion which overcomes some of their limitations and criticisms. This criterion, termed the Green Z-score, is then applied to a simplified real-world problem of designing an irrigation reservoir in the UK under climate change. The criterion is designed to be simple to implement, support robust decision making and provide reproducible results from sub-samples of the UKCP09 probabilistic dataset. It is designed to accommodate a wide range of risk appetites and attitudes and thereby encourage its use by decision makers who are presently struggling to determine whether and how to adapt to future climate change and its potential impacts. Analyses using sub-samples of the complete probabilistic dataset showed that the Green Z-score had comparable reproducibility to Laplace and improved reproducibility compared to other current decision criteria, and unlike Laplace is able to accommodate different risk attitudes. © 2013 The Authors."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906725087&partnerID=40&md5=1d0889b2a2f9fc5d7991cea8a4382632","Due to the fluctuations in energy prices and the global warming effects as a result of pollution emission in the energy generation/conversion processes, energy conservation has gained much attention recently. The buildings in US consume significant amount of energy. Thus undertaking a building retrofitting project, in which new technology and features are added to the existing structure, can potentially both yield a good return on investment due to the future savings in energy consumption and reduce the negative environmental impact due to reduction in greenhouse gas emission. In this project, we study how to optimally perform energy retrofitting of existing building structures. Most existing methods either focus solely on minimizing energy consumption, while overlooking the financial incentives and occupant comfort, or aim at optimizing the energy related expenses under one particular deterministic setting and omitting the stochastic risks, such as volatility in energy pricing, weather uncertainties, in the operating environment. Hence, the building owners may not find the recommendation for building construction/recommendation relevant and profitable over its lifetime, and may not be willing to undertake such projects. This work proposes a novel risk management system on building energy retrofit, which uses a comprehensive optimization framework and considers both deterministic and stochastic factors. In one case study, we show that this system can improve the performance of the building under uncertainties while satisfying constraints imposed by occupant. © 2014 IEEE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911867801&partnerID=40&md5=b6b3b74398984b65237b47e689df335d","In order to use models to understand deeply uncertain future conditions, managers must be able to pose and test hypotheses about their management problems. In Iterative Closed Question Methodology (ICQM), a series of closed questions are used to structure thinking about hypotheses while looking beyond a problem's existing modeling representation. Our research is exploring how ICQM can contribute to a framework called Many Objective Robust Decision Making (MORDM), which uses multiobjective optimization and ensembles of uncertain future states of the world to create and evaluate robust solutions for environmental management. A visualization software tool; AeroVis, has greatly aided implementation of MORDM, allowing a user to plot tradeoffs between conflicting objectives, ""brush"" their preferences on plotted and unplotted variables, and view visualizations of solution robustness. This visualization approach provides a rich set of conclusions which is not always well understood (i.e. the user can interpret results that the modeler did not intend). In this presentation, we explore how visualization tools iteratively generate and evaluate management hypotheses and conclusions. We discuss the types of conclusions that can be made from AeroVis MORDM visualizations and walk through experimental examples of how individuals reason with the decision support tool. This illustrates that working within an MORDM framework helps the user consider alternate model assumptions about future inputs, parameters and model structure, supporting the idea that model assumptions can provide useful scenarios for environmental management."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902539052&partnerID=40&md5=9fe1b303dc14d30acdb57e17467c31cc","Purpose: Plevin et al. (2014) reviewed relevant life cycle assessment (LCA) studies for biofuels and argued that the use of attributional LCA (ALCA) for estimating the benefits of biofuel policy is misleading. While we agree with the authors on many points, we found that some of the arguments by the authors were not presented fairly and that a number of specific points warrant additional comment. The main objective of this commentary is to examine the authors' comparative statements between consequential LCA (CLCA) and ALCA. Methods: We examined the notion that the LCA world is divided into CLCA and ALCA. In addition, we evaluated the authors' notion of ""wrong"" models. Results: We found that the authors were comparing an idealized, hypothetical CLCA with average (or less than average), real-life ALCAs. Therefore, we found that the comparison alone cannot serve as the basis for endorsing real-life CLCAs for biofuel policy. We also showed that there are many LCA studies that do not belong to either of the two approaches distinguished by the authors. Furthermore, we found that the authors' notion of ""wrong"" models misses the essence of modeling and reveals the authors' unwarranted confidence in certain modeling approaches. Conclusions: Dividing the LCA world into CLCAs and ALCAs overlooks the studies in between and hampers a constructive dialog about the creative use of modeling frameworks. Unreasonable confidence in certain modeling approaches based on their ""conceptual"" superiority does not help support ""robust decision making"" that should ultimately land itself on the ground. © 2014 Springer-Verlag."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893424569&partnerID=40&md5=44cc5905f5cc51070efe3b6b26a76652","Global wetland biodiversity loss continues unabated, driven by increased demand for freshwater. A key strategy for conservation management of freshwater systems is to maintain the quantity and quality of the natural water regimes, including the frequency and timing of flows. Formalizing an ecological model depicting the key ecological components and the underlying processes of cause and effect is required for successful conservation management. Models linking hydrology with ecological responses can prove to be an invaluable tool for robust decision-making of environmental flows. Here, we explored alternative water management strategies and identified maximal strategies for successful longterm management of colonial waterbirds in the Macquarie Marshes, Australia. We modeled fluctuations in breeding abundances of 10 colonial waterbird species over the past quarter century (1986-2010). Clear relationships existed between flows and breeding, both in frequencies and total abundances, with a strong linear relationship for flows .200 GL. Thresholds emerged for triggering breeding events in all 10 species, but these varied among species. Three species displayed a sharp threshold response between 100 GL and 250 GL. These had a breeding probability of 0.5 when flows were .180 GL and a 0.9 probability ofbreeding with flows .350 GL. The remaining species had a probability greater than 0.5 ofbreeding with flows .400 GL. Using developed models, we examined the effects of fiveenvironmental flow management strategies on the variability of flows and subsequentlikelihood of breeding. Management to different target volumes of environmental flowsaffected overall and specific breeding probabilities. The likelihood of breeding for all 10 colonial waterbirds increased from a regulated historical mean (6SD) of 0.36 6 0.09 to 0.53 6 0.14, an improvement of 47.5% 6 18.7%. Management of complex ecosystems depends ongood understanding of the responses of organisms to the main drivers of change. Considerable opportunity exists for implementing similar frameworks for other ecosystem attributes, following understanding of their responses to the flow regime, achieving a more complete model of the entire ecosystem."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891489294&partnerID=40&md5=23fdd91ebb2490e57855aba2dfbf520d","This paper addresses four levels in carbon management decision-making: government, enterprise, plant, and process. Robust decision-making at any level requires consideration of the constraints and requirements of other levels. The focus of the paper is the enterprise level, when a power generating company wishes to develop its long term carbon management strategy. The carbon reduction option is solvent-based PCC (post-combustion carbon capture), which has been discussed as the most accessible option for CCS (carbon capture and storage) objectives. The company desires to know whether/when/how to invest in PCC processes in order to satisfy government emission reduction regulations while achieving the maximum economic benefits over the planning horizon. We have developed a multi-period MILP (mixed-integer linear program) with the objective of maximizing NPV (net present value). The model is capable of finding the best investment decision, i.e. whether to invest in a PCC process or pay for the carbon tax/permit. When a PCC process is beneficial, the program determines the number of PCC trains (of different sizes) and the optimal installation time of each process. The model incorporates dynamic electricity and carbon market prices over the planning horizon. This allows the model to define the best operation strategy of a power plant and PCC process to utilize the maximum benefits of market prices by periodic adjustment of power generation and carbon capture rate. With this information, the company can buy or sell carbon permits over the planning horizon when either is more economical. © 2013 Elsevier Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994092365&partnerID=40&md5=97e4859b21a1690dcd47809af6682fea","The first deepwater well in the Saudi Arabian side of the Red Sea was going to be a rank wildcat well. Saudi Aramco Upstream leadership assembled a team of experienced, knowledgeable and dedicated professional staff across the Exploration Geology, Geophysics, Reservoir and Exploration Drilling organization to undertake the challenge of safe and successful well delivery. Contemporary frontier exploration projects utilize project development processes with long inter-nodal phasing schemes that follow traditional lateral cascade workflow methodology or the common waterfall process. Published works on project development best practices cite project development and management processes that are ostensibly an adoption of product development processes with stage gates and decision points. In order to meet the Saudi Aramco leadership challenge for project delivery ∼ speed, scale and a safe delivery mandate, non-negotiable parameters established for the project - innovative project interface management tools and strategies devised to enable multidisciplinary resources both internal and external to recognize vertical and lateral impacts of decision, specification and system dependencies, compatibility, boundaries and critical path sensitivities was developed. The pervasive cohesion and shared sense of responsibility by the core project team and service partners, seamless communication protocols among stakeholders though fraught with complex and diverse perspectives was also a source of robust decision making. Several industry first technologies for well construction were successfully deployed and the meticulous engineering motions undertaken to ensure hitch-free deployment are noteworthy. The strategies for selection of service contractors along service and product lines, capabilities and compatibility for performance management are also described. This paper describes drilling engineering and operations preparation, front end engineering and design activities, and the project delivery success of Saudi Aramco's first deepwater exploration program in the Red Sea. The validation of the potency of the strategies, tools and systems employed is embodied in the drilling to record regional total depth of the first deepwater well of Saudi Aramco's Red Sea Deepwater Exploration Campaign Program. © 2014 Society of Petroleum Engineers."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902491107&partnerID=40&md5=e7b42a1ea5462b2a04984c8988201c50","The aim of this study is to present overview on R packages for structural equation modeling. Structural equation modeling, a statistical technique for testing and estimating causal relations using an amalgamation of statistical data and qualitative causal hypotheses, allow both confirmatory and exploratory modeling, meaning they are matched to both hypothesis testing and theory development. R project or R language, a free and popular programming language and computer software surroundings for statistical computing and graphics, is popularly used among statisticians for developing statistical computer software and data analysis. The major finding is that it is necessary to build excellent and enough structural equation modeling packages for R users to do research. Numerous packages for structural equation modeling of R project are introduced in this study and most of them are enclosed in the Comprehensive R Archive Network task view Psychometrics. © Maxwell Scientific Organization, 2014."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923858526&partnerID=40&md5=b19393a1791a13ebfc66297b9e0ea756","Combining concrete policy-oriented modeling strategies of World War II with what was received as traditional neoclassical theory, in 1956 Robert Solow constructed a simple, clean, and smooth-functioning “design” model that served many different purposes. As a working object, it enabled experimentation with utopian long-run equilibrium growth. As an instrument of measurement, it was applied to time-series data. As a prototype, it was supposed to feed into larger-scale econometric models that were, in turn, thought of as technologies for policy advice. Used as a teaching device, Solow’s design became a medium of “spreading the technique” and one of the symbols for neoclassical macroeconomics that soon became associated with MIT. © 2015 by Duke University Press."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897963193&partnerID=40&md5=d4445d8820b7c5737dfc391208bc385c","The roll-out of new infrastructural networks in space-constrained areas requires the careful consideration of limited paths. This design task is aggravated if the number and/or location of connectors is unknown. The novel combination of graph theory and concepts of exploratory modelling in this contribution allow for an analysis of most likely paths that maximise the value for the planners. We apply this approach to two proposed energy networks in the Netherlands: a biogas network of farmers in the province of Overijssel and an LNG pipeline connecting industries in the Port of Rotterdam. The examples demonstrate the ease of use and simplicity of this approach that transparently deals with unknowns. © 2013 Springer Science+Business Media New York."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897020973&partnerID=40&md5=f12d6eb8b815678392c7ea4d1e1ad2e8","The motesanib phase III MONET1 study failed to show improvement in overall survival (OS) in non-small cell lung cancer, but a subpopulation of Asian patients had a favorable outcome. We performed exploratory modeling and simulations based on MONET1 data to support further development of motesanib in Asian patients. A model-based estimate of time to tumor growth was the best of tested tumor size response metrics in a multivariate OS model (P < 0.00001) to capture treatment effect (hazard ratio, HR) in Asian patients. Significant independent prognostic factors for OS were baseline tumor size (P < 0.0001), smoking history (P < 0.0001), and ethnicity (P < 0.00001). The model successfully predicted OS distributions and HR in the full population and in Asian patients. Simulations indicated that a phase III study in 500 Asian patients would exceed 80% power to confirm superior efficacy of motesanib combination therapy (expected HR: 0.74), suggesting that motesanib combination therapy may benefit Asian patients."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904012559&partnerID=40&md5=d239e954a8d4603116f7f6e0de886ff9","The objective of this contribution is to provide a review and suggest possible extensions of the Failure Mode Effects Analysis (FMEA), Hazard Risk Assessment (HRA) [2] and to demonstrate the importance of these tools to general probabilistic design for reliability (PDfR) [8]. FMEA was first introduced in the 1960s by the U.S. National Aeronautics and Space Administration (NASA) and is currently used extensively across many industries. FMEA is useful in understanding the failure modes of various products, qualifying the effects of failure and aiding in the development of mitigation strategies. It is a useful tool in improving quality, reliability, and the maintainability of designs, and is a critical component in risk management strategies and evaluations. This is, actually, the approach of the prognostics and health monitoring/management (PHM) engineering. Failure mode effects and criticality analysis (FMECA) [1] is an extension of (FMEA). While FMEA is a bottom-up, inductive analytical method which may be performed at either the functional or piece-part level, FMECA extends FMEA by including a criticality analysis that is aimed, like PDfR is, at charting the probability of failure modes against the severity of their consequences. The result highlights failure modes with relatively high probability and severity of consequences, allowing remedial effort to be directed where it will produce the greatest value. FMECA tends to be preferred over FMEA in space and North Atlantic Treaty Organization (NATO) military applications, while various forms of FMEA predominate in other industries. Being extensions of the FMEAs, FMECAs add severity and probability ranking aspects to the problems of interest. This is accomplished through an appropriate HRA - an engineering process of where the risk of an event is quantified by examining the chain of the preceding events, starting with, e.g., the failure mode, then stepping through to the end effects. The approach allows quantification of risk through the use of probabilistic risk analysis (PRA) and is addressed and discussed in detail. Failure oriented accelerated testing (FOAT) [9] could and should be viewed as an important constituent part of the effort. It is shown that care must be taken to establish the appropriate probabilities, to identify the statistical independence of the random variables of importance, as well as to assess the trustworthiness of the available or obtained data. It is indicated that an important drawback of the FMEA is the lack of pure operational (field) failure data. These data are frequently utilized from the computerized maintenance management system (CMMS) software, which does not always provide a true snapshot of the Mean Time Between Failures (MTBF) or other critical characteristics of the product. This results in the situation that personal judgment plays a large part in the development of the FMEA. Several papers have been published recently on development of Fuzzy FMEA methodologies (see, e.g., [7]). This application of fuzzy logic to Hazard Risk Analysis will allow additional uncertainty and inaccuracy to be modeled throughout FMECA development, leading to a more robust decision making with consideration of various uncertainties. © 2014 IEEE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920628526&partnerID=40&md5=a0bc47286c63505d9853ecf297ca7f7a","In recent decades, higher education systems and institutions have been called to respond to an unprecedented number of challenges. Major challenges emerged with the phenomenal increase in the demand for higher education and the associated massive expansion of higher education systems. In response universities were called to adopt planning and research methods that would enable them to identify and address the needs of a larger, more diverse student body. Higher education institutions began to place greater emphasis on planning and marketing, seeking to maintain their position in an increasingly competitive higher education market. Under the current economic downturn, universities are under pressure to further cut costs while maintaining their attractiveness to prospective students. As a result educational policy makers and administrators are called to select the 'right' alternatives, aiming for both efficiency and effectiveness in delivered outcomes. This book provides insights into the use of data as an input in planning and improvement initiatives in higher education. It focuses on uses (and potential abuses) of data in educational planning and policy formulation, examining several practices and perspectives relating to different types of data. The book is intended to address the need for the collection and utilization of data in the attempt to improve higher education both at the systemic and the institutional level. ""In a fast changing world of Higher Education, valid, reliable and meaningful data assume increasing importance as a factor in effective leadership and management. The wide ranging selection of essays provide state-of-the-art consideration of the technical and policy issues which underpin effective decision making in universities."" - Professor Sir Howard Newby, Vice-Chancellor of the University of Liverpool ""This new book edited by Menon, Terkla and Gibbs will be an important resource for those of us in higher education for whom acquiring, reporting, understanding and most importantly using data have become a required core competency. This volume should also be useful to policy makers who are attempting to develop appropriate metrics for assessing institutional performance. While not avoiding the practical limitations and workloads associated with academic data, the editors' orientation is positive as evidenced by their chosen title - Using Data to Improve Higher Education. The ensemble contributions of an impressive collection of chapter contributors navigate a balanced path of demonstrating the power of good data tempered by the caution that having good data is only a necessary, not a sufficient condition for robust decision making. The book's final chapter of conclusions and policy implications employs an underlying theme of mission. This is insightful in that I am convinced that authenticity of institutional mission and the manner in which mission is used to develop and assess people and programs will be critical if the academy is to demonstrate its worth to an increasingly skeptical public."" - Professor Vincent P. Manno, Provost and Dean of Faculty, Professor of Engineering, F.W. Olin College of Engineering. © 2014 SensePublishers-Rotterdam, The Netherlands. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920733214&partnerID=40&md5=62c80873bf0e038fd081e76c430b7b9e","Business Process Management is often associated with software to manage, control, and support operational processes. And in order to meet dynamic and new business needs with flexible information technology solutions, more and more enterprises intend to build their own IT infrastructure under Service Oriented Architecture. Currently analyzing and refining the existing business process models for configuring enterprise information system is a hot area of research. In our approach, we apply the process mining technology to the event logs to discover the scenarios, each of which mainly consists of task originators, sub-process (ordering of service invocations) and business objects. Based on composition of these scenarios, the enterprise is able to redesign the process models and combine closely the business process with the services provided by different application systems in SOA. When new business requirements emerge, solution designers can devise a flexibly composite process that makes the best use of existing scenarios and glue the scenarios together with least augmentation or modification. © 2014 IEEE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901686691&partnerID=40&md5=0d4ccb6dc16b9ba5af6b6193245a6114","Rigdon's (2012) thoughtful article argues that PLS-SEM should free itself from CB-SEM. It should renounce all mechanisms, frameworks, and jargon associated with factor models entirely. In this comment, we shed further light on two subject areas on which Rigdon (2012) touches in his discussion of CB-SEM and PLS-SEM. Rigdon (2012) highlights ways to make better use of PLS-SEM's predictive capabilities, for example, by reverting to set correlations. We discuss this issue in more detail, highlighting the need to examine the predictive capabilities of models when developing and testing theories, and broach the issue of confirmatory versus exploratory modeling. As a result of our discussion, we call for the continuous improvement of the PLS-SEM method to uncover its capabilities for theory testing while retaining its predictive character. © 2014 Elsevier Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900325357&partnerID=40&md5=255e7c79f025dfd7103027ac049150a3","Variability of marine phytoplankton associated with El Niño-Southern Oscillation (ENSO) and potential biological feedbacks onto ENSO are investigated by performing coupled ocean/biogeochemical model experiments forced by realistic surface winds from 1951 to 2010. The ocean model used in this study is the MOM4, which is coupled to a biogeochemical model, called TOPAZ (Tracers in the Ocean with Allometric Zooplankton). In general, it is shown that MOM4-TOPAZ mimics the observed main features of phytoplankton variability associated with ENSO.By comparing the actively coupled MOM4-TOPAZ experiment with the ocean model experiments using prescribed chlorophyll concentrations, potential impacts of phytoplankton on ENSO are evaluated. We found that chlorophyll generally increases mean sea surface temperature (SST) and decreases subsurface temperature by altering the penetration of solar radiation. However, as the chlorophyll concentration increases, the equatorial Pacific SST decreases due to the enhanced upwelling of the cooler subsurface water with shoaling of mixed layer and thermocline. The presence of chlorophyll generally intensifies ENSO amplitude by changing the ocean basic state. On the other hand, interactively varying chlorophyll associated with the ENSO tends to reduce ENSO amplitude. Therefore, the two biological effects on SST are competing against each other regarding the SST variance in the equatorial Pacific. © 2014 Elsevier Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911917201&partnerID=40&md5=d9b4ce5241bb02504183d082818dc5f6","The flooding of rural and urban areas is an increasing hazard to society. Accurate and timely predictions are essential for the water manager to prepare and respond to these hazards. Predicting flooding requires a numerical model that represents the physical processes (rain, evaporation, infiltration, overland flow, groundwater flow). This model, fed with measurements, and possible measures, calculates the expected flooding. The traditional working method consists of a three step process: schematization setup, running and post-processing, with a total feedback time of hours. This process is suitable for confirmatory modeling. Most of the time, models are applied exploratory, requiring a different workflow. Enabling exploratory modeling requires a shift in utilisation of the instrument. Stakeholders are in control and together evaluate ideas by interacting with the model through a mobile compatible website, supported by the modelers expertise. Enabling this type of interactivity requires a new level of performance. The 3Di platform, in which the new approach was applied, consists of a new flooding and hydrological model (1D/2D) with a corresponding cloud based infrastructure. Applications in rural and urban areas of O(1000km2) at a resolution of O(0.1m) have shown its capabilities for both exploratory and confirmatory modeling. The ambition that every component should be at least a 100 times faster than the previous approach, resulted in several advancements, both in the numerical engine and the software that interacts with the user and pushes the data to the web. Here we show advancements in the architecture and model communication."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911892376&partnerID=40&md5=ed3ec4ba6e03f97d452a36393db9ebe3","While food security can be approached as a local issue, it is strongly influenced by factors at inter-regional and global scales related to production, transaction (e.g. trade and distribution) and consumption, and by drivers such as climate, population growth, diet change, as well as social, political and technological developments. Action on food security therefore benefits from being informed by current global patterns and potential future changes and taking an integrated approach to assessing impacts of proposed responses. Modelling can notably contribute by assessing the influence of various factors on food security. Due to the significant complexity and uncertainty involved, model development and use is simplified by approaching it as an exploratory process rather than aiming for a comprehensive historically accurate model. We present a macro-scale conceptual model to help structure and guide this exploration. We begin with the broad question ""Will future developments achieve and maintain food security?"" with the intent of exploring alternate possibilities of future developments, definitions of food security and factors influencing this question, beginning with assessing whether there is enough green and blue water to meet dietary energy requirements under typical current and future climatic variation. The conceptual model guides the selection of factors to explore sequentially through modelling (keeping other variables constant), iteratively building complexity as necessary. This helps to construct understanding using manageable building blocks, with the conceptual model evolving as it is used. The staged decomposition of this complex issue provides a framework to help build capacity for individuals and government agencies to understand their actions and policy respectively in a global context, with the hope that improving knowledge of adaptation options can help secure food supply to everyone."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902535735&partnerID=40&md5=2c23231d6d6b5d29e242aece9f8f11b7","Developing robust policies for complex systems is a profound challenge because of their nonlinear and unpredictable nature. Dealing with these characteristics requires innovative approaches. A possible approach is to design policies that can be adapted over time in response to how the future unfolds. An essential part of adaptive policymaking is specifying under what conditions, and in which way, to adapt the policy. The performance of an adaptive policy is critically dependent on this: if the policy is adapted too late or too early, significant deterioration in policy performance can be incurred. An additional complicating factor is that in almost any policy problem, a multiplicity of divergent and potentially conflicting objectives has to be considered. In this paper we tackle both problems simultaneously through the use of multi-objective robust simulation optimization. Robust optimization helps in specifying appropriate conditions for adapting a policy, by identifying conditions that produce satisfactory results across a large ensemble of scenarios. Multi-objective optimization helps in identifying such conditions for a set of criteria, and providing insights into the tradeoffs between these criteria. Simulation is used for evaluating policy performance. This approach results in the identification of multiple alternative conditions under which to adapt a policy, rather than a single set of conditions. This creates the possibility of an informed policy debate on trade-offs. The approach is illustrated through a case study on designing a robust policy for supporting the transition toward renewable energy systems in the European Union. The results indicate that the proposed approach can be efficiently used for developing policy suggestions and for improving decision support for policymakers. By extension, it is possible to apply this methodology in dynamically complex and deeply uncertain systems such as public health, financial systems, transportation, and housing. © 2014 Elsevier B.V. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890820243&partnerID=40&md5=5fc7e8e6ad053c33ff33266787bae990","In this paper, based on the measure-theoretic probability theory and the theory of stochastic differential equation (SDE), a stochastic fusion framework is proposed for the heterogeneous sensor network for the purpose of robust decision making. In this framework, for each sensor, its sample space and the corresponding σ-algebra are defined. Then, random variables, which are designed to meet the requirements of the operation in the battle field, are defined over the pairs of sample space and its σ-algebra. After that, the conditional expectation is taken for those random variables conditional on the union of σ-algebras to finish the information fusion process. Furthermore, to make the decision making process more robust, the undesired uncertainty in the fused information is hedged out based on the theory of SDEs, before the fused information is used for the decision making. © 2013 ISIF ( Intl Society of Information Fusi."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890034750&partnerID=40&md5=6778131dcb8a3073df18b73e493aa24d","We investigate robust decision-making under utility uncertainty, using the maximin criterion, which optimizes utility for the worst case setting. We show how it is possible to efficiently compute the maximin optimal recommendation in face of utility uncertainty, even in large configuration spaces. We then introduce a new decision criterion, setwise maximin utility (SMMU), for constructing optimal recommendation sets: we develop algorithms for computing SMMU and present experimental results showing their performance. Finally, we discuss the problem of elicitation and prove (analogously to previous results related to regret-based and Bayesian elicitation) that SMMU leads to myopically optimal query sets. © 2013 Springer-Verlag Berlin Heidelberg."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888421661&partnerID=40&md5=7eab8d1449e1190e27dfedb8075a36a2","A widely adopted approach to study and understand complex systems such as ant colonies or economic systems consists in their modeling and simulation. In contrast to the predominat use of models and simulation in science as a substitute for the real system in order to predict the system's beaviour, the methodology of exploratory modeling uses modeling and simulation as a tool to increase knowledge and understanding of the systems themselves, for example to better understand the dynamic properties of a system. In this paper we propose a model which was specifically devised to support this process of exploratory modeling. The model defines four lightweight building blocks such as information processing entities that can be freely combined to model a particular complex system. Furthermore, the model provides an explicit state representation that comprises the entire model including an explicit representation of the information that is individually available to every information processing entity of the model. We illustrate our introduction of the proposed model by means of short examples of a concrete system for the simulation of motions in a flock of birds."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888326915&partnerID=40&md5=b99f0c2b9940fa4a01b4a385529c6935","By illuminating a range of possible futures, scenario analysis has proven valuable as a means for organizing and communicating the many uncertainties associated with predicting the development of the linked energy, economic, and climate systems. Thus far, scenarios have mostly been defined according to a sequential, piecewise process in which experts create plausible storylines that are then used as inputs to formal models. However, as the storylines are drafted separately from model construction, it is often difficult for models to engage completely with scenario themes. As a solution, methods of 'scenario discovery' have been proposed which apply statistical techniques to sets of model simulations to identify regions of the stochastic parameter space that result in distinctively different levels of policy performance. In our previous work, we described a novel multiattribute scenario discovery method and demonstrated application to ENGAGE, an agent-based model (ABM) of economic growth, energy technology, and carbon emissions. In the current contribution, we further demonstrate the utility of this approach by using ENGAGE to generate socioeconomic scenarios relevant to a given emissions target. We find that population growth, improvement in labor productivity, and efficiency of learning-by-doing regarding carbon-free energy technology are the key factors driving the success rate in achieving the selected target. This implies that these features should form essential elements of the storylines underlying socioeconomic scenarios if they are to provide a meaningful exploration of policy efficacy. Such results are consistent with those of more conceptual approaches. However, by being derived from the results of a quantitative model, our formulation is intrinsically consistent with practicable modeling assumptions and specifications. © Springer Science+Business Media New York 2013."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889084767&partnerID=40&md5=41ccb3245657473787b1bd8a72fe30f0","This paper considers how listening to children's views and taking account of them is understood in different ways by educational psychologists. Rationales for listening to children include (1) supporting their rights to express views, (2) therapeutic benefits of participation, and either (3) valuing their views as insightful contributions to problem-solving or (4) considering them as contributory factors to the problem. This creates dilemmas about interpretation on different occasions, and a range of theoretical stances are discussed in terms of how far they can support robust decision-making about this interpretation, and a practical way forward is suggested using principles based on critical realism. © 2013 Association of Educational Psychologists."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894681535&partnerID=40&md5=669e5aa3cec1a746fa45f6f285d0b79c","Autonomy adjustment to a system requires a mechanism to implement the roles of the autonomous entities in the system. The required degree of autonomy to which the autonomous entities adhere is a highly debated topic. On one hand, people argue that strict minimal autonomy to the autonomous entities is sufficient in producing reliable systems. On the other hand, others deliberate that the entities with full autonomic capabilities are essential aspects of advanced intelligent and flexible systems. The adjustable autonomic agent approach can be a solution for both cases. In this paper, we extend the idea of modeling a spectrum of autonomy in a layered structure, where the agents can act at different layers of autonomy in order to fulfill the system's autonomic conditions. Consequently, a logical representation of the conceptual model of Layered Adjustable Autonomy (LAA) is proposed. The LAA model aims to give the system implicit control over the agents' decisions whenever necessary by managing the agents' autonomy, ensuring quality and robust decision-making. An Autonomy Analysis (AAM) and Situation Awareness (SAM) Modules are proposed to attest the dynamic distribution of agent autonomic performance to a degree of autonomy level. © 2013 The authors and IOS Press. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898333195&partnerID=40&md5=2555ff9ec7a0baf09761198b2a2e0dde","It has been declared that practicing science is aptly described as making, using, testing, and revising models. Modeling has also emerged as an explicit practice in science education reform efforts. This is evidenced as modeling is highlighted as an instructional target in the recently released Conceptual Framework for the New K-12 Science Education Standards: it reads that students should develop more sophisticated models founded on prior knowledge and skills and refined as understanding develops. Reflecting the purpose of engaging students in modeling in science classrooms, Oh and Oh (2011) have suggested five modeling activities, the first three of which were based van Joolingen's (2004) earlier proposal: 1) exploratory modeling, 2) expressive modeling, 3) experimental modeling, 4) evaluative modeling, and 5) cyclic modeling. This chapter explores how these modeling activities are embedded in high school physics classrooms and how each is juxtaposed as concurrent instructional objectives and scaffolds a progressive learning sequence. Through the close examination of modeling in situ within the science classrooms, the authors expect to better explicate and illuminate the practices outlined and support reform in science education. © 2013, IGI Global."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885419715&partnerID=40&md5=c08afec64fa5e915b301e34440ca0018","This study assessed the vulnerability of the water supply to climate change and variability in the South Korean provinces for the present and future with a fuzzy VIKOR approach. The key indicators of vulnerability were profiled with the Delphi surveys based on the IPCC-based vulnerability concept, in which vulnerability is defined as a function of sensitivity, adaptive capacity and climate exposure. The fuzzy VIKOR was used to aggregate the key indicators into a vulnerability score because it can provide a compromise solution considering overall satisfaction and regret of the selection of wrong provinces. Markedly different rankings between the fuzzy VIKOR and a conventional weighted sum for overall satisfactions suggest the importance of regret component of the selection of wrong provinces and inclusion of the uncertainty of information in the vulnerability assessment and consequent decision making for adaptation. Furthermore, diverse vulnerability rankings with the six different future scenarios suggest the need for robust decision making given such uncertainty. © 2013 Elsevier Inc."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886584847&partnerID=40&md5=5002872d1be916aa3032d52cc2fa3d19","Modelling the design and implementation of urban water infrastructure (particularly decentralised systems) for strategic planning and policymaking requires detailed information of the spatial environment and quantitative knowledge of social preferences. Currently available models, however, mostly use land use, population and impervious cover data without much regard for detailed urban form or society. This study develops an algorithm for determining urban form from minimal spatial data input by incorporating local planning regulations. The interaction between urban form and implementation of lot-scale infiltration systems under different social, biophysical and climate constraints is then investigated, firstly by looking at how this varies in different residential land uses and subsequently in a case study of a typical Melbourne residential subdivision of mixed land uses. Feasibility of infiltration and its downstream impact (runoff volume, frequency and pollution) were assessed for a range of social preferences (quantified as allowable garden space) and climate scenarios (30 % increase/decrease in rainfall and evapotranspiration). Performance indicators were determined through long-term simulation with the MUSIC software. Results show how different biophysical, planning, social and climate conditions affect infiltration feasibility as well as system performance. High infiltrating soils, for example, allow smaller, well-performing and socially less-imposing systems. Low infiltrating soils lead to larger system sizes, occupy much of the allotment's garden space, but nevertheless provide the benefit of runoff frequency reduction. Overall, climate impact was not significant except for areas with poorly infiltrating soils. Joint consideration of social, planning, climate and water management aspects potentially allows more efficient policymaking, as an array of system configurations can be tested against different multi-faceted scenarios. Such models can help facilitate better participatory planning and policymaking. © 2013 Springer Science+Business Media Dordrecht."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885216749&partnerID=40&md5=fcffeac0302740bd23f6e07f28554dd4","Planning for the future is inherently risky. In most systems, exogenous driving forces affect any strategy's performance. Uncertainty about the state of those driving forces requires strategies that perform well in the face of a range of possible, even improbable, future conditions. This study formalizes the relationship between different methods proposed in the literature for rigorously exploring possible futures and then develops and applies the computational technique of scenario discovery to the policy option of a subsidy for low-income households in downtown Lisbon. The work demonstrates one way in which urban models can be applied to identify robust urban development strategies. Using the UrbanSim model, we offer the first known example of applying computational scenariodiscovery techniques to the urban realm. We construct scenarios from combinations of values for presumed exogenous variables-population growth rate, employment growth rate, gas prices, and construction costs-using a Latin-hypercube-sample experimental design. We then data mine the resulting alternative futures to identify scenarios in which an example policy fails to achieve its goals. This demonstration of concept aims to lead to a new practical application of integrated urban models in a way that quantitatively tests the strategic robustness of urban interventions. © 2013 Pion and its Licensors."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880349947&partnerID=40&md5=8fdca4b23e40fbaff6a77a27157a513b","Scenario discovery offers a new means to characterize and communicate the information in computer simulation models under conditions of deep uncertainty. The approach first defines scenarios as the future states of world where a proposed policy fails to meet its goals and then uses statistical algorithms to find concise descriptions of such regions in large databases of simulation model results. Current scenario discovery applications rely on the Patient Rule Induction Method (PRIM), a user-interactive bump-hunting algorithm that identifies hyper-rectangular regions in the input space of the simulation model. While often successful, scenario discovery applications have been limited because in general a policy's vulnerabilities are not well described by the PRIM's hyper-rectangular regions. This study proposes and evaluates improved scenario discovery algorithms that address this challenge with a Principal Component Analysis (PCA)-based preprocessing step that transforms the original model input parameters so that PRIM can then identify high quality hyper-rectangular scenarios in the new rotated coordination system. We explore two versions. PCA-PRIM allows rotations among all uncertain model input parameters and CPCA-PRIM (for constrained PCA-PRIM) only allows rotations among parameters within user-specified domains. The latter may provide more useful information to users, who may find scenario axes described by linear combinations of related domain parameters more interpretable than combinations of dissimilar parameters. We run two sets of tests on the PCA-PRIM and CPCA-PRIM algorithms, the first using simulated test date and the second results from a model used in a previous RAND study of the cost-effectiveness of renewable energy portfolio standards. We find that the new algorithms produce higher quality scenarios than PRIM alone as evaluated by several important measures of merit. In the test data, PCA-PRIM produces improvements averaging 37 percent, and CPCA-PRIM averaging 14 percent, over PRIM alone. In the renewable energy policy case study, PCA-PRIM and CPCA-PRIM exhibit similar improvements of about 16 percent over PRIM, and CPCA-PRIM generates scenarios interpretable by, and that provide useful information to, decision makers. © 2013 Elsevier Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885638754&partnerID=40&md5=e6fc241011304f0726bc5253207d15ec","The evolutionary transition from water to land required new locomotor modes and corresponding adjustments of the spinal central pattern generators for locomotion. Salamanders resemble the first terrestrial tetrapods and represent a key animal for the study of these changes. Based on recent physiological data from salamanders, and previous work on the swimming, limbless lamprey, we present a model of the basic oscillatory network in the salamander spinal cord, the spinal segment. Model neurons are of the Hodgkin-Huxley type. Spinal hemisegments contain sparsely connected excitatory and inhibitory neuron populations, and are coupled to a contralateral hemisegment. The model yields a large range of experimental findings, especially the NMDA-induced oscillations observed in isolated axial hemisegments and segments of the salamander Pleurodeles waltlii. The model reproduces most of the effects of the blockade of AMPA synapses, glycinergic synapses, calcium-activated potassium current, persistent sodium current, and $$h$$ -current. Driving segments with a population of brainstem neurons yields fast oscillations in the in vivo swimming frequency range. A minimal modification to the conductances involved in burst-termination yields the slower stepping frequency range. Slow oscillators can impose their frequency on fast oscillators, as is likely the case during gait transitions from swimming to stepping. Our study shows that a lamprey-like network can potentially serve as a building block of axial and limb oscillators for swimming and stepping in salamanders. © 2013 Springer-Verlag Berlin Heidelberg."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944902467&partnerID=40&md5=e39bb501fa82d55146ad0e48f06a55ab","It has been declared that practicing science is aptly described as making, using, testing, and revising models. Modeling has also emerged as an explicit practice in science education reform efforts. This is evidenced as modeling is highlighted as an instructional target in the recently released Conceptual Framework for the New K-12 Science Education Standards: it reads that students should develop more sophisticated models founded on prior knowledge and skills and refined as understanding develops. Reflecting the purpose of engaging students in modeling in science classrooms, Oh and Oh (2011) have suggested five modeling activities, the first three of which were based van Joolingen's (2004) earlier proposal: 1) exploratory modeling, 2) expressive modeling, 3) experimental modeling, 4) evaluative modeling, and 5) cyclic modeling. This chapter explores how these modeling activities are embedded in high school physics classrooms and how each is juxtaposed as concurrent instructional objectives and scaffolds a progressive learning sequence. Through the close examination of modeling in situ within the science classrooms, the authors expect to better explicate and illuminate the practices outlined and support reform in science education. © 2014 by IGI Global. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884256921&partnerID=40&md5=b60db0fccc59208ab43db74062b9ed8e","Hydrological modeling was used for projecting average annual water availability in Nigeria in the future, comparing a baseline (1976-2005) with a 30-year future (2036-2065) period, simulated under an ensemble of ten climate projections. Simulations converged in projecting by mid-century an increase in water flows for almost half of the country. Models agreed also in projecting decrease and stability in water flows for 13% of the country, while uncertainty covers about one-third of Nigeria. Lack of agreement among different climate models on precipitation and inflow makes it difficult to project how much water will be effectively available in the future for irrigation. Reservoir size is usually designed to ensure sufficient storage providing a given yield based on past climate. The objective of this paper is evaluating what storage investments are suitable for meeting irrigation development targets under as many climate outcomes as possible. A simple methodology for more robust investment planning is suggested and the risk of over- or under-designing storage based on past climate is exemplified. This preliminary study shows that, for more than half of the country, using the historical climate as a guide to the design of future storage might lead to inappropriate investment decisions, resulting in excessive or insufficient capital outlays. The conclusions of the paper do not entail endorsement by the World Bank or its Board of Directors. © IWA Publishing 2013."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883898253&partnerID=40&md5=f2dee51fe16b947e8e37c8c659ea4972","Natural gas is expected to become more important in the future to support the energy transition. However, the upstream gas sector faces many uncertainties which complicate the decision making to supply the required amounts of natural gas. This study deals with these uncertainties, by framing the natural gas production in a system of systems perspective, building a system dynamics model, carrying out an exploratory analysis and testing policy options. The results show that the annual production volume, energy consumption, onshore land use and CO2 emission objectives can be achieved in the Netherlands only in a small portion of the future scenarios. Policies such as production limits and end of field life techniques are beneficial to increase the production, but also increase CO2 emissions. In future studies, more policy alternatives can be generated and tested, the uncertainty analysis can be extended and the model can be used in broader energy models. Copyright © 2013 Inderscience Enterprises Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883187501&partnerID=40&md5=529d04b73ecf3890bc2880172c3f0849","Purpose: Healthcare resources will always be limited, and as a result, difficult decisions must be made about how to allocate limited resources across unlimited demands in order to maximize health gains per resource expended. Governments and hospitals now in severe financial deficits recognize that reengagement of physicians is central to their ability to contain the runaway healthcare costs. Health economic analysis provides tools and techniques to assess which investments in healthcare provide good value for money vs which options should be forgone. Robust decision-making in healthcare requires objective consideration of evidence in order to balance clinical and economic benefits vs risks. Principal findings: Surveys of the literature reveal very few economic analyses related to anesthesia and perioperative medicine despite increasing recognition of the need. Now is an opportune time for anesthesiologists to become familiar with the tools and methodologies of health economics in order to facilitate and lead robust decision-making in quality-based procedures. For most technologies used in anesthesia and perioperative medicine, the responsibility to determine cost-effectiveness falls to those tasked with the governance and stewardship of limited resources for unlimited demands using best evidence plus economics at the local, regional, and national levels. Applicable cost-effectiveness, cost-utility, and cost-benefits in health economics are reviewed in this article with clinical examples in anesthesia. Conclusions: Anesthesiologists can make a difference in the wider governance of healthcare and health economics if we advance our knowledge and skills beyond the technical to address the ""other"" dimensions of decision-making - most notably, the economic aspects in a value-based healthcare system. © 2013 Canadian Anesthesiologists' Society."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951983257&partnerID=40&md5=ff7109624c685695ed8913ef82b5cf9b","Innovation, which is acknowledged as a key factor for the development of companies, may be too delicate to become a success for companies. Uncertainty related to value creation is often an important dissuasive reason. This is particularly the case for high-tech small and medium enterprises (SMEs). Indeed, they often have scarce human and financial resources which handicap them in accessing new knowledge. This weakness can be crucial at a given stage of the innovation development because SMEs loosely represent processes and knowledge. The second aspect we consider here is the type of innovation developed: a disruptive technological innovation. It presents a high technological uncertainty and involves a market discontinuity. As a result, it is inherently characterized by a lack of knowledge regarding the technology and the market. This context leads to establish that one of the most important challenges in technological disruptive innovation projects in SMEs is the identification and access to critical knowledge. The literature review describes innovation processes as a more or less complex succession of activities and decisions. But the decision-making micro-stages, whose quality depends on the ability of identifying and accessing specific and critical knowledge, is one weak point of high-tech SMEs. We propose here a model to increase efficiency of critical decisions in disruptive innovation projects under the following hypothesis: critical decision-making in a disruptive technological innovation project in a SME is easier, more rational and robust when decision-makers know its impacts in term of value creation and risks associated to all decision alternatives. This model is tested within a project on two decisions assessed as critical; one of them is detailed here. The results of these two experimentations validate our hypothesis. We realized interviews on feedbacks from our case study that demonstrate that the use of this method was very helpful to inform and facilitate the work of decision makers. Our implementation of a formalized decision process enables robust decision-making without hindering the flexibility of the innovation process of the SME."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881539429&partnerID=40&md5=6f16636cf089dfd44dfb2fe15114e4e1","This article reports on the ongoing work and research involved in the development of a sociotechnical model of urban water systems. Socio-technical means the model is not so much concerned with the technical or biophysical aspects of urban water systems, but rather with the social and institutional implications of the urban water infrastructure and vice versa. A socio-technical model, in the view purported in this article, produces scenarios of different urban water servicing solutions gaining or losing influence in meeting water-related societal needs, like potable water, drainage, environmental health and amenity. The urban water system is parameterised with vectors of the relative influence of each servicing solution. The model is a software implementation of the Multi-Pattern Approach, a theory on societal systems, like urban water systems, and how these develop and go through transitions under various internal and external conditions. Acknowledging that social dynamics comes with severe and non-reducible uncertainties, the model is set up to be exploratory, meaning that for any initial condition several possible future scenarios are produced. This article gives a concise overview of the necessary theoretical background, the model architecture and some initial test results using a drainage example. © IWA Publishing 2013."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881218358&partnerID=40&md5=21d0e6e57bff0d300d36d293605cdc3c","The objective of evidence-based medicine is to come to well reasoned and justified clinical decisions regarding an individual patient's case based on the integration of case-specific knowledge, medical expertise, and the best available clinical evidence. One significant challenge implicated in this pursuit stems from the volume of relevant information that can easily exceed what can reasonably be assessed. Thus intelligent systems that can mine and synthesize vast amounts of information would be invaluable. The reconciliation of such systems with the complexity and subtlety of decision support in medicine requires specialized capabilities. One untapped capability is furnished through the gap in information between what is known and what needs to be known to justify a decision. In this paper, we explore the value of an information gap analysis for robust decision-making in the context of evidence-based medicine with an eye to the potential role in automated evidence-based reasoning systems. © 2013 Springer-Verlag."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879735572&partnerID=40&md5=283b0b76e5c3b0fa7fd05f1ffa5bfe49","Transportation infrastructure could be vulnerable to local manifestations of global climate change, such as storm frequencies and durations of seasons. To adapt, transportation agencies need methodologies for reprioritizing their assets subject to the new sources of vulnerability. Prioritizing assets is nontrivial when criteria assessments and owner/operator preferences are considered in conjunction with the possible climate scenarios. Few efforts to date have addressed these scenarios in a priority setting for infrastructure asset management in the literature. This paper extends a scenario-based multicriteria decision framework that can assist decision makers in effectively allocating limited resources to adapt transportation assets to a changing climate. The framework is demonstrated with one of the most susceptible metropolitan transportation systems in the United States, the Hampton Roads region in coastal southeastern Virginia. First, the high-level goals of a long-range transportation plans are used in a traditional multicriteria analysis to generate a baseline prioritization of assets. Next, several scenarios that incorporate and combine a variety of climate conditions are identified. Finally, the scenarios are used to adjust the initial criteria weighting, which results in several reprioritizations of the assets. The results help to identify the most influential scenarios and characterize the sensitivity of the baseline prioritization across multiple scenarios. With these results, additional scientific and investigative efforts can be focused effectively to study and understand the influential scenarios. © 2013 American Society of Civil Engineers."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877110300&partnerID=40&md5=641edb21dc473cb346b81b5d6d5d292c","A consistent methodology enabling the estimation of the economic losses associated with drought and the comparison of estimates between sites and across time has been elusive. In this paper, we develop an ecosystem service approach to fill this research gap. We apply this approach to analysis of the Millennium Drought in the South Australian portion of the Murray-Darling Basin which provided a natural experiment for the economic estimation of hydrological ecosystem service losses. Cataloguing estimates of expenditures incurred by Commonwealth and State governments, communities and individuals, we find that nearly $810. million was spent during the drought to mitigate losses, replace ecosystem services and adapt to new ecosystem equilibria. The approach developed here is transferable to other drought prone regions, providing insights into the potentially unexpected consequences of drought and ecosystem thresholds and socioeconomic and political tipping points after which ecosystem restoration may become very costly. Our application to the South Australian Murray-Darling Basin demonstrates the potential of this approach for informing water, drought preparedness and mitigation policy, and to contribute to more robust decision-making. © 2013."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880380965&partnerID=40&md5=0aa90e4a1f16008a3b100cfdea7f3be0","The design of a robust and reliable groundwater remediation system encounters major difficulties owing to the inherent uncertainty of hydrogeological parameters. Based on the commonly used deterministic groundwater multi-objective optimization methods, a probabilistic improved niched Pareto genetic algorithm (PINPGA) is proposed for this purpose. The PINPGA uses two techniques including probabilistic Pareto domination ranking and probabilistic niche technique to find Pareto optimal solutions of groundwater remediation systems under uncertainty. Also, the performance of the proposed algorithm is evaluated through a synthetic pump-and-treat (PAT) groundwater remediation system under a random lognormal distribution of hydraulic conductivity (K) field. At first, the Sequential Gaussian Simulation (SGSIM) is used to generate conditional lnK realizations based on the sampled conditioning data acquired by the field test. Then Monte Carlo simulation is applied to address uncertainty analysis and risk assessment of contaminant transport fate associated with different numbers of conditional lnK points. Compared with the existing improved niched Pareto genetic algorithm (INPGA) with a simple averaging approach, the proposed PINPGA with a probabilistic and small sample size (as few as 5) of lnK realizations can find Pareto optimal solutions with lower variability and higher reliability, leading to a robust decision-making."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879390451&partnerID=40&md5=fd12761207df807ea1df7a9712e20d57","Energy policy formulation is the process of identifying the policy options mixture with the greatest potential to achieve the long-term goals for an energy system's economic and environmental performance. However, the impact of a policy option adopted today will be visible in a time frame that exceeds the decade. In fact, the longer the time for a policy's impact to become visible, the more vulnerable the policy is to unforeseen future eventualities. This fact makes difficult and even misleading the attempt to quantify the probabilities for the realization of the different possible future outcomes. This article aims to present a methodological framework that provides policy makers with the ability to identify the conditions under which their choice about a policy's implementation would change, as well as the policy option combinations that are characterized by the greatest immunity in relation to the uncertainty of all possible future outcomes. © Taylor & Francis Group, LLC."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878442872&partnerID=40&md5=65aa4f05d249c6b7836436156c0578f2","Stationarity assumptions of linked human-water systems are frequently invalid given the difficult-to-predict changes affecting such systems. In this case water planning occurs under conditions of deep or severe uncertainty, where the statistical distributions of future conditions and events are poorly known. In such situations predictive system simulation models are typically run under different scenarios to evaluate the performance of future plans under different conditions. Given that there are many possible plans and many possible futures, which simulations will lead to the best designs? Robust Decision Making (RDM) and Info-Gap Decision Theory (IGDT) provide a structured approach to planning complex systems under such uncertainty. Both RDM and IGDT make repeated use of trusted simulation models to evaluate different plans under different future conditions. Both methods seek to identify robust rather than optimal decisions, where a robust decision works satisfactorily over a broad range of possible futures. IGDT efficiently charts system performance with robustness and opportuneness plots summarising system performance for different plans under the most dire and favourable sets of future conditions. RDM samples a wider range of dire, benign and opportune futures and offers a holistic assessment of the performance of different options. RDM also identifies through 'scenario discovery' which combinations of uncertain future stresses lead to system vulnerabilities. In our study we apply both frameworks to a water resource system planning problem: London's water supply system expansion in the Thames basin, UK. The methods help identify which out of 20 proposed water supply infrastructure portfolios is the most robust given severely uncertain future hydrological inflows, water demands and energy prices. Multiple criteria of system performance are considered: service reliability, storage susceptibility, capital and operating cost, energy use and environmental flows. Initially the two decision frameworks lead to different recommendations. We show the methods are complementary and can be beneficially used together to better understand results and reveal how the particulars of each method can skew results towards particular future plans. © 2013 Elsevier B.V."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883182173&partnerID=40&md5=9d260958f4c97d2c99a2764a261a0429","Coastal Louisiana's built and natural environment faces risks from catastrophic tropical storms. Concurrently, the region is experiencing a dramatic conversion of coastal land and associated habitats to open water and a loss of important services provided by such ecosystems. Louisiana's Coastal Protection and Restoration Authority (CPRA) engaged in a detailed modeling, simulation, and analysis exercise, the results of which informed Louisiana's 2012 Comprehensive Master Plan for a Sustainable Coast. The Master Plan defines a set of coastal risk-reduction and restoration projects to be implemented in the coming decades to reduce hurricane flood risk to coastal communities and restore the Louisiana coast. Risk-reduction and restoration projects were selected to provide the greatest level of risk-reduction and land-building benefits under a given budget constraint while being consistent with other objectives and principles of the Master Plan. A RAND project team, with the guidance of CPRA and other members of the Master Plan Delivery Team, developed a computer-based decision-support tool, called the CPRA Planning Tool. The Planning Tool provided technical analysis that supported the development of the Master Plan through CPRA and community-based deliberations. This article provides a summary of the Planning Tool and its application in supporting the development of Louisiana's Master Plan. © 2013 Coastal Education & Research Foundation."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876884540&partnerID=40&md5=c33c2c313891a7c173df2ec7c64c0899","The use of scenarios has proven valuable for global change analysis as a means for organizing and communicating information about uncertain future socioeconomic conditions. A small group of well-defined scenarios can provide a set of standard reference cases for assessing the performance of candidate policies under alternative futures. However, traditional methods of defining scenarios may yield storylines that do not align well with the capabilities of downstream models. It can also be difficult to assess whether the number and scope of constructed scenarios most effectively cover the space of possible outcomes. Model-based methods of 'scenario discovery' have recently been proposed that apply statistical data-mining algorithms to a large number of model simulations to identify regions of the stochastic parameter space that lead to unacceptable policy performance. These regions then delineate practically relevant and internally consistent 'discovered scenarios'. To distinguish 'acceptable' from 'unacceptable' policy outcomes, existing methods require pre-specification of a threshold value on a single performance metric. We believe this requirement may present a barrier when decision-makers hold differing views on the relative importance of multiple policy objectives. Therefore, we describe a scenario discovery method that is multidimensional in the outcome space, thus precluding the need for users to agree on a single performance threshold or set of tradeoff weights. We demonstrate application of our approach to the results of ENGAGE, an agent-based model (ABM) of economic growth, energy technology, and carbon emissions. We believe that scenario discovery can add particular value to agent-based modeling, as ABMs typically generate a wider array of possible futures than aggregate-scale models. For this reason, a systematic method for sorting through the many stochastic model simulations to identify policy vulnerabilities and opportunities is essential. We conclude by discussing how our methodology might be applicable to the development of socioeconomic scenarios under the 'representative concentration pathways' (RCP) framework. © 2012 Elsevier Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883417333&partnerID=40&md5=71105d75c818bebe58172a566f53f564","This article describes the theoretical underpinnings and preliminary experimental support for option awareness (OA): the perception and comprehension of the relative desirability of available options, as well the underlying factors and trade-offs that explain that desirability. The authors' research has produced a body of theory and experimental findings supporting the potential for OA to beneficially augment situation awareness (SA) and help decision makers identify the most robust options: those that are most likely to turn out well under the widest range of possible future conditions. OA incorporates perspectives from rationalistic and naturalistic models of decision making, as both are used concurrently in the types of complex high-technology work the authors have examined, including emergency management, infectious disease containment, and air traffic control. The authors have developed approaches to support OA through the use of exploratory modeling and visual analytics. These systems were tested over the course of four humanin-the-loop experiments. The results demonstrate the value of this approach to improve decision accuracy, confidence, and speed for decision makers facing scenarios at varying levels of difficulty. The methodology described here provides a framework to move forward with research on supporting OA in complex and uncertain scenarios in a variety of task domains. Copyright © 2012, Human Factors and Ergonomics Society."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876943607&partnerID=40&md5=3d20a37ea99eecf146fded49dac65449","While the minimax (or maximin) strategy has become the standard and most agreedupon solution for decision making in adversarial settings, as discussed in game theory, computer science, and other disciplines, its power arises from the use of mixed strategies, also known as probabilistic algorithms. Nevertheless, in adversarial settings we face the risk of information leakage about the actual strategy instantiation. Hence, real robust algorithms should take information leakage into account. In this paper we introduce the notion of adversarial leakage in games, namely, the ability of a player to learn the value of b binary predicates about the strategy instantiation of her opponent. Different leakage models are suggested and tight bounds on the effect of adversarial leakage as a function of the level of leakage (captured by b) are established. The complexity of computing optimal strategies under these adversarial leakage models is also addressed. Together, our study introduces a new framework for robust decision making and provides rigorous fundamental understanding of its properties. © 2013 Society for Industrial and Applied Mathematics."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875376336&partnerID=40&md5=e5900a6518ae381652165b746a1854c7","Scenarios are commonly used to communicate and characterize uncertainty in many policy fields. One of the main challenges of scenario approaches is that analysts have to try and capture the full breadth of uncertainty about the future in a small set of scenarios. In the presence of deep uncertainty, this is even more challenging. Scenario discovery is a model-based technique inspired by the scenario logic school that addresses this challenge. In scenario discovery, an ensemble of model runs is created that encompasses the various uncertainties perceived by the actors involved in particular decision making situations. The ensemble is subsequently screened to identify runs of interest, and their conditions for occurring are identified through machine learning. Here, we extend scenario discovery to cope with dynamics over time. To this end, a time series clustering approach is applied to the ensemble of model runs in order to identify different types of dynamics. The types of dynamics are subsequently analyzed to identify dynamics that are of interest, and their causes for occurrence are revealed. This dynamic scenario discovery approach is illustrated with a case about copper scarcity. © 2012 Elsevier Inc."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877050337&partnerID=40&md5=d7af805ee560af8459a0e60729313a72","The selection of climate policies should be an exercise in risk management reflecting the many relevant sources of uncertainty. Studies of climate change and its impacts rarely yield consensus on the distribution of exposure, vulnerability or possible outcomes. Hence policy analysis cannot effectively evaluate alternatives using standard approaches, such as expected utility theory and benefit-cost analysis. This Perspective highlights the value of robust decision-making tools designed for situations such as evaluating climate policies, where consensus on probability distributions is not available and stakeholders differ in their degree of risk tolerance. A broader risk-management approach enables a range of possible outcomes to be examined, as well as the uncertainty surrounding their likelihoods. © 2013 Macmillan Publishers Limited. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874529980&partnerID=40&md5=a7351fa7dbfbcfbcdaff6f2ef4db29d5","This paper introduces many objective robust decision making (MORDM). MORDM combines concepts and methods from many objective evolutionary optimization and robust decision making (RDM), along with extensive use of interactive visual analytics, to facilitate the management of complex environmental systems. Many objective evolutionary search is used to generate alternatives for complex planning problems, enabling the discovery of the key tradeoffs among planning objectives. RDM then determines the robustness of planning alternatives to deeply uncertain future conditions and facilitates decision makers' selection of promising candidate solutions. MORDM tests each solution under the ensemble of future extreme states of the world (SOW). Interactive visual analytics are used to explore whether solutions of interest are robust to a wide range of plausible future conditions (i.e., assessment of their Pareto satisficing behavior in alternative SOW). Scenario discovery methods that use statistical data mining algorithms are then used to identify what assumptions and system conditions strongly influence the cost-effectiveness, efficiency, and reliability of the robust alternatives. The framework is demonstrated using a case study that examines a single city's water supply in the Lower Rio Grande Valley (LRGV) in Texas, USA. Results suggest that including robustness as a decision criterion can dramatically change the formulation of complex environmental management problems as well as the negotiated selection of candidate alternatives to implement. MORDM also allows decision makers to characterize the most important vulnerabilities for their systems, which should be the focus of ex post monitoring and identification of triggers for adaptive management. © 2012 Elsevier Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875181174&partnerID=40&md5=dc6e572c964971f26a1086d72d9ada59","Complexity of selecting a design at the conceptual design (CD) stage in the product development can be increased by imprecise and uncertain product requirements. For many companies, mitigating the decision complexity at the CD stage can result in succeeding in the competitive market. Therefore, having a robust decision-making tool embedded with conflict resolution to valuing potential new product investments to justify their development strategy becomes important. However, limitations, such as uncertainty and inability to provide the information on the compatibility between design concepts, remain as challenges for the concept selection method. Therefore, this study reports a multi-layer graph model that not only resolves the conflict of experts' opinion but also aggregates the layers corresponding to decision criteria into a single graph. This graph is used to evaluate the CD alternatives to find the best alternative. An illustrative example is demonstrated to present the application of the model. © 2013 Copyright Taylor and Francis Group, LLC."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875605093&partnerID=40&md5=f705d2d3f32f470450ef572561bffc2f","The production, use and trade of charcoal for domestic cooking and heating are characterized by contradictions, stereotyping, and misconceptions. Partial information, over-generalizations, and the tendency to consolidate charcoal with other biomass fuels have contributed to gross misrepresentation of charcoal in terms of its actual impact on forests, its role in improving energy access, and in appropriate interventions. An underlying and often amplifying challenge that results from this situation is the lack of reliable, consistent, and comparable data on the charcoal sector which would form a necessary baseline for robust decision making. Further, clarifying misconceptions and debunking of myths is paramount for demonstrating the contribution that charcoal could have in addressing energy access and economic challenges in developing countries. We present five commonly held myths about charcoal that are perpetuated by different stakeholders and actors in the sector. Namely, that: 1) Charcoal is an energy source for the poor; 2) charcoal use is decreasing; 3) charcoal causes deforestation; 4) the charcoal sector is economically irrelevant, and; 5) improved charcoal cook stoves reduce deforestation and GHG emissions. Using a review of the literature and our own experience with charcoal research and practice, we propose reasons for the existence of these myths, why they are highly disputable, and the consequences that the myths have had on policy and intervention responses to charcoal. Widespread beliefs of these myths have and continue to misguide policy response and intervention approaches relating to charcoal. We propose some policy and research recommendations to curb further perpetuation of misconceptions that have been particularly harmful for charcoal. © 2013 Elsevier Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875380537&partnerID=40&md5=61bda09d497e4654a0d5c3cbb90f4327","Ecological compensation is an example of a trade-off whereby loss of natural values is remedied or offset by a corresponding compensatory action on the same site or elsewhere, determined through the process of Environmental Impact Assessment (EIA). Ecological compensation actions are often criticised for having low levels of compliance: meaning that they are achieved only partially or not at all, while development activity proceeds with much greater certainty. Our research investigated compliance with 245 conditions relating to ecological compensation across 81 case studies across New Zealand under the Resource Management Act 1991. Our results show that present tools and practice in New Zealand are not adequately securing the necessary benefits from ecological compensation requirements, with 35.2% of requirements not being achieved. Significant variation in non-compliance with ecological compensation occurs between different activities, applicant types and condition types, while critical variables within the planning process influence levels of compliance. Our research demonstrates the importance of understanding the nature of non-compliance and of providing a consistent and robust decision-making framework for the consideration of ecological compensation in practice. © 2013 Copyright IAIA."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873988228&partnerID=40&md5=3945d70ddfaf3e069fd406b3596280fd","Exploratory Modeling and Analysis (EMA) is an approach that uses computational experiments to analyze complex and uncertain issues. It has been developed mainly for model-based decision support. This paper investigates the extent to which EMA is a promising approach for future oriented technology analysis (FTA). We report on three applications of EMA, using different modeling approaches, in three different technical domains. In the first case, EMA is combined with System Dynamics (SD) to study plausible dynamics for mineral and metal scarcity. The main purpose of this combination of EMA and SD is to gain insight into what kinds of surprising dynamics can occur given a variety of uncertainties and a basic understanding of the system. In the second case, EMA is combined with a hybrid model for airport performance calculations to develop an adaptive strategic plan. This case shows how one can iteratively improve a strategic plan through the identification of plausible external conditions that would cause the plan to perform poorly. In the final case, EMA is combined with an agent-based model to study transition dynamics in the electricity sector and identify crucial factors that positively and negatively affect a transition towards more sustainable functioning of the electricity sector. This paper concludes that EMA is useful for generating foresights and studying systemic and structural transformations despite the presence of a plethora of uncertainties, and for designing robust policies and plans, which are key activities of FTA. © 2012 Elsevier Inc."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873998847&partnerID=40&md5=2df81ded36d1fe1d48992539e48d21ca","Developing strategies, or policies, that automatically adapt to changing conditions is called adaptive decision-making, respectively adaptive policy-making. In this paper, we propose an iterative computational model-based approach to support adaptive decision-making under deep uncertainty. This approach combines an adaptive policy-making framework with a computational approach to generate and explore thousands of plausible scenarios using simulation models, data mining techniques, and robust optimization. The proposed approach, which is very useful for Future-Oriented Technology Analysis (FTA) studies, is illustrated on a policy-making case related to energy transitions. This case demonstrates how the performance of a policy can be improved iteratively by exploring its performance across thousands of plausible scenarios, identifying problematic subsets that require improvement, identifying adaptive high leverage actions with which the adaptive policy needs to be extended until a satisfying dynamic adaptive policy is found for the entire ensemble of plausible scenarios. The approach is not only appropriate for energy transitions; it is also appropriate for any long-term structural and systematic transformation characterized by dynamic complexity and deep uncertainty. © 2012 Elsevier Inc."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886656496&partnerID=40&md5=8c7daa894b1a4b249a28e17aa484498e","This chapter investigates how to provide decision support in complex and uncertain situations. As a stepping stone towards the identification of the key challenges that typically need to be solved, the chapter describes three examples. Subsequently, it generalizes their most important features to specify the challenges. Thereafter, the chapter provides an overview of how the authors' approach addresses each of them. To tackle the challenges, this approach is based on the combination of techniques from decision analysis for making trade-offs and scenarios for 'what-if thinking'. First, these two components, multi-criteria decision analysis (MCDA) and scenario-based reasoning (SBR), are described. Subsequently, their integration is detailed referring to the authors' demonstration system for decision-making in emergency management. While comparisons with related work have been made throughout the chapter, a section summarizes how the authors' approach matches with the characteristics, notions of robust decision-making, and key challenges described. © 2013 John Wiley & Sons, Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949422615&partnerID=40&md5=9b1d1ae1c16cab042059e3d38d3b75ff","Decision-making in emergency management is a challenging task as the consequences of decisions are considerable, the threatened systems are complex and information is often uncertain. This paper presents a distributed system facilitating better-informed decision-making in strategic emergency management. The construction of scenarios provides a rationale for collecting, organising, and processing information. The set of scenarios captures the uncertainty of the situation and its developments. The relevance of scenarios is ensured by gearing the scenario construction to assessing alternatives, thus avoiding time-consuming processing of irrelevant information. The scenarios are constructed in a distributed setting allowing for a flexible adaptation of reasoning (principles and processes) to both the problem at hand and the information available. This approach ensures that each decision can be founded on a coherent set of scenarios. The theoretical framework is demonstrated in a distributed decision support system by orchestrating experts into workflows tailored to each specific decision. © 2013, IGI Global."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872338625&partnerID=40&md5=2b01d42e35778b79f94bc219c194623f","Purpose - The purpose of this paper is to propose a methodology that may assist quality professionals in assessing process variation with a combination of tools based on simple robust statistics. The technique targets alternative way of screening and detection of common and special causes in individuals' control charts (ICC). Design/methodology/approach - The technique is using the classical box plot to detect and filter out outliers attributed to special causes. Then, the runs test is used to partition the data streak at points where the p-value exceeds an assigned critical value. The transition between partitions is where the onset of a common cause is suspected. Findings - The approach presented is supplemented with a case study from foundry operations in large-scale can-making operations. It is demonstrated how the magnesium content of an aluminium alloy is trimmed against special causes and then the location of the common causes is identified in a step-by-step fashion. Research limitations/implications - The proposed method is useful when the collected data do not seem to comply with a known reference distribution. Since it is rare that an initial monitoring of a process will abide to normality, this technique saves time in recycling control charting which point often to misleading assignable causes. This is because the outliers are identified through the box plot one and out. Practical implications - The technique identifies and eliminates quickly the off-shoot values that tend to cause major instability in a process. Moreover, the onset for non-assignable data points is detected in an expedient fashion without the need to remodel each time the inspected data series or to test against a score of multifarious test rules. The ingredients of the method have been well researched in the past, therefore, they may be implemented immediately without a further need to prove their worth. Originality/value - The method mixes up two distinctive robust tools in a unique manner to aid quality monitoring to become fortified against data model inconsistencies. The technique is suitable for controlling processes that generate numerical readings. As such, the approach is projected to be useful for industrial as well as service operations. © Emerald Group Publishing Limited."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871811489&partnerID=40&md5=097cb630baa5e229806ae069e07b863b","In a prior study, the authors showed that decomposing holistic, blackbox building energy models into discrete components can increase the computational efficiency of large-scale retrofit analysis. This paper presents an extension of that methodology to include an economic cost-benefit model. The entire framework now comprises an integrated modelling procedure for the simulation and optimisation of retrofit decisions for individual buildings. Potential decisions can range from the installation of demand-side measures to the replacement of energy supply systems and combinations therewithin. The classical decision theories of Wald, Savage, and Hurwicz are used to perform non-probabilistic optimisation under both technical and economic uncertainty. Such techniques, though simple in their handling of uncertainty, may elucidate robust decisions when the use of more intensive, probabilistic assessments of uncertainty is either infeasible or impractical. © 2012 Elsevier B.V. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879306495&partnerID=40&md5=2676a1be4f1f6954dd48cf525e936b73","There is increasing interest in long-term plans that can adapt to changing situations under conditions of deep uncertainty. We argue that a sustainable plan should not only achieve economic, environmental, and social objectives, but should be robust and able to be adapted over time to (unforeseen) future conditions. Large numbers of papers dealing with robustness and adaptive plans have begun to appear, but the literature is fragmented. The papers appear in disparate journals, and deal with a wide variety of policy domains. This paper (1) describes and compares a family of related conceptual approaches to designing a sustainable plan, and (2) describes several computational tools supporting these approaches. The conceptual approaches all have their roots in an approach to longterm planning called Assumption-Based Planning. Guiding principles for the design of a sustainable adaptive plan are: explore a wide variety of relevant uncertainties, connect short-term targets to long-term goals over time, commit to short-term actions while keeping options open, and continuously monitor the world and take actions if necessary. A key computational tool across the conceptual approaches is a fast, simple (policy analysis) model that is used to make large numbers of runs, in order to explore the full range of uncertainties and to identify situations in which the plan would fail. © 2013 by the authors."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873746718&partnerID=40&md5=ec4debcc836ddc6f0e9efe12fc2de0bf","Planning appropriate portfolios of new water supplies and demand management measures requires considering a wide array of options and their interactions over a largely unknown future. Various modelling-assisted approaches are available to help this planning process. This paper applies two such frameworks to the UK's Thames water resource system and compares their methods and outputs: how they consider uncertainty, how they represent supply and demand management options, and what plans each recommends. The first method is the current England and Wales industry standard: annual least-cost capacity expansion optimisation over a 25 to 30 year time horizon considering capital, operating (fixed and variable), social and environmental costs. The second approach uses stochastic simulation and regret analysis to select a preferred alternative, then statistical cluster analysis to identify causes of system failure enabling further plan improvement. When applied iteratively with system planners this second approach is referred to as Robust Decision Making (RDM). The economic optimisation approach considers all plausible combinations of supply and conservation schemes and recommends the least-cost schedule of their implementation. Our RDM application considers a smaller number of options but makes a more detailed assessment of the effect of uncertainty (supply, demand and energy price uncertainty were considered) on multiple criteria of system performance. The simulation-based approach also enables more realistic interaction amongst supply and demand management schemes. Both approaches recommended different plans which we explain by discussing the benefits and limitations of each. Joint application is recommended to produce least-cost plans that are robust considering multiple criteria of performance across a wide range of futures. © 2012 Springer Science+Business Media B.V."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897402177&partnerID=40&md5=86a468d9663fc594c9f725f139a9aff2","CREAT has a flexible and customizable risk assessment framework that organizes available climate data and guides users through a process of identifying threats, vulnerable assets, and adaptation options to help reduce risk. CREAT supports utilities in considering impacts at multiple locations, assessing multiple climate scenarios, and documenting the implications of adaptation on energy use. To support more robust decision-making, CREAT encourages users to compare the performance of adaptation options in multiple time periods across climate scenarios. In CREAT 2.0, water utility owners and operators use information about their utility to identify climate change threats, assess potential consequences, and evaluate adaptation options. This approach allows utilities to assess impacts based on established thresholds when utility operations are disrupted and assets are impacted."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903650359&partnerID=40&md5=8b067e05dd6f3c388ccf5384fcfae79b","In this paper we propose three ICTMC (Interval Continous-Time Markov Chain) algorithms to improve simulation when significant variabilities exist. The ICTMC models takes into account the effects of variabilities in exponential transition rates represented by intervals. A case study is presented doing a comparision between interval steady state probabilities obtained from interval linear systems of equations solution and from ICTMC simulation. ICTMC simulation incorporates variabilities and uncertainties based on imprecise probabilities, where the statistical distribution parameters in the simulation are intervals instead of precise real numbers. Interval arithmetic is used to simulate a set of scenarios simultaneously in each simulation run. This simulation procedure can be applied to support robust decision making. © 2013 IEEE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870687701&partnerID=40&md5=af55134db8b24474cd420af9a5b49808","In this paper, we examine the inflation persistence puzzle by applying the robust control approach of Hansen and Sargent (2008). In line with the literature suggesting that inflation persistence may be affected by the monetary policy design and its institutional characteristics, we find that inflation persistence is positively related to the central bank's preference for model robustness. In effect, model uncertainty and robust decision making may be considered as a mechanism generating inflation persistence, for a given non-zero degree of autocorrelation in supply-side shocks. Further, the policy implication is that the central bank's monetary policy under model uncertainty renders, in terms of the sacrifice ratio, the output-cost of inflation stabilization more important. © 2012 Elsevier B.V."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905653115&partnerID=40&md5=09e1a57cea8c938096c454f1445a150b","We designed two decision support tools and employed them during a one-week, simulation-driven experiment that included emergency responders acting in their real-life roles. Each tool visualized a ""decision space"": A diagrammatic depiction of the relative desirability of one option versus another, including the inherent uncertainty in the potential outcomes. One requirement was to develop a tool accounting for the impacts of decisions on others, so that emergency responders can make ""sympathetic decisions."" For example, one decision space enabled responders to request resources from surrounding jurisdictions while also considering the potential negative effects on the lending organizations. Another decision space enabled responders to engage in a strategic dialogue with the public: ""listening"" to the public's greatest concerns by mining social media to measure emotion, and thereby suggesting strategic communications addressing those concerns. We report how we designed the decision spaces and the qualitative results of using these spaces during the experiment."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929991772&partnerID=40&md5=b0f81a286ab9a157c72fcd91858bd839","The current fierce competition within the manufacturing industry throughout the world is a result of globalization and dynamic changes in the market. This new era within the production world requires shorter lead times, integrated logistics, capabilities regarding handling changes in product volumes and variety, as well as conformity to environmental rules and regulations legislated by governments and organizations. Apparently, the multitude of variables needed to solve problems complicates the decision-making process. In order to provide solutions for complex problem solving within production development which include many variables and a certain amount of uncertainty there is a need for robust decision making tools. Discrete-event simulation (DES) is one of the virtual tools that can be used as a support for decision-making for production related problems. The current paper addresses challenges which cause low utilization of DES in industry along with a framework for handling those challenges and performing DES projects in an effective and efficient way. © Springer International Publishing Switzerland 2013."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878337624&partnerID=40&md5=d754dcd1115bbf26d3d671c275b760bd","This paper presents the application of information gap decision theory (IGDT) to help the distribution network operators (DNOs) in choosing the supplying resources for meeting the demand of their customers. The three main energy resources are pool market, distributed generations (DGs), and the bilateral contracts. In deregulated environment, the DNO is faced with many uncertainties associated to the mentioned resources which may not have enough information about their nature and behaviors. In such cases, the classical methods like probabilistic methods or fuzzy methods are not applicable for uncertainty modeling because they need some information about the uncertainty behaviors like probability distribution function (PDF) or their membership functions. In this paper, a decision making framework is proposed based on IGDT model to solve this problem. The uncertain parameters considered here, are as follows: price of electricity in pool market and demand of each bus. The robust strategy of DNO is determined to hedge him against the risk of increasing the total cost beyond what it is willing to pay. The effectiveness of the proposed tool is assessed and demonstrated by applying it on a large distribution network. © 2010-2012 IEEE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897017328&partnerID=40&md5=60f947814b218e3b97911c194886f7b4","Electricity networks need robust decision making mechanisms that enable the system to respond swiftly and effectively to any type of disruption or anomaly in order to ensure reliable electricity flow. Electricity load dispatch is concerned with the production of reliable electricity at the lowest costs, both monetary and environmental, within the limitations of the considered network. In this study, we propose a novel DDDAMS-based economic load dispatching framework for the efficient and reliable real-time dispatching of electricity under uncertainty. The proposed framework includes 1) a database fed from electrical and environmental sensors of a power grid, 2) an algorithm for online state estimation of the considered electrical network using particle filtering, 3) an algorithm for effective culling and fidelity selection in simulation considering the trade-off between computational requirements, and the environmental and economic costs attained by the dispatch, and 4) data driven simulation for mimicking the system response and generating a dispatch configuration which minimizes the total operational and environmental costs of the system, without posing security risks to the energy network. Components of the proposed framework are first validated separately through synthetic experimentation, and then the entirety of the proposed approach is successfully demonstrated for different scenarios in a modified version of the IEEE-30 bus test system where sources of distributed generation have been added. The experiments reveal that the proposed work premises significant improvement in the functional performance of the electricity networks while reducing the cost of dynamic computations. © 2013 The Authors. Published by Elsevier B.V."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880843866&partnerID=40&md5=9ba58fe85cb00d56dde47f6b37214419","There has been some tendency to view decision science and resilience theory as opposing approaches, or at least as contending perspectives, for natural resource management. Resilience proponents have been especially critical of optimization in decision science, at least for those cases where it is focused on the aggressive pursuit of efficiency. In general, optimization of resource systems is held to reduce spatial, temporal, or organizational heterogeneity that would otherwise limit efficiency, leading to homogenization of a system and making it less able to cope with unexpected changes or disturbances. For their part, decision analysts have been critical of resilience proponents for not providing much practical advice to decision makers. We believe a key source of tension between resilience thinking and application of decision science is the pursuit of efficiency in the latter (i.e., choosing the ""best"" management action or strategy option to maximize productivity of one or few resource components), vs. a desire in the former to keep options open (i.e., maintaining and enhancing diversity). It seems obvious, however, that with managed natural systems, there must be a principle by which to guide decision making, which at a minimum allows for a comparison of projected outcomes associated with decision alternatives. This is true even if the primary concern of decision making is the preservation of system resilience. We describe how a careful framing of conservation problems, especially in terms of management objectives and predictive models, can help reduce the purported tension between resilience thinking and decision analysis. In particular, objective setting in conservation problems needs to be more attuned to the dynamics of ecological systems and to the possibility of deep uncertainties that underlie the risk of unintended, if not irreversible, outcomes. Resilience thinking also leads to the suggestion that model development should focus more on process rather than pattern, on multiple scales of influence, and on phenomena that can create alternative stability regimes. Although we acknowledge the inherent difficulties in modeling ecological processes, we stress that formulation of useful models need not depend on a thorough mechanistic understanding or precise parameterization, assuming that uncertainty is acknowledged and treated in a systematic manner. © 2013 by the author(s)."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924297694&partnerID=40&md5=0b8144947d7f6caf5f3b23f0aa0499ed","Research on quality issues of business process models has recently begun to explore the process of creating process models. With growing complexity, the creation of business process models requires the presence of several, potentially spatially distributed stakeholders. As a consequence, the question arises how this affects the process of process modeling. For this purpose, we utilized a collaborative modeling environment based on Cheetah Experimental Platform for analyzing the collaborative process of process modeling. In this work, we present hypotheses describing observations of the collaborative process of process modeling obtained from exploratory modeling sessions. These hypotheses will be tested in future work."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874650576&partnerID=40&md5=41c185dfecf4cbd2d1ac60e5e3e58125","In this paper, a system of systems (SoS) framework for the reliability analysis of telecommunication networks is proposed. In this framework, two hazard analysis techniques, hazard and operability analysis and fault tree analysis, are combined in a hybrid scheme. This is further enhanced using the Bayesian network model along with sensitivity analysis in order to answer complex probability queries and to estimate the impact of residual mishap risks, unknown events, or events that cannot easily be modeled. The SoS emergent behavior is further revealed using exploratory modeling. The proposed SoS framework is applied in the case of a fiber-to-the-curb VDSL telecommunication network. © 2007-2012 IEEE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905643480&partnerID=40&md5=7d1594da4b53642f3ed8f87295cc6871","Decision making in extreme events presents a difficult challenge to emergency managers who are legally responsible for protecting life, property, and maintaining continuity of operations for their respective organizations or communities. Prior research has identified the benefits of gaining situation awareness in rapidly changing disaster contexts, but situation awareness is not always sufficient. We have investigated ""option awareness"" and the decision space to provide cognitive support for emergency managers to simulate computationally possible outcomes of different options before they make a decision. Employing a user-centered design process, we developed a computational model that rapidly generates ranges of likely outcomes for different options and displays them visually through a prototype decision-space interface that allows rapid comparison of the options. Feedback from emergency managers suggests that decision spaces may enable emergency managers to consider a wider range of options for decisions and may facilitate more targeted, effective decision making under uncertain conditions."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900301723&partnerID=40&md5=98f499ddf4b84645800afa4618df11b1","Many key societal problems share a set of common features-multiple interacting temporal and spatial scales, multiple valid perspectives, changing requirements and unquantifiable uncertainties. These characteristics substantially stress our cognitive and computational resources, limiting our capacity to effectively address these problems. We introduce an approach for dealing with the inherent complexity of such problems. At the core of this approach is the notion of a multi-model ecology-an interacting and constantly evolving system of models, datasets, interfaces and humans tasked with enhancing the ability of decision makers to effectively address a complex policy problem. The multi-model ecology approach entails the systematic fragmentation and gradual reconstitution of a problem's multiple components and dimensions in an evolving participatory context. We describe an implementation of this approach currently in progress-focused on electricity infrastructure vulnerability to climate change-and identify several key areas of research for developing this approach further. © ECMS Webjørn Rekdalsbakken."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897562170&partnerID=40&md5=c39f5023b77aae012841ed3430131bef","This paper primarly focuses on the proper description of single- and multi-component hydrocarbon transport in nanoporous organic-rich shales and therefore presents the relevant issues and exploratory modeling studies to account for the effects necessary for accurate reservoir simulation. The deviation from the Darcian flow behavior of the gas/condensate transport through shale reservoirs is shown to occur mainly by alteration of fluid properties because of pore proximity effects at elevated reservoir pressure and temperature conditions although the Knudsen and Klinkenberg type corrections may involve at low pressures at later stages of hydraulically-fractured wells in shale reservoirs. A systematic methodology is presented for modification of the fluid and phase behavior relevant to transport at elevated pressure and temperature conditions. The modifications of the phase diagram, density, viscosity, and surface tension for typical hydrocarbon components and mixtures in various size pores reduced further by adsorbed components layers are compared with those of the bulk systems. Further, the role of organic connectivity in shales characterized by organic and inorganic nanopores on production from shale wells is investigated. The role and performance of isolated organics in the form of a 'raisin-bread' model in comparion with organics that are organized in slabs with relatively longer correlation lengths are demosntrated by simulation. The methodology for haldling of the organic connectivity and the pore-proximity effects on fluid properties provided here may be instrumental for accurate shale gas and liquids-rich shale reservoir simulation. Copyright 2013, Society of Petroleum Engineers."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870536622&partnerID=40&md5=5b8cf823fd71aa63586cf9367ada23a7","This study evaluates the influence of plausible changes in East Antarctic Ice sheet (EAIS) thickness and the subsequent glacio-isostatic response as a contributor to the Antarctic warming indicated by ice core records during the Last Interglacial period (LIG). These higher temperatures have been estimated primarily using the difference in the δD peak (on average. ~. 15‰) in these LIG records relative to records for the Present Interglacial (PIG). Using a preliminary exploratory modelling study, it is shown that introducing a relatively moderate reduction in the amount of thickening of the EAIS over the LIG period introduces a significant increase (up to 8‰) in the predicted elevation-driven only δD signal at the central Antarctic Ice sheet (AIS) ice core sites compared to the PIG. A sensitivity test in response to a large prescribed retreat of marine-based ice in the Wilkes and Aurora subglacial basins (equivalent to ~. 7. m of global mean sea-level rise) results in a distinct elevation signal that is resolvable within the ice core stable isotope records at three sites (Taylor Dome, TALDICE and EPICA Dome C). These findings have two main implications. First, EAIS elevation's only effects could account for a significant fraction of the LIG warming interpreted from ice core records. This result highlights the need for an improved estimate to be made of the uncertainty and size of this elevation-driven δD signal which contributes to this LIG warming and that these effects need to be deconvolved prior to attempting to extract a climatic-only signal from the stable isotope data. Second, a fingerprint of significant retreat of ice in the Wilkes and Aurora basins should be detectable from ice core δD records proximal to these basins and therefore used to constrain their contribution to elevated LIG sea levels, after accounting for ice sheet-climate interactions not considered in our approach. © 2012 Elsevier B.V.."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870232194&partnerID=40&md5=76e793866e8768a075f8a564a897656a","Background: Projections of health risks of climate change are surrounded with uncertainties in knowledge. Understanding of these uncertainties will help the selection of appropriate adaptation policies. Methods: We made an inventory of conceivable health impacts of climate change, explored the type and level of uncertainty for each impact, and discussed its implications for adaptation policy. A questionnaire-based expert elicitation was performed using an ordinal scoring scale. Experts were asked to indicate the level of precision with which health risks can be estimated, given the present state of knowledge. We assessed the individual scores, the expertise-weighted descriptive statistics, and the argumentation given for each score. Suggestions were made for how dealing with uncertainties could be taken into account in climate change adaptation policy strategies. Results: The results showed that the direction of change could be indicated for most anticipated health effects. For several potential effects, too little knowledge exists to indicate whether any impact will occur, or whether the impact will be positive or negative. For several effects, rough 'order-of-magnitude' estimates were considered possible. Factors limiting health impact quantification include: lack of data, multi-causality, unknown impacts considering a high-quality health system, complex cause-effect relations leading to multi-directional impacts, possible changes of present-day response-relations, and difficulties in predicting local climate impacts. Participants considered heat-related mortality and non-endemic vector-borne diseases particularly relevant for climate change adaptation. Conclusions: For possible climate related health impacts characterised by ignorance, adaptation policies that focus on enhancing the health system's and society's capability of dealing with possible future changes, uncertainties and surprises (e.g. through resilience, flexibility, and adaptive capacity) are most appropriate. For climate related health effects for which rough risk estimates are available, 'robust decision-making' is recommended. For health effects with limited societal and policy relevance, we recommend focusing on no-regret measures. For highly relevant health effects, precautionary measures can be considered. This study indicated that analysing and characterising uncertainty by means of a typology can be a very useful approach for selection and prioritization of preferred adaptation policies to reduce future climate related health risks. © 2012 Wardekker et al.; licensee BioMed Central Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894190082&partnerID=40&md5=72f632da0f2c38008904ec3962dbc96a","Portfolios of market-based instruments have been shown to improve the reliability of water supplies, using simulations that utilize a single best estimate of distributions of data to evaluate performance. However, the estimates of problem information and likelihoods could be incorrect, especially when planning for climate change, which can modify streamflow availability, or projecting the trajectories of future water demands. These conditions are termed deep uncertainty, in which decision makers cannot fully conceptualize or agree upon the full range of risks to their system. This presentation will advance a new interactive framework that combines robust decision making (RDM) with many-objective optimization using evolutionary algorithms (MOEA) to confront deep uncertainty for water planning. The framework is demonstrated using a case study that examines a single city's water supply in the Lower Rio Grande Valley (LRGV) in Texas, USA. We use a MOEA to develop a tradeoff set of water supply portfolios for the LRGV, and develop a suite of values for key uncertainties using RDM that represent an ensemble of ""states of the world"". Each solution is tested under the ensemble of plausible future states of the world, with interactive visualizations being used to identify robust solutions for the system. Scenario discovery methods that use statistical data mining algorithms are then used to identify what assumptions and system conditions strongly control the cost-effectiveness, efficiency, and reliability of the robust alternatives. The results suggest that combining robust decision making, many-objective optimization, and visual analytics can dramatically improve risk-based planning decisions."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882630365&partnerID=40&md5=cd7b9de6250a100eb3197da9221a074d","Fault diagnosis is crucial for ensuring the safe operation of complex engineering systems and avoiding the execution of an unsafe behavior. This chapter deals with Robust Decision Making(RDM) for fault detection of electromechanical systems by combining the advantages of Bond Graph(BG) modeling and Fuzzy logic reasoning. A fault diagnosis method implemented in two stages is proposed. In the first stage, the residuals are deduced from the BG model allowing the building of a Fault Signature Matrix(FSM) according to the sensitivity of residuals to different parameters. In the second stage, the result of FSM and the robust residual thresholds are used by the fuzzy reasoning mechanism in order to evaluate a degree of detectability for each set of components. Finally, in order to make robust decision according to the detected fault component, an analysis is done between the output variables of the fuzzy system and components having the same signature in the FSM. The performance of the proposed fault diagnosis methodology is demonstrated through experimental data of an omni directional robot. © 2012 Bentham Science Publishers. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893001382&partnerID=40&md5=2af83a237c18eacf66196e60128cf02a","Artificial Intelligence (AI) has developed rapidly since it was founded in the mid 1950's aimed at understanding and designing intelligent agents. Among a number of competitions and prizes to promote research in AI, the RoboCup, an annual international robot soccer competition, is one of the most interesting and well known ones. In the RoboCup Small-Size League (SSL) teams consisting of five cylinder-looking robots autonomously play soccer games. A computerised decision-making module is used as the brain of the team to assess the current situation, to establish an appropriate and near optimal action strategy and to ultimately send commands to the robots. In this paper, an architecture for the design of a robust decision-making module for the RoboCup SSL is presented. This decision-making module improves upon the existing modules by combining the best strategies of existing modules with newly developed ones. In addition, it is robust in terms of its ability to cope with unexpected situations that might occur midgame. Battery depletion or key components breakdown are good examples of such situations. © 2012 CIE & SAIIE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871172675&partnerID=40&md5=a1fe2bcadfb5a85f644ea8266b281124","We develop and apply a judgment-based approach to selecting robust alternatives, which are defined here as reasonably likely to achieve objectives, over a range of uncertainties. The intent is to develop an approach that is more practical in terms of data and analysis requirements than current approaches, informed by the literature and experience with probability elicitation and judgmental forecasting. The context involves decisions about managing forest lands that have been severely affected by mountain pine beetles in British Columbia, a pest infestation that is climate-exacerbated. A forest management decision was developed as the basis for the context, objectives, and alternatives for land management actions, to frame and condition the judgments. A wide range of climate forecasts, taken to represent the 10-90% levels on cumulative distributions for future climate, were developed to condition judgments. An elicitation instrument was developed, tested, and revised to serve as the basis for eliciting probabilistic three-point distributions regarding the performance of selected alternatives, over a set of relevant objectives, in the short and long term. The elicitations were conducted in a workshop comprising 14 regional forest management specialists. We employed the concept of stochastic dominance to help identify robust alternatives. We used extensive sensitivity analysis to explore the patterns in the judgments, and also considered the preferred alternatives for each individual expert. The results show that two alternatives that are more flexible than the current policies are judged more likely to perform better than the current alternatives on average in terms of stochastic dominance. The results suggest judgmental approaches to robust decision making deserve greater attention and testing. © 2012 Society for Risk Analysis."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869090859&partnerID=40&md5=e9069b5639cfd7777416d9295e73a2ca","We consider in this paper the robustness of decisions based on probabilistic thresholds. To this effect, we propose the same-decision probability as a query that can be used as a confidence measure for threshold-based decisions. More specifically, the same-decision probability is the probability that we would have made the same threshold-based decision, had we known the state of some hidden variables pertaining to our decision. We study a number of properties about the same-decision probability. First, we analyze its computational complexity. We then derive a bound on its value, which we can compute using a variable elimination algorithm that we propose. Finally, we consider decisions based on noisy sensors in particular, showing through examples that the same-decision probability can be used to reason about threshold-based decisions in a more refined way. © 2012 Elsevier Inc. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879748281&partnerID=40&md5=2407ff4fa9aaefe8ce491430b5ad1fa5","Rapid advances in information technology and the increased emphasis on joint operations results in highly networked, interoperable Systems-of-Systems (SoS). The units comprising the SoS must act in collaboration with each other to establish and maintain information superiority and battlespace dominance. The problem that arises, however, is that there are few Modeling & Simulation tools that exist for quantifying the benefits of collaboration during a military mission. To address this need, this paper details the application of an Architecture Resource-Based Collaborative Network Evaluation Tool (ARCNET). ARCNET provides an estimation of the impact on mission effectiveness resulting from changes in collaboration between military units. These changes may be due to differences in interoperability, resource exchanges, and force structure. Ultimately, this will lead to more robust decision making during military SoS architecture selection. © 2012 IEEE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864316954&partnerID=40&md5=464990725b5422fe9094213bfbe6c0b2","The authors propose a general modeling framework called the general monotone model (GeMM), which allows one to model psychological phenomena that manifest as nonlinear relations in behavior data without the need for making (overly) precise assumptions about functional form. Using both simulated and real data, the authors illustrate that GeMM performs as well as or better than standard statistical approaches (including ordinary least squares, robust, and Bayesian regression) in terms of power and predictive accuracy when the functional relations are strictly linear but outperforms these approaches under conditions in which the functional relations are monotone but nonlinear. Finally, the authors recast their framework within the context of contemporary models of behavioral decision making, including the lens model and the take-the-best heuristic, and use GeMM to highlight several important issues within the judgment and decision-making literature. © 2012 American Psychological Association."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894132926&partnerID=40&md5=b49c3b1d71d4e3c41dd0119695c781d4","An alternative to the typical application of rational choice models to climate policy is the coupling of agent-based modeling and exploratory approaches. Agent-based models (ABMs) represent the world as made up of heterogeneous, boundedly-rational agents who act in their own interests and yet engage in substantive communication. Rather than focusing on optimal outcomes, agent-based models are primarily concerned with the evolution of large-scale properties that 'emerge' from the lower-level behavior. Consequently, ABMs have the potential to address complex system properties and generate a wider array of plausible storylines than more traditional integrated assessment modeling methodologies. We provide an overview of a new agent-based model of economic growth, energy technology, and climate change, and demonstrate use of the model for scenario discovery. Scenario discovery generates ensembles of plausible futures under alternative assumptions and hypotheses concerning system behavior. Such scenarios can help identify policy vulnerabilities and opportunities, thus supporting the design of robust climate change mitigation strategies."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883535708&partnerID=40&md5=78a208ca76390307e58f1ed3cd6021a0","Space Operations deal with very expensive elements and some of them are completely inaccessible and cannot be repaired in case of error. All decisions are taken very carefully to proceed always within the minimal risk and this requires an expensive constant human control. Flight control Engineers analyze the telemetry received from the spacecraft to detect possible anomalies and when an anomaly is detected, it is also their job to investigate and characterize it. The complexity in this process is that the telemetry may consist of 20 000 different parameters. Dealing with this amount of data and finding the parameters that may be related to the given anomaly is very labour intensive and currently only based on the engineer knowledge and experience. This research will try to find automated tools to help on this process by addressing the following points: Better way to monitor telemetry Anomaly investigation: 1) Find related parameters/spacecraft subsystems or 2) find other occurrences of the same anomaly. An Agent is an Artificial Intelligence entity capable of supporting to accomplish tasks on behalf of another entity, either human or computational. This technology is already used in the industry to improve control systems, reduce costs and increase safety. Multi-Agent technology will be used as a base for the implementation and support of other technologies that share the same goal: help engineers with the monitoring and diagnostics tasks. Copyright © (2012) by the International Astronautical Federation."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873189257&partnerID=40&md5=78a243e932471fa9b14d94483e3036c2","A performance assessment is a projection of what can happen, how likely is it, and the associated consequences for potentially many thousands of years into the future. Performance assessment is widely used in the waste disposal community to provide information to decision makers on projected future risks to humans and the environment. Performance assessments of waste disposal facilities typically incorporate uncertainty and variability in data and parameters. Though performance assessments may be probabilistic or deterministic, modern computational advances have allowed greater use of probabilistic assessments. Performance assessments can be data intensive, and in many cases data can be sparse. Sparse site-specific data typically leads to the use of generic sources of information, whose basis may range from sparse to fairly complete data sets. If caution is not taken, misinterpretation or misrepresentation of the variance in information, particularly when site-specific supporting information is sparse, may result in ineffective decision-making. Therefore, results produced from models with sparse data and limited site-specific information can require careful representation. This paper presents the results of probabilistic performance assessment simulations of a generic waste disposal site (including source term release, unsaturated flow and transport, and saturated zone flow and transport) to illustrate the importance of proper interpretation and representation of data. The results highlight the need for site-specific measurements of key information in order to facilitate robust decision-making."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894155771&partnerID=40&md5=74bcbc493ff46f70b2ecf1c5d1232476","Safe operating space for human activities that will not push the planet out of the 'Holocene state' that has seen human civilizations arise, develop, and thrive can be defined with respect to among others global freshwater use. Establishing such limits is a methodological challenge because they are critically depended on local conditions, the role of management, and financial and institutional capacity in magnifying or ameliorating problems. Moreover estimates of these limits are plagued by uncertainty arising out of conflicting models, regional variations, limitation of expansion of water use through financial and institutional capacity, and uncertainty about the realization and efficiency of trans-boundary water transfers. This paper aims at investigating the limits to global freshwater use through exploratory modelling and analysis. To this end, the behaviour of a dynamic world water model that also included the socio-economic system is explored across a wide variety of uncertainties. The resulting dynamics are analysed and dynamics indicative of water shortage are identified. In order to identify the conditions for occurring of these dynamics, we use the Patient Rule Induction Method (PRIM). PRIM can be used for data analytic questions where the analyst tries to find simultaneous combinations of values for input variables that result in similar characteristic values for the outcome variables. Specifically, one seeks a set of subspaces of the input variable space within which the values of output variables are considerably different from the average value over the entire input domain. In as far as these output sub-spaces are indicative for unsustainable water conditions, the boundaries of their congruent input parameter spaces constitute limits to global fresh water use."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872013858&partnerID=40&md5=d5d6865c59c3e67a35d97b220a8519d1","Demands for thermal comfort, better indoor air quality together with lower environmental impacts have had ascending trends in the last decade. In many circumstances, these demands could not be fully covered through the soft approach of bioclimatic design like optimisation of the building orientation and internal layout. This is mostly because of the dense urban environment and the internal energy loads of the building. In such cases, heating, ventilation, air-conditioning and refrigeration (HVAC&amp;R) systems make a key contribution to fulfilling the requirements of the indoor environment. Therefore, the most appropriate HVAC&amp;R system must be selected. In this study, a robust decision making approach for HVAC&amp;R system selection is proposed. Technical performance, economic and environmental impacts of 36 permutations of primary and secondary systems are taken into account to choose the most appropriate HVAC&amp;R system for a case study office building. The building is a representative of the dominant form of office buildings in the UK. Dynamic performance evaluation of HVAC&amp;R alternatives using the TRNSYS package together with life-cycle energy cost analysis is undertaken to provide a reliable basis for decision making. The six scenarios considered in this paper broadly cover the decision makers' attitudes on HVAC&amp;R system selection. These scenarios are analyzed through an Analytical Hierarchy Process (AHP), and sensitivity analyses which prove the robustness of the final decision for each scenario. One of the significant outcomes of this study reveals that the compound heating, cooling and power system (CCHP), despite the high level of energy demand and also a higher initial investment, is one of the top-ranked alternatives due to the lower energy cost and CO2 emissions. The proposed approach and the results of this study could be used by researchers and designers, especially in the early stages of a design process where all the involved bodies commonly face a lack of time, information and the tools necessary for the evaluation of a variety of HVAC&amp;R systems."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894169127&partnerID=40&md5=dfc02778d14c6c234b1bf54d0596dfec","Most of the models used to analyze international climate policy treat the problem as if it operates on only a single level. This is the result of a reliance on integrated assessment models (IAMs) that, for reasons of analytical tractability, typically employ assumptions that allow the economy to be modeled as if it is managed by a single, utility-maximizing central planner without regard to the influence of either lower-level actors or international pressures. While game theoretic models have been used to study international negotiations, they generally do not consider feedback from domestic actors who have heterogeneous beliefs and vulnerability to climate change. In this paper, we provide an overview of a preliminary policy modeling framework, called ENGAGE, styled after the Putnam two-level game in which interactions among negotiators at the international level are linked with the preferences of constituents at the domestic level. Domestic constituents in our model include firms and households who function as agents within an evolutionary representation of economic growth, energy technology, and climate change, resulting in a two-way dynamic feedback between international agreements and domestic policy outcomes. We present the basic elements of the two levels of the model and discuss plans for future model development."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871986867&partnerID=40&md5=826347e30bcc18911c7155693f31214d","A system-of-systems (SoS) approach for wavelength-division multiplexing (WDM) and time-division multiplexing (TDM) fiber-to-the-home (FTTH) telecommunication networks is presented. Cost evolution curves for individual systems as well for whole FTTH WDM and TDM networks are presented. The analysis can be exploited for a fast and accurate analysis of FTTH deployment costs in dense urban, urban, and suburban areas from the technoeconomic point of view, which is of paramount importance for telecom operators, equipment vendors, regulators, and policy makers. The impact of delaying the deployments or adopting different rollout strategies is also investigated and presented. The SoS emergent behavior is further revealed using exploratory modeling. The results reveal that in all cases, the WDM solution is more expensive than TDM. The total cost for suburban areas is almost six times higher than in dense urban areas and four times than urban areas. © 2012 IEEE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878373656&partnerID=40&md5=31b08f8de67bca0f0939bab39ae2994e","Recent contextual developments constitute a backdrop of change for the Dutch electricity system. Institutional change driven by liberalization, changing economic competitiveness of the dominant fuels, new technologies, and changing end-user preferences regarding electricity supply are some examples of these developments. This paper explores plausible transition trajectories in the face of these developments given technological uncertainty about investment and operating costs, and fuel efficiency of various alternative technologies; political uncertainty about future CO2 abatement policies such as emission trading; and socio-economic uncertainty about fuel prices, investment decisions of suppliers, and load curves. Various alternative developments for these uncertainties are specified. The consequences of each of these alternative developments are assessed using an agent-based model of the Dutch electricity system. The outputs are analyzed using various data-mining and data visualization techniques in order to reveal arch-typical transition trajectories and their conditions for occurring. Policy recommendations are derived from this. The results indicate that most transition trajectories point towards a future energy supply system that is reliant on clean coal and gas. Under the explored uncertainties, only rarely a transition to renewables occurs. The various sustainable energy support programs appear to be ineffective in steering the energy supply system towards a more sustainable mode of functioning across the various uncertainties. © 2012 Springer Science+Business Media New York."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872503353&partnerID=40&md5=a73bc4516ca0e576c6ed4e6200970a2d","Degrading permafrost below roadway embankments is a widespread problem in the north. Thermal modeling can help to determine thermally stable embankment configurations; however, this modeling typically does not include the effects of groundwater flow on the geosystem, which will cause permafrost degradation to occur faster than atmospheric warming alone. As part of a larger, ongoing research project, we present preliminary results of heat transfer modeling for an Alaska Highway test section near Beaver Creek, Yukon Territory, Canada. This experimental highway test section is located in an area characterized by muskeg vegetation underlain by ice-rich permafrost. While the overall project includes field work and laboratory measurements, this paper focuses on the results of exploratory modeling. A two-dimensional finite element program capable of mathematically coupling both heat and groundwater flow was used for modeling. Comparing model results produced using conductive heat flow only to model results using both heat and groundwater flow (heat advection) indicates that groundwater has a significant effect on the configuration of the thaw bulb and the temperature distribution within and below the roadway embankment. For example, using 50-year model results, including groundwater flow increases modeled thaw depths below the embankment by approximately 8 m, and can increase temperatures to an excess of +2.5 C at the bottom of the embankment. For wet terrain conditions, these preliminary modeling results indicate that it is essential to incorporate groundwater flow into thermal modeling, in order to understand better the complex interactions between roadway embankments and underlying permafrost. © 2012 American Society of Civil Engineering."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894168128&partnerID=40&md5=a662ec78f59ec13a8a08f7c80231f9a5","Sustainable water management in a changing environment full of uncertainty is a profound challenge. To deal with uncertainties, dynamic adaptive policies can be used. Such policies can change over time in response to how the future unfolds, to what we learn about the system, to changes in environment, and to changes in societal preferences. This paper presents a model driven approach that supports the development of dynamic adaptive policies, and illustrates the approach using a hypothetical case. The key idea of the approach is that the dynamic behavior of a fast and simple Integrated Assessment Meta Model (IAMM) is explored across a wide variety of uncertainties. Next, the performance of a set of candidate policy actions is assessed across these uncertainties. This provides insight into the sell-by date of the various actions, and thus when to modify or replace the action. From this, we deduce a logical sequencing of actions, constituting potential pathways. The performance of these pathways is in turn assessed, and iteratively improved. The hypothetical case is inspired by a river reach in the Rhine delta of the Netherlands. Like in the real world, this case is characterized by uncertainties about the future (e.g. climate change, socioeconomic developments, and natural variability), uncertainties about the system (e.g. chance on dike failure in relation to high water levels), and uncertainties about societal preferences (e.g. weight society gives to nature). Using a rule induction algorithm, we identify the vulnerabilities and opportunities presented by each pathway. We modify the pathways to address these opportunities and vulnerabilities through capitalizing and defensive actions. With the results it is possible to make an informed decision on a dynamic adaptive policy in a changing environment that is able to achieve the intended objectives despite the multitude of uncertainties present."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892111424&partnerID=40&md5=b88a0d8d5d7edd874c32576c9bd11684","Tissue engineering (TE) is an emerging multidisciplinary field that draws on expertise from medicine, biology, chemistry, genetics, engineering, computer and life science. Its mission is to discover solutions to one of the most challenging medical problems faced by humans: replace tissue and organ functions when damage is beyond natural recovery process. A precondition for successful TE is having an adequate understanding of the principles of tissue genesis. The goal is to apply that knowledge to produce functional tissue replacements suitable for clinical use. Specifically achieve biological-inspired, biocompatible, tissue-mimetic structures that, when implanted in vivo, restore or improve failed or compromised human tissue and/or organ function. Impressive progress in human tissue regeneration followed development and implementation of advanced technologies that enabled better understanding and control signalling within microenvironments during growth and maturation of tissue functionalization. In particular, the latest generation of bioreactors have demonstrably improved in vitro tissue maturation prior to implantation. That achievement was made possible by two engineering advances: i) repeatable and automated bioprocesses, and ii) recapitulation of key physiologic, physicochemical and mechanical cues in vitro. Despite this progress critical, large gaps in our knowledge are slowing progress. For example, how can cell level operating principles and environmental cues be orchestrated in advanced bioreactors to enable the formation of a physiological-like functional tissue? What are those cell level operating principles? Because the tissue is developing ex vivo, will the orchestration need to be different in important ways from that occurring during organogenesis? When detailed information is limited, uncertainties are large, and feasible wet-lab experiments are limited by costs and other factors, in silico exploratory modelling and simulation can be a cost-effective adjunct strategy for answering those and related questions. It can help achieve critically needed mechanistic insight into complexities of suitable cellular responses to the conditioning procedures. Improving and experimenting upon multi-attribute, multiscale computational models is a relatively new, scientific approach for i) evaluating cellular responses to the conditioning procedures within bioreactors, ii) predicting or anticipating the dynamic modification of the cellular behaviours during culture as a function of external stimuli, and iii) discovering relevant features and protocols for optimizing bioreactor working conditions (i.e., the external stimuli for the cells), in terms of interactions between cells and the bioartificial hosting environment. Within this chapter we discuss how coupling i) computational fluid dynamics (CFD) and ii) multi agent systems (MAS) modelling methods is enabling rationale design and subsequent establishment of in silico bioreactors. Problems associated with designing TE experiments can be explored using in silico high-throughput experiments, and plausible solutions can be identified in advance by creating a software framework, which incorporates a variety of phenomena known to influence tissue growth, along with a model of cell population dynamics. Upon maturation, the approach is expected to provide exploitable insight into how tissue generation and maturation emerge and can be controlled. That insight can be leveraged to fine-tune system parameters to achieve desired cellular responses during in vitro conditioning while reducing reliance on costly physical experiments. Computational modelling and simulation in TE is presented as a diverse, active, and powerful trans-disciplinary expansion of scientific and engineering methods for overcoming many of the current limitations in identifying optimal regenerative therapies for diseased and damaged tissues. © 2012 Nova Science Publishers, Inc. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949908616&partnerID=40&md5=bc5289b75ed93c9eee6eb1a6b5d9aa61",[No abstract available]
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872959813&partnerID=40&md5=6d8dc5d9fc2b585cbda8d48912ff8da0","Follow with the methodology of reverse engineering, an exploratory modeling and analysis of security of the typical Kremer-Markowitch multi-party non-repudiation protocol were carried out with a novel method named extended-CSP approach which had been proposed for two-party non-repudiation protocol in preliminary work. With the analysis, the inherent fact that this protocol statisfies not timeliness but non-repudiality and fairness was found successfully. The result of the analysis indicates that, as applying to two-party non-repudiation protocols, the extended-CSP approach can also be a new method for security verification of multi-repudiation protocols."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874535965&partnerID=40&md5=06c00acc4d2ac644832f6560bcb5bdf1","The assessment of a structure's resistance and performance is directly linked with reliability and safety issues, as well as with intervention decisions and their relative cost. This assessment can be enhanced by the use of probabilistic models, invoked by the random nature of load and resistance effects. Moreover, intrinsic properties of such models should relate to the commonly encountered dependencies among the input random variables. With respect to that, a significant advance towards effective modelling in current practice of structural reliability engineering has been achieved by integrating scalar indices, such as correlation coefficients. However, case may be that this approach overlooks, or even induces considerable uncertainties. Therefore, models which acknowledge and estimate this uncertainty type can serve the requirements for high reliability and robust decision-making. In the present study, the uncertainties pertaining to multivariate idiosyncrasy are discussed, and a generalised dependence structure paradigm is suggested in order to explicitly handle such uncertainties. The predictive potential of resulting models is demonstrated on an application on fracture mechanics, while the influence on contemporary assessment procedures is discussed. The corresponding numerical techniques can facilitate the inclusion of advanced correlation modes in the simulation methods of structural performance."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867061168&partnerID=40&md5=5a3edf73d5ebd3e8765d2aa42391a00a","Fault diagnosis is crucial for ensuring the safe operation of complex engineering systems and avoiding to execute an unsafe behaviour. This paper deals with robust decision making (RDM) for fault detection of an electromechanical system by combining the advantages of Bond Graph (BG) modelling and Fuzzy logic reasoning. The proposed fault diagnosis method is implemented in two stages. In the first stage, the residuals are deduced from the BG model allowing to build a Fault Signature Matrix (FSM) according to the sensitivity of residuals to different parameters. In the second stage, the result of FSM and the robust residual thresholds are used by the fuzzy reasoning mechanism in order to evaluate a degree of detectability for each set of components. Finally, in order to make robust decision according to the detected fault component, an analysis is done between the output variables of the fuzzy system and components having the same signature in the FSM. The performance of the proposed fault diagnosis methodology is demonstrated through experimental data of an omnidirectional robot. © 2012 IFAC."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866991683&partnerID=40&md5=6b6ac75a3ecacd0c0c28c39144a01979","This study compares two widely used approaches for robustness analysis of decision problems: the info-gap method originally developed by Ben-Haim and the robust decision making (RDM) approach originally developed by Lempert, Popper, and Bankes. The study uses each approach to evaluate alternative paths for climate-altering greenhouse gas emissions given the potential for nonlinear threshold responses in the climate system, significant uncertainty about such a threshold response and a variety of other key parameters, as well as the ability to learn about any threshold responses over time. Info-gap and RDM share many similarities. Both represent uncertainty as sets of multiple plausible futures, and both seek to identify robust strategies whose performance is insensitive to uncertainties. Yet they also exhibit important differences, as they arrange their analyses in different orders, treat losses and gains in different ways, and take different approaches to imprecise probabilistic information. The study finds that the two approaches reach similar but not identical policy recommendations and that their differing attributes raise important questions about their appropriate roles in decision support applications. The comparison not only improves understanding of these specific methods, it also suggests some broader insights into robustness approaches and a framework for comparing them. © 2012 RAND Corporation."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867754938&partnerID=40&md5=a1fc9af1556810011fb2e72129936551","Field experiences on corporate foresight show predominantly a learning process to open up the mind-set for fresh thinking, visionary strategizing and robust decision-making. It combines techniques of futures research with the facilitation of group processes to assemble relations, to reframe a situation, to induce action. The combination of different approaches a supporting system for implementation and the continuous readjustment of the outcomes establish individual and team learning as well as organizational development. This article presents practical cases and how in these cases, the trajectories were changed actively. Some cases are technological, others are more structural. Copyright © 2012 Inderscience Enterprises Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865832587&partnerID=40&md5=b96443fffbe0e78f507ff686e95305e8","Climate models project large changes in rainfall, but disagree on their magnitude and sign. The consequences of this uncertainty on optimal dam dimensioning is assessed for a small mountainous catchment in Greece. Optimal dam design is estimated using a Cost-Benefit Analysis (CBA) based on trends in seasonal temperature and precipitations from 19 IPCC-AR4 climate models driven by the the SRES A2 emission scenario. Optimal reservoir volumes are modified by climate change, leading to up to 34% differences between optimal volumes. Contrary to widely-used target-based approaches, the CBA suggests that reduced rainfall should lead to smaller water reservoirs. The resulting change in the Net Present Value (NPV) of water supply is also substantial, ranging from no change to a large 25% loss, depending on the climate model, even assuming optimal adaptation and perfect foresight. In addition, climate change uncertainty can lead to design errors, with a cost ranging from 0. 3 to 2. 8% of the NPV, depending on site characteristics. This paper proposes to complement the CBA with a robust decision-making approach that focuses on reducing design-error costs. It also suggests that climate change impacts in the water sector may reveal large, that water reservoirs do not always provide a cost-efficient adaptation strategy, and that alternative adaptation strategies based on water conservation and non-conventional water production need to be considered. © 2012 Springer Science+Business Media B.V."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866990641&partnerID=40&md5=d4bc572dffd1d2e69303b06dde30a6c4","How can risk analysts help to improve policy and decision making when the correct probabilistic relation between alternative acts and their probable consequences is unknown? This practical challenge of risk management with model uncertainty arises in problems from preparing for climate change to managing emerging diseases to operating complex and hazardous facilities safely. We review constructive methods for robust and adaptive risk analysis under deep uncertainty. These methods are not yet as familiar to many risk analysts as older statistical and model-based methods, such as the paradigm of identifying a single ""best-fitting"" model and performing sensitivity analyses for its conclusions. They provide genuine breakthroughs for improving predictions and decisions when the correct model is highly uncertain. We demonstrate their potential by summarizing a variety of practical risk management applications. © 2012 Society for Risk Analysis."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867021563&partnerID=40&md5=ee5278d00bcd6196eb22a5493a6e4673","Risk analysis is challenged in three ways by uncertainty. Our understanding of the world and its uncertainties is evolving; indeterminism is an inherent part of the open universe in which we live; and learning from experience involves untestable assumptions. We discuss several concepts of robustness as tools for responding to these epistemological challenges. The use of models is justified, even though they are known to err. A concept of robustness is illustrated in choosing between a conventional technology and an innovative, promising, but more uncertain technology. We explain that nonprobabilistic robust decisions are sometimes good probabilistic bets. Info-gap and worst-case concepts of robustness are compared. Finally, we examine the exploitation of favorable but uncertain opportunities and its relation to robust decision making. © 2012 Society for Risk Analysis."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867297408&partnerID=40&md5=d3f22d37053782ef3a2f1c67552f57b8","Recent interest in the economics of biodiversity and wider ecosystem services has been given empirical expression through a focus upon economic valuation. This emphasis has been prompted by a growing recognition that the benefits and opportunity costs associated with such services are frequently given cursory consideration in policy analyses or even completely ignored. The valuation of biodiversity and ecosystem services is therefore increasingly seen as a crucial element of robust decision making and this has been reflected in a growing body of related research. We provide a critical review of some of this research, considering the valuation methods applied to date and focussing upon their limitations in respect to certain categories of ecosystem services (particularly cultural services) and the applicability of the extant literature to new settings. Substantial questions also remain at the interface of natural science and economics and we consider the potential contribution of the conceptualization of ecosystems as assets as a response to this challenge. As part of this review we also highlight the role which large scale 'ecosystem assessments' have played as an impetus to extending the valuation evidence base and the way in which frameworks and assessments of how ecosystems contribute to human wellbeing might be translated into policy thinking and decision analyses. © The Authors 2012. Published by Oxford University Press."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865793634&partnerID=40&md5=d26d9d50e24abe7f26e2380915bde19e","Life cycle costing (LCC) is a method of analysis deployed by facility owners tasked with maintaining large infrastructure systems such as highways, hospitals, and subway systems; however its application to the design development process is fraught with a myriad of assumptions. Assumptions affecting the cost of future repairs can greatly impact the initial design decisions and enhanced methods are needed to evaluate future cash flows, judge the accuracy of future cost projections, and ascertain the schedule (When) these repairs may be needed in the future. By expanding the LCC evaluation process to include risk analysis, future costs can be analyzed using standard present worth analysis, and probabilistic ranges over the facility life. More robust decision making will enhance comparisons between design alternatives with improved accuracy in calculating the total cost of ownership and improved projections for the scheduling of the required maintenance events. Armed with improved information concerning life cycle cost, owners can balance initial and future cost to optimize designs, improve the value of their programs, and support their design decisions with defensible cost analysis leading to a higher probability of net value improvement and ultimate value alternative implementation."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866376657&partnerID=40&md5=49566defbdbd56221af2f0765f535abf","In this paper, we formulate a single-period portfolio choice problem with parameter uncertainty in the framework of relative regret. Relative regret evaluates a portfolio by comparing its return to a family of benchmarks, where the benchmarks are the wealths of fictitious investors who invest optimally given knowledge of the model parameters, and is a natural objective when there is concern about parameter uncertainty or model ambiguity. The optimal relative regret portfolio is the one that performs well in relation to all the benchmarks over the family of possible parameter values. We analyze this problem using convex duality and show that it is equivalent to a Bayesian problem, where the Lagrange multipliers play the role of the prior distribution, and the learning model involves Bayesian updating of these Lagrange multipliers/prior. This Bayesian problem is unusual in that the prior distribution is endogenously chosen by solving the dual optimization problem for the Lagrange multipliers, and the objective function involves the family of benchmarks from the relative regret problem. These results show that regret is a natural means by which robust decision making and learning can be combined. © 2012 INFORMS."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865137458&partnerID=40&md5=8cd6469fbe9e99f20f34cb9321a0fd7b","Coal is an abundant energy resource, consumed in the United States chiefly by the power generation sector. Due to potential energy security benefits, it has also been considered as an alternate source for gasoline and diesel production. Life cycle assessment (LCA) studies have previously estimated the greenhouse gas emissions associated with coal combustion as well as upstream activities such as mining and transport, to compare its environmental impact with other fuels. Until recent years, LCA studies predominantly ignored the uncertainty and variability inherent in life cycle assessment. More recent work has estimated the uncertainty in the life cycle inventories of fossil fuels, but the use of these uncertainty ranges to model system-wide impacts has been limited. As shown by previous studies, uncertainty often affects the conclusions of comparative life cycle assessments, especially when differences in average environmental impacts between two competing fuels/products are small. This study builds upon an existing deterministic life cycle framework for coal and develops uncertainty estimates of associated greenhouse gas emissions, with the objective of supporting more robust decision-making in comparative energy systems analyses involving coal. Greenhouse gas emissions from fuel use and methane releases at coal mines, fuel use for coal transport and combustion of coal, based on publicly available data are included in the life cycle framework. Mean life cycle GHG emissions from coal are estimated to be 96 g CO 2e/MJ, while the 90% confidence interval ranged between 89 and 106 g CO 2e/MJ. Life cycle greenhouse gas emissions from Fischer-Tropsch (FT) coal-based gasoline are stochastically compared to emissions from petroleum-based gasoline. In the base case modeled, emissions from coal-based FT gasoline were found to be higher than emissions from petroleum-based gasoline with a probability of 80%, while they are lower with a probability of 20%. Results suggest that incorporating uncertainty in life cycle estimates is important, especially if these estimates are to be used within policy frameworks. © 2012 American Chemical Society."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864011929&partnerID=40&md5=67e37935b85fdc80524d0f4f360810b2","Translocations are commonly used conservation actions that aim at establishing new, self-sustaining populations of threatened species. However, many translocated populations are not self-sustaining but managed through supplemental feeding from the onset. Often, the decision to start managing is ad hoc, but managers will eventually have to make decisions for the future, for example, stop intervening, continue as it is or change the quantity of food provided. Such a decision requires managers to quantify the importance of supplemental feeding in determining the performance and population dynamics of translocated populations, information that is rarely available in the published literature. Using the hihi as a case study, we examined the importance of supplemental feeding for the viability of a translocated population in New Zealand. We found that supplemental feeding positively affected the survival and abundance of translocated adult hihi but also found evidence of negative density dependence on recruitment. We present two stochastic population models that project the hihi population under different management scenarios, quantitatively assessing the impact supplemental feeding has had on the population. Our results illustrate how important long-term targeted monitoring is for robust decision making about adaptive management. © 2012 The Zoological Society of London."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865232582&partnerID=40&md5=96af4f65b4b5900bc91eb227685b3af8","Without constant adaption, infrastructures tend to have a limited useful lifetime. This is because the demands placed on them as to function and use, are constantly changing. Through creating scenarios of alternate futures, it is possible to get insights into future opportunities and threats. Only then, it is possible to design infrastructure suitable for now as well as for a long-term future. This approach is given the name scenario planning. In this paper, we illustrate this approach through use of a case study. The expectation is, that as global oil reserves diminish, and oil trade patterns change, some refineries in Europe may close down. In Port of Rotterdam some (valuable) sites may become available, and can be put to alternate uses. The objective of the case study is to plan for a new function of such a site, which is suitable for the given location, is in line with the port vision and goals, and above all, a robust choice in any future that materialises. Copyright © 2012 Inderscience Enterprises Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865794995&partnerID=40&md5=e48fef2ca507cacffaba1255b0e31b57","Current projections of long-term trends in Atlantic hurricane activity due to climate change are deeply uncertain, both in magnitude and sign. This creates challenges for adaptation planning in exposed coastal communities. We present a framework to support the interpretation of current long-term tropical cyclone projections, which accommodates the nature of the uncertainty and aims to facilitate robust decision making using the information that is available today. The framework is populated with projections taken from the recent literature to develop a set of scenarios of long-term hurricane hazard. Hazard scenarios are then used to generate risk scenarios for Florida using a coupled climate-catastrophe modeling approach. The scenarios represent a broad range of plausible futures; from wind-related hurricane losses in Florida halving by the end of the century to more than a four-fold increase due to climate change alone. We suggest that it is not possible, based on current evidence, to meaningfully quantify the relative confidence of each scenario. The analyses also suggest that natural variability is likely to be the dominant driver of the level and volatility of wind-related risk over the coming decade; however, under the highest scenario, the superposition of this natural variability and anthropogenic climate change could mean notably increased levels of risk within the decade. Finally, we present a series of analyses to better understand the relative adequacy of the different models that underpin the scenarios and draw conclusions for the design of future climate science and modeling experiments to be most informative for adaptation. © 2012 Elsevier Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864047618&partnerID=40&md5=42f9f92792b5d24c38bccaa1cfedbdeb","In this paper we assess the efficacy of a dynamic adaptive planning (DAP) approach for guiding the long-term development of infrastructure. The efficacy of the approach is tested on the specific case of airport strategic planning. Utilizing a fast and simple model of an airport, and a composition of small models that can generate a wide spectrum of alternative futures, the performance of a dynamic adaptive plan is compared with the performance of a static, rigid implementation plan across a wide spectrum of conceivable futures. These computational experiments reveal that the static rigid plan outperforms the dynamic adaptive plan in only a small part of the spectrum. Moreover, given the wide array of possible futures, the dynamic adaptive plan has a narrower spread of outcomes than the static rigid plan, implying that the dynamic adaptive plan exposes planners to less uncertainty about its future performance despite the wide variety of uncertainties that are present. These computational results confirm theoretical hypotheses in the literature that DAP approaches are more efficacious for planning under uncertainty. © 2012 Pion and its Licensors."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862569203&partnerID=40&md5=3294164b28ed0d7da060125811540525","As part of the National Institute for Health and Clinical Excellence (NICE) single technology appraisal (STA) process, the manufacturer of febuxostat (Adenuric; Ipsen, UK) was invited to submit evidence for the clinical and cost effectiveness of febuxostat for the management of hyperuricaemia in patients with gout. The School of Health and Related Research Technology Appraisal Group (ScHARR-TAG) at the University of Sheffield were commissioned to act as the independent Evidence Review Group (ERG). This article provides details of the company's initial submission, the ERG's clarification questions and the ERG report submitted to NICE. The decision made by NICE is provided alongside a brief comment on additional results produced by a substantially different model, which were presented by the manufacturer after the production of the appraisal consultation document. The ERG produced a critical review of the evidence for the clinical evidence and cost effectiveness of the technology based upon the manufacturer's submission to NICE. The clinical evidence was derived from two head-to-head, phase III, multiarm, randomized, double blind, controlled trials comparing the efficacy and safety of febuxostat with fixed-dose allopurinol (300/100mg/day) in patients with hyperuricaemia and gout. The ERG considered that the trials were of reasonable methodological quality and measured a clinically relevant range of outcomes. Although a simple pooled analysis of the individual patient- level data from the two trials was undertaken, the statistical approach for combining the data was considered inappropriate by the ERG as it failed to preserve randomization and introduced bias and confounding. There was substantial uncertainty in the relationships reported by the manufacturer regarding serum uric acid levels and the incidence of gout flares and underlying patient utility. The mathematical model developed was flawed and was not corrected despite ERG comments. It focused only on patients receiving febuxostat (80mg/day titrated to 120mg/day if necessary) with fixed-dose allopurinol (300/100mg/day). Sequential treatment was not modeled, nor was titrating allopurinol to 900mg/day, which is regarded as best practice. Numerous other errors were identified, which included the uncertain price of febuxostat being sampled within the probabilistic sensitivity analyses. Supplementary exploratory modelling addressing the position of febuxostat where patients were intolerant or contraindicated to allopurinol was provided to the NICE Appraisal Committee following the release of the appraisal consultation document. The NICE Appraisal Committee concluded that febuxostat be recommended as an option for the management of chronic hyperuricaemia in gout only for people who are intolerant to allopurinol or for whom allopurinol is contraindicated. © 2012 Springer International Publishing AG."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859108005&partnerID=40&md5=b3936099838febe585389175b55d747d","This paper proposes and demonstrates a new interactive framework for sensitivity-informed de Novo planning to confront the deep uncertainty within water management problems. The framework couples global sensitivity analysis using Sobol' variance decomposition with multiobjective evolutionary algorithms (MOEAs) to generate planning alternatives and test their robustness to new modeling assumptions and scenarios. We explore these issues within the context of a risk-based water supply management problem, where a city seeks the most efficient use of a water market. The case study examines a single city's water supply in the Lower Rio Grande Valley (LRGV) in Texas, using a suite of 6-objective problem formulations that have increasing decision complexity for both a 10-year planning horizon and an extreme single-year drought scenario. The de Novo planning framework demonstrated illustrates how to adaptively improve the value and robustness of our problem formulations by evolving our definition of optimality while discovering key tradeoffs. © 2011 Elsevier Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861888662&partnerID=40&md5=f62e61b4f5b8edbca5fde575ed0c4bdd","The Afghanistan National Development Strategy identified billions of dollars of needs for transportation, water, energy, telecommunications, and other necessary infrastructure development for the rebuilding of Afghanistan. With economic sustainability as a primary aim, the coordination and prioritization of investments has been a challenge in part because of Afghanistan's volatile security situation along with the intricacies of the negotiating and coordinating efforts of numerous stakeholders. An understanding of the contributions of infrastructure systems and associated projects to the national development strategy is needed. This paper formulates a scenario-informed multicriteria approach to prioritize major project investments for infrastructure development subject to deep, nonprobabilistic uncertainties. The methods are inclusive of stakeholder values and accounts for deep uncertainties in governance, security, economy, environment, workforce, and other topics. The methods are applied in Afghanistan's Nangarhar province to assist in the selection among twenty-seven candidate infrastructure projects that are vulnerable to potential refugee immigration among other emergent conditions. The paper describes the relationships of selected projects to strategic goals while facilitating collaboration among government and nongovernment investors, donors, technologists, and other stakeholders. © 2012 American Society of Civil Engineers."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861084131&partnerID=40&md5=6ed364879813cc6bd283fa206131912a","Due to fast changing technologies, shortening product lifecycles, and increased global competition, companies today often need to develop new products continuously and faster. Successful introduction and acceleration of new product development (NPD) is important to obtain competitive advantage for companies. Since technology selection for NPD involves complex decision makings that are critical to the profitability and growth of a company, the selection of the most appropriate technology for a new product requires the use of a robust decision-making framework capable of evaluating several technology candidates based on multiple criteria. This paper presents an integrated model that adopts interpretive structural modeling (ISM) and fuzzy analytic network process (FANP) to evaluate various different available technologies for NPD. The ISM is used to understand the interrelationships among the factors, and the FANP is to facilitate the evaluation process of decision makers under an uncertain environment with interrelated factors. A case study of a flat panel manufacturer is performed to examine the practicality of the proposed model. The results show that the model can be applied for group decision making on the available technology evaluation and selection in new product development. Copyright © 2012 He-Yau Kang et al."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857194860&partnerID=40&md5=5307989a60f7f56a3c14ab712bea0c42","Supervisory control and data acquisition (SCADA) infrastructures have so far been brought into the huge amount of focus since the need for a more reliable power system accosting with the catastrophic events of interruptions has been raised. Hence, benefiting from an automation technology which can remotely monitor and coordinate commands to enable immediate response and switching are obviously inevitable. This paper outlines an implementable method in response to the goal of placement of remote terminal units (RTUs) which are in charge for data acquisition and control in a power distribution system. In this light, a new practical methodology based on the robust decision making (DM) method, analytical hierarchical process (AHP), is proposed to simply and at the same time profoundly exploit some practical aspects which have not been considered before in the cases of placements. The proposed method investigates both qualitative and quantitative aspects interrelated with the placement problem. Fuzzy sets are then involved to overcome the existent uncertainty and judgment vagueness. The optimum number of RTUs to be located in the previous-step obtained candidates is determined through reliability cost/worth analysis. Having the belief practically implemented on a real distribution test feeder of Iran's power grid, the efficacy and accuracy of the proposed methodology are satisfactorily confirmed. © 2011 Elsevier Ltd. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866167279&partnerID=40&md5=b9bf0cf69a1103088bbec78015140497","Shark attacks have historically been studied from a viewpoint of encounter number per region and so limited to the areas in which the attacks occurred. In this exploratory modeling study, the goal was to examine whether an area-specific cluster analysis algorithm undertaken with a modern cluster analysis tool (SaTScan™ 9.1.0) could enhance our spatial and spatio-temporal understanding of attack patterns. The data used were from Florida's east coast between 1994 and 2009. The program suggests several high- and low-risk areas for shark attacks. The results are discussed from a quantitative rather than qualitative perspective. © 2012 Copyright Taylor and Francis Group, LLC."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865638631&partnerID=40&md5=0a3dfe6cc497d8078de6c2c302624f0b","Emerging ""(computational) systems medicine"" challenges neuropsychiatry regarding the development of heuristic computational brain models which help to explore symptoms and syndromes of mental disorders. This methodology of exploratory modelling of mental functions and processes and of their pathology requires a clear and operational definition of the target variable (explanandum). In the case of schizophrenia, a complex and heterogeneous disorder, single psychopathological key symptoms such as working memory deficiency, hallucination or delusion need to be defined first. Thereafter, measures of brain structures can be used in a multilevel view as biological correlates of these symptoms. Then, in order to formally ""explain"" the symptoms, a qualitative model can be constructed. In another step, numerical values have to be integrated into the model and exploratory computer simulations can be performed. Normal and pathological functioning is to be tested in computer experiments allowing the formulation of new hypotheses and questions for empirical research. However, the crucial challenge is to point out the appropriate degree of complexity (or simplicity) of these models, which is required in order to achieve an epistemic value that might lead to new hypothetical explanatory models and could stimulate new empirical and theoretical research. Some outlines of these methodological issues are discussed here, regarding the fact that measurements alone are not sufficient to build models. © Georg Thieme Verlag KG Stuttgart · New York."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-83055181975&partnerID=40&md5=763b7bc841bbf54a9451881b34f2db73","Integrated catchment management (ICM), as promoted by recent legislation such as the European Water Framework Directive, presents difficult challenges to planners and decision-makers. To support decision-making in the face of high complexity and uncertainty, tools are required that can integrate the evidence base required to evaluate alternative management scenarios and promote communication and social learning.In this paper we present a pragmatic approach for developing an integrated decision-support tool, where the available sources of information are very diverse and a tight model coupling is not possible. In the first instance, a loosely coupled model is developed which includes numerical sub-models and knowledge-based sub-models. However, such a model is not easy for decision-makers and stakeholders to operate without modelling skills. Therefore, we derive from it a meta-model based on a Bayesian Network approach which is a decision-support tool tailored to the needs of the decision-makers and is fast and easy to operate. The meta-model can be derived at different levels of detail and complexity according to the requirements of the decision-makers. In our case, the meta-model was designed for high-level decision-makers to explore conflicts and synergies between management actions at the catchment scale. As prediction uncertainties are propagated and explicitly represented in the model outcomes, important knowledge gaps can be identified and an evidence base for robust decision-making is provided. The framework seeks to promote the development of modelling tools that can support ICM both by providing an integrated scientific evidence base and by facilitating communication and learning processes. © 2011 Elsevier Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949787569&partnerID=40&md5=bd0a067d7a1eb1b1bacd46fda7aea671","Modelling has permeated virtually all areas of industrial, environmental, economic, bio-medical or civil engineering: yet the use of models for decision-making raises a number of issues to which this book is dedicated: How uncertain is my model Is it truly valuable to support decision-making What kind of decision can be truly supported and how can I handle residual uncertainty How much refined should the mathematical description be, given the true data limitations Could the uncertainty be reduced through more data, increased modeling investment or computational budget Should it be reduced now or later How robust is the analysis or the computational methods involved Should could those methods be more robust Does it make sense to handle uncertainty, risk, lack of knowledge, variability or errors altogether How reasonable is the choice of probabilistic modeling for rare events How rare are the events to be considered How far does it make sense to handle extreme events and elaborate confidence figures Can I take advantage of expert phenomenological knowledge to tighten the probabilistic figures Are there connex domains that could provide models or inspiration for my problem Written by a leader at the crossroads of industry, academia and engineering, and based on decades of multi-disciplinary field experience, Modelling Under Risk and Uncertainty gives a self-consistent introduction to the methods involved by any type of modeling development acknowledging the inevitable uncertainty and associated risks. It goes beyond the ""black-box"" view that some analysts, modelers, risk experts or statisticians develop on the underlying phenomenology of the environmental or industrial processes, without valuing enough their physical properties and inner modelling potential nor challenging the practical plausibility of mathematical hypotheses; conversely it is also to attract environmental or engineering modellers to better handle model confidence issues through finer statistical and risk analysis material taking advantage of advanced scientific computing, to face new regulations departing from deterministic design or support robust decision-making. Modelling Under Risk and Uncertainty: Addresses a concern of growing interest for large industries, environmentalists or analysts: robust modeling for decision-making in complex systems. Gives new insights into the peculiar mathematical and computational challenges generated by recent industrial safety or environmental control analysis for rare events. Implements decision theory choices differentiating or aggregating the dimensions of risk/aleatory and epistemic uncertainty through a consistent multi-disciplinary set of statistical estimation, physical modelling, robust computation and risk analysis. Provides an original review of the advanced inverse probabilistic approaches for model identification, calibration or data assimilation, key to digest fast-growing multi-physical data acquisition. Illustrated with one favourite pedagogical example crossing natural risk, engineering and economics, developed throughout the book to facilitate the reading and understanding. Supports Master/PhD-level course as well as advanced tutorials for professional training Analysts and researchers in numerical modeling, applied statistics, scientific computing, reliability, advanced engineering, natural risk or environmental science will benefit from this book. © 2012 John Wiley & Sons, Ltd. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859615274&partnerID=40&md5=831d4572f471220870e6091a07dd5649","Olive production is currently increasing in Portugal, particularly in the Guadiana river basin. In Serpa and Moura, a traditional region of dry farming olive plantations now comprise up to 50% of the total irrigated area. The present paper reports some of the results of a benchmarking study conducted in the framework of the AquaStress Integrated Project in the Guadiana Case Study (GCS). The aim of the work was to produce references to anticipate water requirements when the future Ardila Irrigation System, which is connected to the Alqueva dam, opens in 2010. This system is intended to supply water to some 30 000ha of newly irrigated land in the region. During the study, public participation workshops were held to understand the specific needs and points of view of farmers and other stakeholders. The benchmarking analysis was based on continuous monitoring of 28 olive farms and collection of information on practices and production options available in 2007. An originality of this work is the use of a structural equation modelling (SEM) procedure to identify the effects of farmers' agricultural and irrigation practices on production and quality parameters. SEM is a statistical technique for testing and estimating causal relationships using a combination of statistical data and qualitative causal assumptions. It allows both confirmatory and exploratory modelling, meaning it is suited to both theory testing and development. The benchmarking process was based on participatory workshops that helped farmers understand how to improve their own irrigation and agricultural practices based on the experience of the rest of the group. Our approach was based on a heterogeneous set of plots, and results indicated that certain technical irrigation parameters, namely the number and discharge of drippers per tree, might have a greater influence on final production than total water applied. Such an approach could also be applied to data collected on a regular basis by extension services to identify on long term the influence of production management and practices on productivity. © 2011 John Wiley & Sons, Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856096493&partnerID=40&md5=7c8a1a7137caa040dd3d726ba1b8c512","There is a need to extend and refine the use of crash surrogates to enhance safety analyses. This is particularly true given opportunities for data collection presented by naturalistic driving studies. This paper connects the original research on traffic conflicts to the contemporary literature concerning crash surrogates using the crash-to-surrogate ratio, π. A conceptual structure is developed in which the ratio can be estimated using either a Logit or Probit formulation which captures context and event variables as predictors in the model specification. This allows the expansion of the crash-to-surrogate concept beyond traffic conflicts to many contexts and crash types. The structure is tested using naturalistic driving data from a study conducted in the United States (Dingus et al.; 2005). While the sample size is limited (13 crashes and 38 near crashes), there is reasonable correspondence between predicted and observed crash frequencies using a Logit model formulation. The paper concludes with a summary of empirical results and suggestions for future research. © 2011 Elsevier Ltd. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856983509&partnerID=40&md5=ff21beedd169a61a9b46f6954f888016","This paper will discuss the approach to autonomous navigation used by ""Q,"" an unmanned ground vehicle designed by the Trinity College Robot Study Team to participate in the Intelligent Ground Vehicle Competition (IGVC). For the 2011 competition, Q's intelligence was upgraded in several different areas, resulting in a more robust decision-making process and a more reliable system. In 2010-2011, the software of Q was modified to operate in a modular parallel manner, with all subtasks (including motor control, data acquisition from sensors, image processing, and intelligence) running simultaneously in separate software processes using the National Instruments (NI) LabVIEW programming language. This eliminated processor bottlenecks and increased flexibility in the software architecture. Though overall throughput was increased, the long runtime of the image processing process (150 ms) reduced the precision of Q's realtime decisions. Q had slow reaction times to obstacles detected only by its cameras, such as white lines, and was limited to slow speeds on the course. To address this issue, the image processing software was simplified and also pipelined to increase the image processing throughput and minimize the robot's reaction times. The vision software was also modified to detect differences in the texture of the ground, so that specific surfaces (such as ramps and sand pits) could be identified. While previous iterations of Q failed to detect white lines that were not on a grassy surface, this new software allowed Q to dynamically alter its image processing state so that appropriate thresholds could be applied to detect white lines in changing conditions. In order to maintain an acceptable target heading, a path history algorithm was used to deal with local obstacle fields and GPS waypoints were added to provide a global target heading. These modifications resulted in Q placing 5 th in the autonomous challenge and 4 th in the navigation challenge at IGVC. © 2012 SPIE-IS&amp;T."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863277712&partnerID=40&md5=8353c96f20ee2c88acb45a44e667d72a","The target is to study the effectiveness of air defense combat from the standpoint of flow management. Firstly, as one new concept, force flow management was proposed and elaborated. Then, the matter of Exploratory Modeling and Analysis was pulled into the effectiveness and evaluation of force flow management. And at the same time, the effectiveness and evaluation framework of force flow managemen based on EM&A, was also proposed. On the background of operation upon problem, the math model of force flow management was built. A specific analysis way of force flow management based on EM&A and the conclusive suggestion were proposed. The simulation does a whole research on the result which is produced by all kinds of uncertain combat factor. By the simulation, the analysis of air defense combat strategic decision will become more flexible and efficient. The suitability is better, too. © Right."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905593765&partnerID=40&md5=c606f42a43884c41c4769d67a2aecfe1","This paper presents empirical results of the use of a novel decision support prototype for emergency response situations, which was designed to enhance the understanding of the relative desirability of one potential course of action versus another. We have termed this understanding ""option awareness."" In particular, this paper describes the process employed by pairs of experiment participants while performing emergency responder roles using different types of ""decision space"" visualizations to help them collaborate on decisions. We examined the decision making process via a detailed analysis of the communication between the cooperating team members. The results yield implications for design approaches for visualizing option awareness. © 2012 ISCRAM."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900318949&partnerID=40&md5=36e65583e553c2884992328e00d4c0fc","Efficient and reliable electricity plays a vital role in today's society affecting a broad range of aspects of everyday life. Electricity networks need robust decision making mechanisms that enable the system to respond swiftly and effectively to any type of disruption or anomaly in order to ensure reliable electricity flow. Economic load dispatch (ELD) is the operation of generation plants producing reliable electricity at the lowest cost, while recognizing limitations of the system. The environmental economic load dispatch (EELD) problem extends the ELD to include environmental considerations which makes it even more challenging. In this study, we propose a novel two-stage economic and environmental load dispatching framework using particle filtering for the efficient and reliable dynamic dispatching of electricity under uncertainty. The proposed framework includes 1) a short term demand forecasting algorithm using wavelet transform adaptive modeling, and 2) a dynamic load dispatching algorithm using particle filtering developed in a simulation environment. The proposed approach has been successfully demonstrated for different scenarios in the original IEEE-30 bus test system, which has been benchmarked against the literature; and a modified version of the IEEE-30 bus test system, where the loads are updated according to the weather and other external conditions."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860853274&partnerID=40&md5=71bbd6d0ac450d7266895d3a5ccce51f","The life cycle of tile products are decreasing especially for customized products. The demand changes also fluctuate from time to time for each product type. This phenomena created crucial issue in meeting customers' demands within required due date. The occurrences of uncertain conditions caused the production line performance not able to meet the requirement because they faced uncertain changes in setup time, machinery breakdown time, lead time of manufacturing, and scraps. Hence, an accurate estimation on the production line in the presence of these uncertainties is required. Robust decision making on production line could be made when an accurate estimation of uncertain variables is modeled. Two approaches based on Bayesian inference and adaptive neuro-Fuzzy inference system (ANFIS) were utilized in this study for models development to estimate the effect of uncertain variables of production line in the tile industry. The models were validated and tested based on data obtained from a tile factory in Iran. The strength of our developed models is that the coefficients of decision variables are nonconstant. The best model was judged according to the mean absolute percentage error (MAPE) criterion. The results demonstrated that the ANFIS model generates the lower MAPE by 0.022 and higher correlation by 0.991 compared to the Bayesian model. Consequently, better decisions are generated due to easier identification of uncertainty data and the elaboration made the production planning process better understood."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-82455163668&partnerID=40&md5=bd7e891bfdf4ac5adeccfd31530d6991","Background: Hip fracture is a common problem in people aged > 60 years. The treatment options for individuals with high pre-fracture mobility, function and independence are hemiarthroplasty (HA) and total hip arthroplasty (THA). Objective: The aim of this report is to assess the clinical effectiveness and costeffectiveness evidence of THA compared with HA in patients with displaced intracapsular fracture who are cognitively intact with high pre-fracture mobility or function. Data sources: A systematic search was made of 11 databases of published and unpublished literature from their inception to December 2010: MEDLINE, EMBASE, Cumulative Index to Nursing and Allied Health Literature, The Cochrane Library, Biological Science Citation Index, Social Science Citation Index, Conference Proceedings Citation Index - Science, UK Clinical Trials Research Network and the National Research Register archive, Current Controlled Trials and ClinicalTrials.gov. Review methods: A systematic review of randomised controlled trials (RCTs) to assess the effectiveness of THA compared with HA in terms of dislocations, revisions, pain and function, and quality of life. Meta-analysis, independent subgroup analyses and exploratory cost-effectiveness modelling were performed. Results: The literature search identified 532 unique citations, of which eight RCTs with almost 1000 participants satisfied the criteria for the effectiveness review. Meta-analysis found a statistically significant increased risk of dislocation for patients treated with THA compared with HA (p = 0.01), but a reduced risk of revision (p = 0.0003). There were no differences in terms of mortality. In all trials, individuals treated with THA reported better function and mobility and less pain than those treated with HA. Four trials reporting utility data found similar trends. Sensitivity analyses indicated that there were no statistically significant differences in outcomes based on follow-up, study quality, surgical approach taken, type of head or the use of cement. Four papers reported a cost-utility analysis or the cost-effectiveness of THA compared with HA. Exploratory modelling was undertaken that showed that THA is likely to be cost-effective compared with HA even when the limitations of the data and methodology are considered. Limitations: The costs and disutilities associated with revisions and dislocations were not included in the economic evaluation. Conclusions: THA appears to be more cost-effective than HA. It is likely that THA will be associated with increased costs in the initial 2-year period, but lower longer-term costs, owing to potentially lower revision rates. However, these longer-term costs have not been modelled. The capacity and experience of surgeons to perform THA have not been explored and these would need to be addressed at local level were THA to become recommended for active, elderly patients in whom THA is not contraindicated. Further studies examining the impact of surgeon experience on performing the two procedures may offer more robust evidence on outcomes. Funding: The National Institute for Health Research Health Technology Assessment programme. © Queen's Printer and Controller of HMSO 2011."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874008378&partnerID=40&md5=fb57afabdce5cc14217225269cc03e17","This demo presents an ICT system for collaborative situation assessment and strategic decision making that supports effective and efficient protection of the population and the environment against chemical hazards in industrial areas. Robust decision support taking into account multiple objectives entails the combination of Multi-Criteria Decision Analysis (MCDA) and Scenario-Based Reasoning (SBR). The ad-hoc formed workflow of (human and artificial) experts generates scenarios capturing uncertainties. Combining MCDA and SBR allows for structuring complex problems and accounting for uncertainties by the selection of a decision alternative that performs (sufficiently) well for various aims under a variety of different possible situation developments (i.e., scenarios)."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-83055172715&partnerID=40&md5=a62b42b50a4067741c77a5c8e7753951","Water resources management is in a difficult transition phase, trying to accommodate large uncertainties associated with climate change while struggling to implement a difficult set of principles and institutional changes associated with integrated water resources management. Water management is the principal medium through which projected impacts of global warming will be felt and ameliorated. Many standard hydrological practices, based on assumptions of a stationary climate, can be extended to accommodate numerous aspects of climate uncertainty. Classical engineering risk and reliability strategies developed by the water management profession to cope with contemporary climate uncertainties can also be effectively employed during this transition period, while a new family of hydrological tools and better climate change models are developed. An expansion of the concept of ""robust decision making,"" coupled with existing analytical tools and techniques, is the basis for a new approach advocated for planning and designing water resources infrastructure under climate uncertainty. Ultimately, it is not the tools and methods that need to be revamped as much as the suite of decision rules and evaluation principles used for project justification. They need to be aligned to be more compatible with the implications of a highly uncertain future climate trajectory, so that the hydrologic effects of that uncertainty are correctly reflected in the design of water infrastructure. © 2011 American Water Resources Association."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856720806&partnerID=40&md5=59b70443ae2582d1757c33248810a4c4","Risk managers and decision-makers seeking to prepare for malevolent manmade events, accidents, and natural disasters must appropriately plan for the interdependent nature of today's complex infrastructure systems. However, the study of such events and such complex systems is fraught with uncertainty. With this research, we seek to answer: how preparedness strategies are robust to the uncertainties present in the stochastic nature of disruptive events and in our ability to model those effects in interdependent economic and infrastructure sectors. As such, this work integrates the relatively recent approaches of robust optimization (Ben-Tal et al. 2009) with the Inoperability Input-Output Model (IIM) (Santos & Haimes 2004, Santos 2006), a risk-based transformation of the traditional economic input-output model, for robust planning in interdependent systems. An application to inland waterway port risk planning is discussed © 2011 Taylor & Francis Group, London."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859860220&partnerID=40&md5=6b0f3464de63ea32918582f4bd378d28","Power system economic dispatch with renewable energies like wind power is a typical decision-making problem with uncertainty. Firstly, a robust decision-making model for this type of decision-making problem is formulated in this paper based on the two-person zero sum game, and the detailed robust economic dispatch model for the power system with wind power and plug-in electric vehicles (PEV) is proposed. Further, a relaxation algorithm is adopted to get the optimal solution for the min-max problem. Then, the one hour ahead, ten hours ahead and day ahead economic dispatch simulations on the modified IEEE-9 bus system with wind generators and PEV are carried out and analyzed. And finally the effect of PEVs on smoothing load demand is verified. © 2011 IEEE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855443581&partnerID=40&md5=5c70d9a587008bf4dae09be29fc7743c","So far, the need for power systems to be control automated has been approached. Moreover, the advent of smart technological infrastructures calls for a more systematic and well-designed schemes for data acquisition process. Hence, Remote Terminal Units (RTUs) are going to be considered as a crucial part of smart grid puzzle in power distribution systems. This paper aims to present a novel practical approach to have the RTU placement dilemma unraveled in power distribution systems. Practically approaching in this paper, some pragmatic factors of major importance in this respect are introduced. A framework with which the utility's experts' knowledge could be efficiently exploited is presented and handled through a robust decision making (DM) method referred to as Analytical Hierarchical Process (AHP). To overcome the existing inaccuracies associated with the experts knowledge and to effectively deal with the existent uncertainties and imprecision in the conventional AHP approach, fuzzy sets theory is incorporated to reach the most reliable and optimal substations as the RTU placement candidates. The efforts are focused on the pragmatic aspects interrelated with the procedure. Being applied to a real distribution power system of Iran's power grid, the applicability and practicality of the proposed fuzzy AHP (FAHP) method is well approved. © 2011 IEEE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857854661&partnerID=40&md5=f8a243e85dac6fd8cc8b3bca10511d28","Users today have come to expect constant connectivity through their mobile devices even when they are on the move, as most urban areas are heavily blanketed with Access Points and GSM Base Stations. Network selection poses a problem however as it is driven primarily by physical layer information, such as Received Signal Strength, which is a poor indicator of actual network performance. Thus there is a need for a more sophisticated method of discovering network resources that goes beyond radio interface characteristics. A way to address this is to use information not only from within the node, but also externally, for instance by enabling nodes to exchange information in a peer to peer manner. One of the biggest challenges of adopting this approach stems from the fact that nodes participating in such systems are not equally trustworthy. Therefore it is necessary to develop mechanisms that make the decision making process robust against dishonest information. In this work we evaluate data fusion techniques in the context of reconciling conflicting information due to the presence of dishonest or malfunctioning nodes. We show that it is possible to adapt classical data fusion techniques to develop a more robust decision-making mechanism, and moreover that Dempster-Shafer Theory is the optimal choice. © 2011 IEEE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865581439&partnerID=40&md5=81cce2cfb03ece89143920a4bad36a49","Many projects fail because project performance in unfavourable economic conditions was not anticipated at the investment decision stage. This type of failure may have been avoided through the selection of a strategy that enables the project to perform well under a range of possible future economic conditions (a 'robust' strategy); rather than an 'optimal' strategy for expected future conditions that will never eventuate. This paper describes a method of rapidly generating mine plans for a number of project strategies, and then evaluating it against a representative range of equally likely economic conditions. The key performance indicators derived for each strategy are the mean value and the distribution of values (variance). A prudent investor will identify prospective strategies not only on the merit of highest mean values (as a proxy for economic reward), but also on the probability of not meeting specified investment hurdles (a proxy for risk). A robust strategy will achieve its financial objective even under adverse economic variations. This paper focuses on the effect of commodity price uncertainty; particularly around the choice of mining and processing capacity levels. Results from enumerating 2000 prices scenarios against 63 strategy combinations show how processing and mining capacity can be manipulated to manage the risk-reward trade-off."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855679801&partnerID=40&md5=bb261ad44b2603acd7e76f487d89df68","Purpose Although public-private partnerships (PPPs) have been used internationally, the New Zealand Government has only recently started to consider using PPPs to deliver public assets and services. However, there is uncertainty about whether the New Zealand Government should actively enter into PPP arrangements. The government lacks a robust decision-making tool for assisting with choosing alternative procurement methods. PPPs are seen as risky, but innovative procurement options, with obstacles to overcome before they use can become common place. Nervousness about the use of PPPs requires the New Zealand Government to have a thorough understanding of the drivers and obstacles, and also to understand the applicability of international PPP experience to New Zealand. The purpose of this paper is to investigate the drivers and obstacles for adopting PPPs in New Zealand and provide details on how these obstacles might be overcome by using innovative country-specific solutions. Design/methodology/approach Semi-structured interviews with senior industry players and round table discussions are the research methods used. Findings The research found that the drivers for PPP adoption include acceleration of infrastructure provision, better risk allocation, whole of life cost savings, improved quality of services, access additional revenue sources, benefits for local economic and social development, and improved project scrutiny. The results show that the drivers appear to be more than securing private financing for public infrastructure. Greater efficiency in the use of resources has been emphasised by New Zealand practitioners. With regard to the apparent obstacles, research showed these to be: political, social and legal risks, unfavourable economic and commercial conditions, high transaction costs and lengthy lead time, problems related to the public sector and problems with the private sector. Possible solutions to these obstacles are derived from national and international research and assessed for their applicability to New Zealand. Research limitations/implications The paper presents discussion on the concerns expressed by the New Zealand industry about PPPs at strategic, institutional, and industry level. The identified obstacles and suggested solutions provide some initial guidance on how to proceed with PPP implementation in New Zealand. More research needs to be done to understand the various key facets identified here (e.g. tendering process, contractual arrangement, and risk allocation) and their wider effects. The research is based on interviews with a limited number of senior industry respondents, along with the general results of three industry round table discussions. Therefore, follow-on interviews need to be conducted with private sector partners, sponsors and funding bodies, in order to gain a wider view of the issues under investigation. Originality/value The findings of the research are of assistance to decision makers in both the public and private sectors in New Zealand. By understanding the drivers and obstacles for PPP adoption, and posing solutions to these obstacles, the New Zealand construction industry might be in a better position to adopt PPP schemes. © 2011 Emerald Group Publishing Limited. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861792528&partnerID=40&md5=147203f3f227f6d151ebac90a5b39129","The performative co-construction of academic life through myriad metrics is now a global phenomenon as indicated by the plethora of university research or journal ranking systems and the publication of 'league' tables based on them. If these metrics are seen as actively constituting the social world, can an analysis of this 'naturally occurring' data reveal how these new technologies of value and measure are recursively defining the practices and subjects of university life? In the UK higher education sector, the otherwise mundane realities of academic life have come to be recursively lived through a succession of research assessment exercises (RAEs). Lived through not only in the RAEs themselves, but also through the managed incremental changes to the academic and organizational practices linked to the institutional imaginings of planning for, and anticipating the consequences of, the actual exercises. In the 'planning for' mode an increasing proportion of formerly sociology submissions have shifted into 'social policy'. This is one instance of how institutional 'game-playing' in relation to the RAE enacts the social in quite fundamental ways. Planning an RAE 2008 submission in Sociology required anticipation of how a panel of 16 peers would evaluate 39 institutions by weighted, relative worth of: aggregated data from 1,267 individuals who, between them cited a total of 3,729 'outputs'; the detailed narrative and statistical data on the research environment; and a narrative account of academic 'esteem'. This data provided such institutional variables as postgraduate student numbers, sources of student funding, and research income from various sources. To evaluate the 'quality' of outputs various measures of the 'impact' and/or 'influence' of journals, as developed from the Thomson-Reuters Journal Citation Reports, was linked to the data. An exploratory modelling exercise using these variables to predict RAE 2008 revealed that despite what we might like to think about the subtle nuances involved in peer review judgements, it turns out that a fairly astonishing 83 per cent of the variance in outcomes can be predicted by some fairly simple 'shadow metrics': quality of journals in the submission, research income per capita and scale of research activity. We conclude that measuring the value of sociology involves multiple mutual constructions of reality within which ever more nuanced data assemblages are increasingly implicated and that analysis of this data can make explicit some of the parameters of enactment within which we operate in the contemporary academy. © 2012 The Authors. Editorial organisation © 2012 The Editorial Board of the Sociological Review."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855820730&partnerID=40&md5=2099c86d86f36b3c4fa36c78a53041d6","Kiwifruit breeding has been supported in the past by the genetic analysis of breeding value of parents based on dissection of the components of variance. Data were collected from biparental mating designs such as the North Carolina Model 2 (NCM2) that is well suited to dioecious species such as kiwifruit. In the last decade we assisted the first attempts to apply marker-assisted selection (MAS) to traits such as gender and others characterised by simple Mendelian inheritance. The rapid evolution of molecular markers, the current development of large sets of them, of the order of tens of thousands, and the declining cost of large-scale genotyping, including 'de novo' sequencing of individuals through next generation sequencing (NGS), recently opened the way to more ambitious goals, such as the rapid detection of marker-trait associations through the linkage disequilibrium (LD) approach and the estimation of the breeding value of genotypes based on the genome-wide highdensity markers profile. The development of robust decision making support tools for such genomic selection requires the development of training populations of individuals well characterized for both phenotypic and molecular profiles. The latter topic together with the improvement of statistical models for predicting correlations between the true breeding value and its genomic estimate and the need to adopt adequate breeding programs conclude the discussion. Frequent references to the main kiwifruit breeding programs link the theoretical discussion to the peculiarities of the kiwifruit crop."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054926877&partnerID=40&md5=5f7f858029dcaabd157e043f1a36d51f","We introduce the notion of strategic uncertainty for bound-edly rational, non-myopic agents as an analog to the equilibrium selection problem in classical game theory. We then motivate the need for and feasibility of addressing strategic uncertainty and present an algorithm that produces decisions that are robust to it. Finally, we show how agents' rationality levels and planning horizons alter the robustness of their decisions. Copyright © 2011, Association for the Advancement of Artificial Intelligence. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054958520&partnerID=40&md5=d791f811aff854531d515fe74d498c8b","The proceedings contain 11 papers. The topics discussed include: learning adversarial reasoning patterns in customer complaints; strategy purification; addresing execution and observation error in security games; application of microsimulation towards modelling of behaviours in complex environments; robust decision making under strategic uncertainty in multiagent environments; linear-time resource allocation in security games with identical fully protective resources; computing randomized security strategies in networked domains; toward addressing human behavior with observational uncertainty in security games; leadership games and their application in super-peer networks; towards the integration of multi-attribute optimization and game theory for border security patrolling strategies; and towards analyzing adversarial behavior in clandestine networks."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053349557&partnerID=40&md5=6308a0b94366c9f27a62a13c887de264","Models are frequently used for decision support in modern day decision making. This approach is referred as model-based decision support and it is mostly-at least implicitly-used for predictive purposes. However, predictions are almost always wrong and can be dramatically misleading for policy making. Another shortcoming related to predictive model use is the lack of proper consideration of deep uncertainty. Deep uncertainty refers to the lack of knowledge or agreement related to the correct representation of a system and the evaluation of (model-based) outcomes. This paper proposes to embrace deep uncertainty by using models in an exploratory way in order to improve model-based decision support under deep uncertainty. For this purpose, a new research methodology for analyzing complex and deeply uncertain systems - Exploratory Modeling and Analysis- is combined with System Dynamics modeling to capture deep uncertainties and dynamic complexities related to energy transitions. In this paper, we illustrate this methodological approach by using three different versions of a world wind power model (to introduce structural uncertainties) and by introducing some parametric uncertainties. This case clearly illustrates the need to consider both structural and parametric uncertainties for technology management under deep uncertainty. © 2011 IEEE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891680684&partnerID=40&md5=a389f4a0ba72fe41efd243c24c32db67","Hip fracture is a common problem in people aged > 60 years. The treatment options for individuals with high pre-fracture mobility, function and independence are hemiarthroplasty (HA) and total hip arthroplasty (THA). The aim of this report is to assess the clinical effectiveness and cost-effectiveness evidence of THA compared with HA in patients with displaced intracapsular fracture who are cognitively intact with high pre-fracture mobility or function. A systematic search was made of 11 databases of published and unpublished literature from their inception to december 2010: MEDLINE, EMBASE, Cumulative Index to Nursing and Allied Health Literature, The Cochrane Library, Biological Science Citation Index, Social Science Citation Index, Conference Proceedings Citation Index - Science, UK Clinical Trials Research Network and the National Research Register archive, Current Controlled Trials and ClinicalTrials.gov. A systematic review of randomised controlled trials (RCTs) to assess the effectiveness of THA compared with HA in terms of dislocations, revisions, pain and function, and quality of life. Meta-analysis, independent subgroup analyses and exploratory cost-effectiveness modelling were performed. The literature search identified 532 unique citations, of which eight RCTs with almost 1000 participants satisfied the criteria for the effectiveness review. Meta-analysis found a statistically significant increased risk of dislocation for patients treated with THA compared with HA (p = 0.01), but a reduced risk of revision (p = 0.0003). There were no differences in terms of mortality. In all trials, individuals treated with THA reported better function and mobility and less pain than those treated with HA. Four trials reporting utility data found similar trends. Sensitivity analyses indicated that there were no statistically significant differences in outcomes based on follow-up, study quality, surgical approach taken, type of head or the use of cement. Four papers reported a cost-utility analysis or the cost-effectiveness of THA compared with HA. Exploratory modelling was undertaken that showed that THA is likely to be cost-effective compared with HA even when the limitations of the data and methodology are considered. The costs and disutilities associated with revisions and dislocations were not included in the economic evaluation. THA appears to be more cost-effective than HA. It is likely that THA will be associated with increased costs in the initial 2-year period, but lower longer-term costs, owing to potentially lower revision rates. However, these longer-term costs have not been modelled. The capacity and experience of surgeons to perform THA have not been explored and these would need to be addressed at local level were THA to become recommended for active, elderly patients in whom THA is not contraindicated. Further studies examining the impact of surgeon experience on performing the two procedures may offer more robust evidence on outcomes. The National Institute for Health Research Health Technology Assessment programme."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052230550&partnerID=40&md5=cd069bbf8b9a83b353870c3fac2a5b91","Internal service encounters facilitate the benefits brought to the firm by external service encounters. However, we lack understanding of internal service encounters in non-Western contexts. This study develops understanding of the internal service encounter in the distinct culture of the Chinese. Data is in the form of critical incidents (n = 526) that coded to 1,373 judgments. Results reveal nine categories that explain internal customers' judgments of internal encounters. The findings reveal the relational orientation, prevalent in Chinese culture, and this forms the basis for exploratory modeling of underlying mechanism of the internal encounter."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79956339708&partnerID=40&md5=50e200d6379273f6269bd17473b81166","On June 29th, 2009 the derailment of a freight train carrying 14 LPG (Liquefied Petroleum Gas) tank-cars near Viareggio, in Italy, caused a massive LPG release. A gas cloud formed and ignited triggering a flash-fire that resulted in 31 fatalities and in extended damages to residential buildings around the railway line. The vulnerability of the area impacted by the flash-fire emerged as the main factor in determining the severity of the final consequences. Important lessons learnt from the accident concern the need of specific regulations and the possible implementation of safety devices for tank-cars carrying LPG and other liquefied gases under pressure. Integrated tools for consequence assessment of heavy gas releases in urban areas may contribute to robust decision making for mitigation and emergency planning. © 2011 Elsevier Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79956301085&partnerID=40&md5=03ee6a559e6502d7dddd2f656b51dc6b","Advanced low-carbon energy technologies can substantially reduce the cost of stabilizing atmospheric carbon dioxide concentrations. Understanding the interactions between these technologies and their impact on the costs of stabilization can help inform energy policy decisions. Many previous studies have addressed this challenge by exploring a small number of representative scenarios that represent particular combinations of future technology developments. This paper uses a combinatorial approach in which scenarios are created for all combinations of the technology development assumptions that underlie a smaller, representative set of scenarios. We estimate stabilization costs for 768 runs of the Global Change Assessment Model (GCAM), based on 384 different combinations of assumptions about the future performance of technologies and two stabilization goals. Graphical depiction of the distribution of stabilization costs provides first-order insights about the full data set and individual technologies. We apply a formal scenario discovery method to obtain more nuanced insights about the combinations of technology assumptions most strongly associated with high-cost outcomes. Many of the fundamental insights from traditional representative scenario analysis still hold under this comprehensive combinatorial analysis. For example, the importance of carbon capture and storage (CCS) and the substitution effect among supply technologies are consistently demonstrated. The results also provide more clarity regarding insights not easily demonstrated through representative scenario analysis. For example, they show more clearly how certain supply technologies can provide a hedge against high stabilization costs, and that aggregate end-use efficiency improvements deliver relatively consistent stabilization cost reductions. Furthermore, the results indicate that a lack of CCS options combined with lower technological advances in the buildings sector or the transportation sector is the most powerful predictor of high-cost scenarios. © 2010 Elsevier B.V."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959285635&partnerID=40&md5=2e0ce5162c5d82a3096fb4afc5613963","In the context of rapid urbanization across Sub-Saharan Africa there is a critical need for more robust decision-making between different ways of providing sanitation services in existing and new peri-urban areas. In several countries, authorities tried to find solutions by developing strategies to address sanitation problems in the form of Strategic Sanitation Plans. In Burkina, Strategic Sanitation Plans have been elaborated and implemented since the 1990s. Fada N'Gourma, a secondary city in Burkina, also adopted a Strategic Plan for wastewater and excreta management in 2006. In this study we use material flow analysis as a decision making tool to verify technology options of the Plan. A model was developed and data was collected in order to assess material and nitrogen flows. The status quo situation was compared to scenario based on the proposals made in the Sanitation Plan. Results show that the technology options which were recommended improved human health in the short term. However, the options led to groundwater pollution in the medium term. Compared to the current situation, matter and nitrogen flows would increase by 7% and 7.4% respectively in groundwater. It is thus concluded that the proposed options will not achieve the Plan's stated objectives of environmental protection. © IWA Publishing 2011."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958109244&partnerID=40&md5=2e3e724a05bc906786eb4c80bd40912c","Decision-making for conservation is conducted within the margins of limited funding. Furthermore, to allocate these scarce resources we make assumptions about the relationship between management impact and expenditure. The structure of these relationships, however, is rarely known with certainty. We present a summary of work investigating the impact of model uncertainty on robust decision-making in conservation and how this is affected by available conservation funding. We show that achieving robustness in conservation decisions can require a triage approach, and emphasize the need for managers to consider triage not as surrendering but as rational decision making to ensure species persistence in light of the urgency of the conservation problems, uncertainty, and the poor state of conservation funding. We illustrate this theory by a specific application to allocation of funding to reduce poaching impact on the Sumatran tiger Panthera tigris sumatrae in Kerinci Seblat National Park, Indonesia. To conserve our environment, conservation managers must make decisions in the face of substantial uncertainty. Further, they must deal with the fact that limitations in budgets and temporal constraints have led to a lack of knowledge on the systems we are trying to preserve and on the benefits of the actions we have available (Balmford & Cowling 2006). Given this paucity of decision-informing data there is a considerable need to assess the impact of uncertainty on the benefit of management options (Regan et al. 2005). Although models of management impact can improve decision making (e.g.Tenhumberg et al. 2004), they typically rely on assumptions around which there is substantial uncertainty. Ignoring this 'model uncertainty', can lead to inferior decision-making (Regan et al. 2005), and potentially, the loss of the species we are trying to protect. Current methods used in ecology allow model uncertainty to be incorporated into the model selection process (Burnham & Anderson 2002; Link & Barker 2006), but do not enable decision-makers to assess how this uncertainty would change a decision. This is the basis of information-gap decision theory (info-gap); finding strategies most robust to model uncertainty (Ben-Haim 2006). Info-gap has permitted conservation biology to make the leap from recognizing uncertainty to explicitly incorporating severe uncertainty into decision-making. In this paper we present a summary of McDonald-Madden et al (2008a) who use an info-gap framework to address the impact of uncertainty in the functional representations of biological systems on conservation decision-making. Furthermore, we highlight the importance of two key elements limiting conservation decision-making - funding and knowledge - and how they interact to influence the best management strategy for a threatened species. Copyright © ASCE 2011."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79957959300&partnerID=40&md5=af8b6b03e0669a63d33548353aad928a","Climate change is challenging the way water utilities plan for the future. Observed warming and climate model projections now call into question the stability of future water quantity and quality. As water utilities cope with preparing for the large range of possible changes in climate and the resulting impacts on their water systems, many are searching for planning techniques to help them consider multiple possible conditions to better prepare for a different, more uncertain, future. Many utilities need these techniques because they cannot afford to delay significant decisions while waiting for scientific improvements to narrow the range of potential climate change impacts. Several promising methods are being tested in water utility planning and presented here for other water utilities to consider. The methods include traditional scenario planning, classic decision making, robust decision making, real options, and portfolio planning. Unfortunately, for utilities vulnerable to climate change impacts, there is no one-size-fits-all planning solution. Every planning process must be tailored to the needs and capabilities of the individual utility. © 2011 American Water Resources Association."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79957657110&partnerID=40&md5=225a00be2bc00adcd69f874f0ab959c2","Past seasonality changes are often poorly represented by Quaternary proxies because one season, or one factor, dominates the reconstructed signal. During the early Holocene in New Zealand, mean annual temperatures were at least 1.5°C warmer than present. However, treelines were lower, suggesting summers were cooler. Here we use a forest ecosystem process model, LINKNZ, to explore past precipitation and temperature seasonality changes in an intermontane basin of the Southern Alps, New Zealand. Pollen and macrofossils from the basin show that during an early-Holocene warm event (11 500 to 9500 cal. yr BP) podocarp and broadleaved species dominated. In exploratory modelling runs, mean annual temperatures were increased by up to 2°C, precipitation was reduced by 20-30%, and temperature seasonality reduced. When mean annual temperature was increased by 1.0-2.0°C, LINKNZ reconstructed wet forest associations, very different to those in the early-Holocene fossil assemblages. Acceptable matches were made with the early-Holocene vegetation using elevated temperature scenarios with up to 30% lower annual precipitation and decreased temperature seasonality. Longer growing seasons apparently compensate for cooler summer temperatures. We suggest that during the early Holocene in this area, westerly wind flow over the Southern Alps to the west of the basin was less, reducing spill-over rainfall and vapour pressure deficits. Warm oceans generated milder, cloudier climates reducing seasonal contrasts. Inverse modelling is recommended as a tool for exploring past climate scenarios when proxies fail to reconstruct important climate variables. © The Author(s) 2011."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79956091068&partnerID=40&md5=68acfb79dc949302c657f75ed2b3a310","Previously, we used exploratory modeling to forecast a landscape of plausible outcomes for a set of options. Outcomes were displayed as a set of box-plots that illustrate outcome-frequencies distributed across an evaluative dimension (e.g., cost, score, or utility). Such a display depicts a ""decision space"" because it directly supports comparing and deciding among available courses of action rather than simply providing the facts about the situation (the ""situation space""). Our previous research showed that such decision spaces provide what we termed ""option awareness"" - an ability to determine robust options that will have good outcomes across the broadest swath of plausible futures. Moreover, such option awareness resulted in increased confidence in the chosen option. When extending this approach to collaborative decision making, issues arise when team members' decision spaces are in conflict - when merely jointly executing the most robust individual options does not yield the most robust collaborative option because each individual's decision space does not account for the synergy that may emerge from collaboration. The current paper describes issues for providing collaborative decision support. It includes a proposed categorization of different types of synergies and defines conflicted versus unconflicted cases. Further, this paper describes experiments that are now underway to determine the robustness of joint decisions, the confidence with which these decisions are made, and the nature of the coordination among teams of decision makers. © 2011 IEEE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959333256&partnerID=40&md5=656203e407c1acc92aae6e69cbaafbde","This research demonstrates that when individuals are expected to detect novel targets, they will be best prepared when trained with diverse categories. Participants were trained in a simulated luggage screening task in one of three conditions of diversity: high (participants searched for dangerous objects belonging to five different categories); low (participants searched for targets belonging to one of the five categories); and no training (control condition). After training, all participants were asked to look for the same novel dangerous objects in the bags. Results show that, during training, the low diversity condition resulted in highest hit rates and fastest response times. In contrast, after training, results were reversed: participants that trained in a high diversity condition were most effective at detecting novel targets. Those with no training at all were equally poor at detecting novel targets as those that trained in a low diversity condition. © 2011 Psychology Press."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955602733&partnerID=40&md5=a50a1d8579d6b7cc42e8853c34b6777b","In Taiwan, four alternatives have been considered for final disposal of food wastes; they are pig feed, anaerobic digestion, feedstuff production, and composting. However, an optimal method has not been determined. This research applied multivariate factor analysis and cluster analysis to analyze the information collected using the Likert scale questionnaires so that reliable and effective quantitative results can be obtained for more robust decision making. Composting stands out as the most acceptable method through the analysis of the quantified degrees of preference and the grades of acceptability. Additionally, a quantified strength, weakness, opportunity, and threat (SWOT) analysis was compiled through the original questionnaires. Comparison of the results obtained using the multiple-variable statistical analysis and the SWOT analysis confirms that SWOT results are valuable references for general decision making. However, whether results are consistent or not depends on the conditions of individual cases studied. © Copyright 2011, Mary Ann Liebert, Inc."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953754310&partnerID=40&md5=d4ef5b45fcba7b1cd163c237286f37ff","Modern nuclear magnetic resonance (NMR) spectroscopy generates complex and information-rich metabolic profiles. These require robust, accurate, and often sophisticated statistical techniques to yield the maximum meaningful knowledge. In this chapter, we describe methods typically used to analyze such data. We begin by describing seven goals of metabolic profile analysis, ranging from production of a data table to multi-omic integration for systems biology. Methods for preprocessing and pretreatment are then presented, including issues such as instrument-level spectral processing, data reduction and deconvolution, normalization, scaling, and transformations of the data. We then discuss methods for exploratory modeling and exemplify three techniques: principal components analysis, hierarchical clustering, and self-organizing maps. Moving to predictive modeling, we focus our discussion on partial least squares regression, orthogonal partial least squares regression, and genetic algorithm approaches. A typical set of in vitro metabolic profiles is used where possible to compare and contrast the methods. The importance of validating statistical models is highlighted, and standard techniques for doing so, such as training/test set and cross-validation are described. Finally, we discuss the contributions of statistical techniques such as statistical total correlation spectroscopy, and other correlation-based methods have made to the process of structural characterization for unknown metabolites."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650858064&partnerID=40&md5=8dd3dcdb1835e37072d93d5a6a748c87","In recent years, numerous attempts have been made to reduce the global environmental and associated socio-economic impacts of construction activities to achieve sustainable development goals. A sustainable system or activity refers to an eco-friendly, cost effective and socio-politically viable solution. This paper utilizes triple-bottom-line (TBL) sustainability criteria for the selection of a sustainable flooring system in Tehran (Iran). Three types of block joisted flooring systems - concrete, clay, and expanded polystyrene (EPS) blocks - have been investigated using life cycle analysis (LCA). Proposed approach provides a comprehensive evaluation system based on TBL criteria that are further divided into thirteen sub-criteria. It includes: (1) Environmental concerns (resource depletion, waste and emissions, waste management, climate change, environmental risk, embodied energy and energy loss); (2) Economic concerns (material cost, construction cost, and occupation and maintenance cost); and (3) Socio-political issues (social acceptance, vulnerability of area, and building weight). Analytical hierarchy process (AHP) is used as a multi-criteria decision making technique that helps to aggregate the impacts of proposed (sub)criteria into a sustainability index (SI) through a five-level hierarchical structure. Integration of AHP and LCA provides a framework for robust decision making that is consistent with sustainable construction practices. A detailed analysis shows that the EPS block is the most sustainable solution for block joisted flooring system in Tehran. © 2010 Elsevier Ltd. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955712510&partnerID=40&md5=8556e6720cf4cfd8b6d57e246adcdec9","Agent-based simulation (ABS) studies have recently been employed to support policy decisions. This article addresses the particular potentials and problems that ABS faces in this usage. First, the author warns against taking ""familiarity"" with specific ABS as a criterion for having confidence in the model's policy recommendations. Second, he shows that specific epistemic issues-in particular the high number of detailed simulated systems-require additional reflection on which decision rules to choose for policy decisions based on ABS. Third, the author points out directions in which the construction and uses of ABS in policy decision could be improved. Each of these issues is illustrated by simulation studies undertaken to investigate smallpox vaccination policies. © 2011 SAGE Publications."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952381731&partnerID=40&md5=0d7a0749b6669eaa9025f1b6d529f54a","Purpose - In a soccer robot game, the environment is highly competitive and dynamic. In order to work in the dynamically changing environment, the decision-making system of a soccer robot system should have the features of flexibility and real-time adaptation. The purpose of this paper is to focus on the middle-size soccer robot league (MSL) and present new hierarchical hybrid fuzzy methods for decision making and action selection of an MSL robot. Design/methodology/approach - In this paper, new hierarchical hybrid fuzzy methods for decision making and action selection of a robot in MSL are presented. First, the behaviors of an agent are introduced, implemented and classified in two layers, the low-level behaviors and the high-level behaviors. In the second layer, a two-phase mechanism for decision making is introduced. In phase one, some useful methods are implemented which check the robot's situation for performing required behaviors. In the next phase, the team strategy, team formation, robot's role and the robot's positioning system are introduced. A fuzzy logical approach is employed to recognize the team strategy and furthermore to tell the player the best position to move. Findings - This methodology was implemented on the ADRO RoboCup Team and ADRO team performance 2008 was compared with its previous version 2007. The results showed the success of this methodology; the team performance in coordination and collaboration highly improved; in fact, the players switched their strategic area smoothly as the team strategy changed in a reasonable manner, the robots carried out the high-level behaviors much more efficiently and the final results were enhanced significantly. Originality/value - This paper is a result of the authors' original research work in the field of autonomous robot-middle size soccer robot, supported by Islamic Azad University - Khorasgan Branch, Isfahan, Iran. © Emerald Group Publishing Limited [ISSN 0143-991X]."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651378384&partnerID=40&md5=991427e0e681031b6076ebcfcef10068","The Korean National Emergency Management Agency proposed to replace existing public safety wireless networks of 46 agencies with a nation-wide consolidated network. This study compares the publicprivate partnership alternative of sharing a network with the conventional alternative of building a government autonomous network. Using exploratory modeling and real option analysis which compute path-dependent values (including network effects and switching costs) of all the plausible sequential incremental investments against a wide range of future states, this study has designed adaptive investment strategies (""start robust, then adapt"") which start in the highest pay-off area, and then make investment decisions about whether to expand or switch to lower pay-off areas, based on an updated information of technology prospects and the previous-stage performances of inter-agency operational effectiveness and publicprivate partnership. This case study has demonstrated that well-designed adaptive investments will enhance long-term values and reduce downfalls arising from publicprivate partnership. © 2010 Elsevier Ltd. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857959240&partnerID=40&md5=e3964495e1ce01285b7e0827699ecda7","Simulations of future climate change impacts are highly uncertain, particularly for catchment hydrology, where output from models of complex dynamic systems (global climate) are used as inputs to models of complex dynamic systems (hydrology models). This is problematic where decision-making for adaptation is underpinned by future climate predictions, and where policy-makers have opted to delay adaptation until either uncertainties are reduced, or climate change signals emerge from observations. This paper, using the Boyne catchment in the east of Ireland as a case study, discusses the uncertainties involved in climate change impact assessment for catchment hydrology and highlights why uncertainties are unlikely to be constrained or reduced in the time-scale required for adaptation. In addition, by calculating the time required for climate change signals to emerge from the observational record and the magnitude of change required for detection, it is highlighted that waiting for climate signals to be statistically detectable is not an option for effective adaptation. The paper concludes by considering how a paradigm shift in how we use the output from climate impact assessments can progress the adaptation agenda given the limits to prediction identified. © 2011 Copyright Geographical Society of Ireland."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951540364&partnerID=40&md5=0d0f33880565599342de9a41ae57c5a8","Risky decision making on the Iowa Gambling Task (IGT) has been observed in several psychiatric disorders, including substance abuse, schizophrenia, and pathological gambling. Such deficits are often attributed to impaired processing within the orbitofrontal cortex (OFC) because patients with damage to this area or to the amygdala, which is strongly interconnected with the OFC, can likewise show enhanced choice of high-risk options. However, whether damage to the OFC or amygdala impairs subjects' ability to learn the task, or actually affects the decision-making process itself, is currently unclear. To address these issues, rats were trained to perform a rodent gambling task (rGT) either before or after bilateral excitotoxic lesions to the basolateral amygdala (BLA) or OFC. Maximum profits in both the rGT and IGT are obtained by favoring smaller rewards associated with lower penalties, and avoiding the tempting, yet ultimately disadvantageous, large reward options. Lesions of the OFC or BLA made before task acquisition initially impaired animals' ability to determine the optimal strategy, but did not disrupt decision making in the long term. In contrast, lesions of the BLA, but not the OFC, made after the task had been acquired increased risky choice. These results suggest that, although both regions contribute to the development of appropriate choice behavior under risk, the BLA maintains a more fundamental role in guiding these decisions. The maladaptive choice pattern observed on the IGT in patients with OFC lesions could therefore partially reflect a learning deficit, whereas amygdala damage may give rise to a more robust decision-making impairment. Copyright © 2011 the authors."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79954457123&partnerID=40&md5=2358f6c872ab34ab00097440b413c016","Armada area defense is a complex work. The long-range programming, tha having System of Systems(SoS) characteristics at strategic level, is very important and difficult for it. It's necessary to use Exploratory Analysis(EA) to help us have a broad understanding about the issue, by quantificationally exploring at a high-level model that specially built for EA, before going deep into details. The Armada area defence issue was analyzed at macro-level view, a software tool-EASim to support EA based on time-sliced influence diagrams was developed, a high-level analysis model was built, and some simple analysis illustrations were given."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78751636549&partnerID=40&md5=71eae3f7f7d200963bce38ec8b9fcf01","As part of the National Institute for Health and Clinical Excellence (NICE) single technology appraisal (STA) process, the manufacturer of febuxostat (Adenuric®; Ipsen, UK) was invited to submit evidence for the clinical and cost effectiveness of febuxostat for the management of hyperuricaemia in patients with gout. The School of Health and Related Research Technology Appraisal Group (ScHARR-TAG) at the University of Sheffield were commissioned to act as the independent Evidence Review Group (ERG).This article provides details of the companys initial submission, the ERGs clarification questions and the ERG report submitted to NICE. The decision made by NICE is provided alongside a brief comment on additional results produced by a substantially different model, which were presented by the manufacturer after the production of the appraisal consultation document.The ERG produced a critical review of the evidence for the clinical evidence and cost effectiveness of the technology based upon the manufacturers submission to NICE.The clinical evidence was derived from two head-to-head, phase III, multi-arm, randomized, double blind, controlled trials comparing the efficacy and safety of febuxostat with fixed-dose allopurinol (300100mgday) in patients with hyperuricaemia and gout. The ERG considered that the trials were of reasonable methodological quality and measured a clinically relevant range of outcomes. Although a simple pooled analysis of the individual patient-level data from the two trials was undertaken, the statistical approach for combining the data was considered inappropriate by the ERG as it failed to preserve randomization and introduced bias and confounding. There was substantial uncertainty in the relationships reported by the manufacturer regarding serum uric acid levels and the incidence of gout flares and underlying patient utility. The mathematical model developed was flawed and was not corrected despite ERG comments. It focused only on patients receiving febuxostat (80mgday titrated to 120mgday if necessary) with fixed-dose allopurinol (300100mgday). Sequential treatment was not modeled, nor was titrating allopurinol to 900mgday, which is regarded as best practice. Numerous other errors were identified, which included the uncertain price of febuxostat being sampled within the probabilistic sensitivity analyses. Supplementary exploratory modelling addressing the position of febuxostat where patients were intolerant or contraindicated to allopurinol was provided to the NICE Appraisal Committee following the release of the appraisal consultation document.The NICE Appraisal Committee concluded that febuxostat be recommended as an option for the management of chronic hyperuricaemia in gout only for people who are intolerant to allopurinol or for whom allopurinol is contraindicated. © 2011 Adis Data Information BV. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905647841&partnerID=40&md5=f016983d8ee9f06e9645a7991b0eecd0","We have been using exploratory modeling to forecast multiple plausible outcomes for a set of decision options situated in the emergency response domain. Results were displayed as a set of box-plots illustrating outcome frequencies distributed across an evaluative dimension (e.g., cost, score, or utility). Our previous research showed that such displays provide what we termed ""option awareness"" - an ability to determine robust options that will have good outcomes across the broadest number of plausible futures. This paper describes an investigation into extending this approach to collaborative decision making by providing a visualization of both collaborative and individual decision spaces. We believe that providing such visualizations will be particularly important when each individuals decision space does not account for the synergy that may emerge from collaboration. We describe how providing collaborative decision spaces improves the robustness of joint decisions and engenders high confidence in these decisions."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906237335&partnerID=40&md5=a4693078af7536e8628f9fb302334a63","Due to relatively low precipitation, high population and low storage, south-east England is vulnerable to water stress. With population growth and climate change water stress periods are expected to become more frequent. Infrastructure capacity expansion will likely be part of the solution to these challenges. This paper investigates the performance of a new reservoir with three possible storage capacities, a water transfer scheme, a conjunctive use groundwater scheme and the expansion of the new desalination plant under severe climate, demand growth and energy cost uncertainty. Twenty infrastructure options are considered. These infrastructure portfolios are screened to identify the best performing candidate. The robustness of this candidate strategy to uncertainty is assessed using Robust Decision Making (RDM), a quantitative framework that helps devise robust system management plans when multiple sources of poorly specified uncertainty are present. Under RDM, numerous runs of a trusted system simulation model are made for each plan by combining inputs representing different plausible futures. We determine the vulnerabilities of the strategies using a cluster finding algorithm that finds clusters of failures in the solution space identifying under which ranges of input conditions an infrastructure portfolio has high failure rates. Performance is based on storage reliability, environmental and cost performance measures."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906231482&partnerID=40&md5=ca8b0e8808d2fece294e00017d47f736","In the UK, water resource managers are expected to develop comprehensive 25 year management plans based on deeply uncertain information of the effects of climate change on local hydrology and future socio-economic changes on localized demand regimes. Robust Decision Making (RDM) offers a method to quantitatively assess the robustness of different portfolios of management options over a broad range of possible futures. A robust water management regime is one that provides enough water at a reasonable price whilst safeguarding the environment over the largest range of uncertainty. Info-Gap decision theory offers an economical approach to RDM. Between a robustness curve that delineates the worst results of a management choice as all uncertain variables tend to the worse, and an opportuness curve that marks the line of the greatest possible success as all uncertain variables tend towards the positive, lie all the potential futures that could arise from multivariate supply/demand uncertainty. This paper utilizes Info-Gap decision theory to investigate the robustness of a series of basic management options on a simple one supply reservoir and one demand node water resource system. The case study described in this paper shows that Info-Gap decision theory offers a new lens to assess the ability of different management options to deal with severe uncertainties associated with climate change and future demand pressures. The robustness curves highlight potential preference reversals for management actions as uncertainties increase."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650618421&partnerID=40&md5=c65e58a84558df0d2149955dfcb18a1b","The proceedings contain 240 papers. The topics discussed include: the impact of unmanned weapon systems on individual and team performance; defining the entity transfer interoperability reference model for military applications; visualizing the human, social, cultural and behavioral components of a complex conflict ecosystem; violent extremist network representation and attack the network course of action analysis in social simulation; a constraint-based solver for the military unit path finding problem; what information does this question convey? leveraging help-seeking behavior for improved modeling in a simulation-based intelligent tutor; a cordon and search model and simulation using timed, stochastic, colored Petri nets for robust decision-making; DEVS-based doctrine validation of fleet anti-air defense; automating instruction support: insights from field research; and instructional strategies for scenario-based training: insights from applied research."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650600645&partnerID=40&md5=83f2f46c4dfab4ce11419b36d7eb94ef","In the current military operating environment, cordon and search missions (village searches) are conducted daily. It is expected that this mission profile will not change in the near future. Despite the frequency of this mission type, the planning tools available to military leaders are rudimentary and tedious. Planners must rely on over-simplified data tables and personal experience when planning a cordon and search. Computer tools to facilitate faster, more accurate plans do not yet exist. This paper proposes a timed, stochastic Petri Net model for cordon and search missions that can provide planners valuable mission insight using Monte Carlo simulation methods. This model can be used by military leaders to make robust decisions during mission planning and thus improve the quality of the final plan. © 2010 SCS."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874257707&partnerID=40&md5=e63949bfbd6390faff15d5e3fa90cc08","• 1st step in quantifying and incorporating uncertainty in TX water planning • Considers uncertainty in projections for demand and supply • Calculates deficit distributions and assesses reliability of water management strategies • Can analyze trade-offs in cost, yield, and reliability • Enables more robust decision making. © 2010 American Water Works Association."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79551510840&partnerID=40&md5=788303ff7bac57b9ba8443610c0ca1dc","Water resources management is in a difficult transition phase, trying to accommodate the large uncertainties associated with climate change, while struggling with implementing a difficult set of principles and institutional changes associated with integrated water resources management (IWRM) and adaptive management (AM). Water management is the principal medium through which many of the projected impacts of global warming will be felt and ameliorated. Many standard hydrological practices, based on assumptions of a stationary climate and variability, can be extended to accommodate numerous aspects of climate uncertainty. Adaptations of various strategies developed by the water management profession to cope with contemporary uncertainties and climate variability can also be effectively employed during this transition period, as a new family of hydrological tools and better climate change models are developed. ""Robust decision-making"" is among the new approaches being advocated for planning and designing water resources infrastructure under climate uncertainty. Copyright © 2010 IAHS Press."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880485039&partnerID=40&md5=82c93441a18308a367b83bb1dce50aab","University of Pittsburgh, University of Arkansas, The Analytic Hierarchy Process (AHP) is an established decision method used to synthesize judgments and select the best alternative. The AHP literature extensively discusses both theory and case studies for judgments made by one person. However, instances exist when the judgments of a group of individuals are needed for accurate knowledge representation and robust decision making. One such instance is spare parts processes for nuclear electricity plants. Elicitation of group knowledge is necessary because each work group may have different experiences and attitudes towards spare parts management. This paper presents an interview protocol used for group knowledge elicitation using AHP for nuclear spare parts inventory management. Inconsistencies in the data and challenges in AHP group aggregation are examined. A numerical example of employee responses is included. This research benefits the engineering manager by presenting a methodology to collect a range of knowledge across work groups. The authors' overall decision tool supports existing corporate culture while striving for continuous process improvement."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877880043&partnerID=40&md5=a8cd803d97740e1c8548ea4b88b5b02b","The Idaho National Laboratory is funded through the U.S. Department of Energy Office of Nuclear Energy and other customers who have direct contracts with the Laboratory. The people, equipment, facilities, and other infrastructure at the laboratory require continual investment to maintain and improve the laboratory's capabilities. With ever tightening federal and customer budgets, the ability to direct investments into the people, equipment, facilities, and other infrastructure that are most closely aligned with the laboratory's mission and customers' goals grows increasingly important. The ability to justify those investment decisions based on objective criteria that can withstand political, managerial, and technical criticism also becomes increasingly important. The Systems Engineering tools of decision analysis, risk management and roadmapping, when properly applied to such problems, can provide defensible decisions. © 2010 by Battelle Energy Alliance, LLC."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80055012476&partnerID=40&md5=a0e46b9afdbee03244f9f3817d20770b","Uncertainty and complexity are inherent characteristics of a modern product development process. Concurrent engineering necessitates the use of interim information that may be incomplete, provisional, inconsistent and unreliable for the purpose of the tasks at hand. Mechanisms that allow actors in the design process to exchange interim information while being able to estimate the remaining risk of rework is of great importance to ensure robust decision making and to realize continuous progress. However, product development processes are traditionally managed through the use of milestone, or earned value methods without enabling the measurement as well as the capture of progress according to the state of progress of design tasks. The present paper presents a new methodology for monitoring interim information transfers. The approach supports design process planners by providing them with a monitoring system to control when interim information should be released, with which pace, and at which point of progress. This takes into account the criticality of information that is one of the main drivers of rework risk. Copyright © 2010 by ASME."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952094876&partnerID=40&md5=3bacc72672bd472ca3bf169e20188a19","This paper presents a distributed system facilitating robust decision-making under uncertainty in complex situations often found in strategic emergency management. The construction of decision-relevant scenarios, each being a consistent, coherent and plausible description of a situation and its future development, is used as a rationale for collecting, organizing, filtering and processing information for decision-making. The construction of scenarios is targeted at assessing decision alternatives avoiding time-consuming processing of irrelevant information. The scenarios are constructed in a distributed setting ensuring that each decision can be founded on a coherent and consistent set of assessments and assumptions provided by the best (human or artificial) experts available within limited time. Our theoretical framework is illustrated by means of an emergency management example. © 2010 IEEE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960067343&partnerID=40&md5=da59feaff76f3b0f09beddd251b8d924","We summarize here the background and key concepts related to robust solutions in the context of supporting decision-making for problems characterized by deep uncertainties, which also were in the focus of the previous workshops on Coping with uncertainty, see, e.g., [3]. Although such problems are fundamentally different from statistical decision models, yet basic ideas of robust statistics are applicable to methods supporting robust decision-making under uncertainty. The main new issues are concerned with a proper representation of uncertainty, and its interactions with decisions. In particular, a key issue is the sensitivity of robust decisions with respect to low probability catastrophic events, that are of critical importance for analyzing global change problems. Robust decisions for problems exposed to extreme catastrophic events are essentially different from over-simplified decisions that ignore such events. Specifically, a proper treatment of extreme/rare events requires new paradigms of rational decisions, new performance indicators, and new spatio-temporal dimensions of heterogeneous interdependencies including etwork externalities and risks. This, in particular, needs new approaches to downscaling, upscaling and discounting. © Springer-Verlag Berlin Heidelberg 2010."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650687364&partnerID=40&md5=b39ac3ad8885befdcf627087746b3d3e","Objective: The aim of this study was to predict the number and trends of cancer cases for radiotherapy up to the year 2015 in Manipal Teaching Hospital, Pokhara, Nepal. Methods: A retrospective study was carried out on data retrieved from the radiotherapy treatment records of patients treated at Manipal Teaching Hospital between 28 September 2000 and 31 December 2008. Different statistical programmes were used for statistical modelling and prediction. Using curve-fitting methods, Linear, Logarithmic, Inverse, Quadratic, Cubic, Compound, Power, Exponential, and Growth models were tested. Results: Including constant term, none of the models were best fitted. However, excluding the constant term, the cubic model was best fitted; R2=0. 95, p=0.001 for total cancer cases, R2=0. 94, p=0.001 for female cancer cases and R2=0. 95, p=0.001 for male cancer cases. The cancer cases estimated using cubic model showed a steady increase in the total frequency of cancers (including male and female cancer cases) following the year 2010. The three most common cancers reported were head and neck 24.2% (CI 21.6 - 27.0), lung 20.9% (CI 18.4 -23.6), cervix 15.9% (CI 13.7-18.3) respectively. Conclusion: The cancer cases in need of radiotherapy will increase in future years. The curve fitting method could be an effective exploratory modelling technique for predicting cancer frequency and trends over the years."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953285776&partnerID=40&md5=5fa594cd22343d0e83a77bbc68551e71","WaterSim, a simulation model, was built and implemented to investigate how alternative climate conditions, rates of population growth, and policy choices interact to affect future water supply and demand conditions in Phoenix, AZ. WaterSim is a hierarchical model that represents supply from surface and groundwater sources and demand from residential, commercial, and agricultural user sectors, incorporating the rules that govern reservoirs, aquifer use, and land-use change. In this paper we: (1) report on the imperative for exploratory modeling in water-resource management, given the deep uncertainties of climate change, (2) describe the geographic context for the Phoenix case study, (3) outline the objectives and structure of WaterSim, (4) report on testing the model with sensitivity analyses and history matching, (5) demonstrate the application of the model through a series of simulation experiments, and (6) discuss the model's use for scenario planning and climate adaptation. Simulation results show there are significant challenges to Phoenix's water sustainability from climate change and rapid growth. Policies to address these challenges require difficult tradeoffs among lifestyles, groundwater sustainability, the pace of growth, and what is considered to be an appropriate level of risk of climate-induced shortage. © 2011 Pion Ltd and its Licensors."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957280570&partnerID=40&md5=7dd34bbc52528e727a3b4a16ec055955","While in the beginning of the environmental debate, conflicts over environmental and technological issues had primarily been understood in terms of ""risk"", over the past two decades the relevance of ignorance, or nonknowledge, was emphasized. Referring to this shift of attention to nonknowledge the article presents two main findings: first, that in debates on what is not known and how to appraise it different and partly conflicting epistemic cultures of nonknowledge can be discerned and, second, that drawing attention to nonknowledge in technology conflicts results in significant institutional effects and new constellations of actors in public debates. To illustrate and substantiate this political dynamics of nonknowledge we draw upon examples from the areas of agri-biotechnology and mobile phoning. In a first step, we develop in greater detail the concept of scientific cultures of nonknowledge and identify three such cultures involved in the social conflicts within the two areas. Subsequently, we analyze the specific dynamics of the politicisation of nonknowledge looking at the variety of actors involved and the pluralisation of perceptions and evaluations of what is not known. Then, we point out some of the institutional reactions to the political and cultural dynamics of scientific nonknowledge. We argue that the equal recognition of the diverse cultures of nonknowledge is a key prerequisite for socially legitimate and ""robust"" decision-making under conditions of politicised scientific nonknowledge. © The Author(s) 2010."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958504511&partnerID=40&md5=304636d0d0da8027a7f2275f6d364a16","This paper is concerned with multi-agent team modelling and robust decision-making problems arising in mission planning under uncertainty. We consider multi-agent planning problem with task coordination using stochastic programming. Agents within a centralised team cooperate so that the expected team performance is maximised. The success of each agent to accomplish any task is critical for the team performance. Inaccuracy on estimation of uncertain success probability is addressed using robust optimisation where different uncertainty sets are considered. Robust optimisation computes the optimal task allocation simultaneously with worst-case by taking into account of all scenarios. The computational results show trade-off between team efficiency and robustness of solution. © 2010 Inderscience Enterprises Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955863451&partnerID=40&md5=947c7d099201b62c42ecb5ab47db45d0","Whether and how to conduct a clinical study is one of the critical decisions to be made every day in the pharmaceutical industry. Robust measurements on the probability of success (POS) of a study are crucial in the decision-making process. Among many factors that can affect the POS, establishment of the statistical alternative hypothesis (H1) is arguably the most important consideration. The classical power is universally used for assessing the probability of H1 being accepted, given a certain value of the parameter of interest. If H1 is composite, power is often provided for a range of plausible values of the parameter to reflect our uncertainty about the parameter. This could become inconvenient for decision makers and potentially introduce biases into the decision-making process. This paper proposes an extension of Bayesian expected power (eBEP) as a single metric for assessing POS defined as the probability that H1 is accepted in a study as a tool for decision-making. eBEP is convenient, driven by scientific evidence, and systematically integrates the uncertainty, thus facilitates robust decision-making. The computation procedure of eBEP via Monte Carlo methods is provided. The application of eBEP is illustrated using a real-life bioequivalence study. Copyright © 2010 Taylor &amp; Francis Group, LLC."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955084450&partnerID=40&md5=eece44a58b97672f3acb12e70198c16a","As use of Akaike's Information Criterion (AIC) for model selection has become increasingly common, so has a mistake involving interpretation of models that are within 2 AIC units (ΔAIC ≤ 2) of the top-supported model. Such models are <2 ΔAIC units because the penalty for one additional parameter is 2 AIC units, but model deviance is not reduced by an amount sufficient to overcome the 2-unit penalty and, hence, the additional parameter provides no net reduction in AIC. Simply put, the uninformative parameter does not explain enough variation to justify its inclusion in the model and it should not be interpreted as having any ecological effect. Models with uninformative parameters are frequently presented as being competitive in the Journal of Wildlife Management, including 72 of all AIC-based papers in 2008, and authors and readers need to be more aware of this problem and take appropriate steps to eliminate misinterpretation. I reviewed 5 potential solutions to this problem: 1) report all models but ignore or dismiss those with uninformative parameters, 2) use model averaging to ameliorate the effect of uninformative parameters, 3) use 95 confidence intervals to identify uninformative parameters, 4) perform all-possible subsets regression and use weight-of-evidence approaches to discriminate useful from uninformative parameters, or 5) adopt a methodological approach that allows models containing uninformative parameters to be culled from reported model sets. The first approach is preferable for small sets of a priori models, whereas the last 2 approaches should be used for large model sets or exploratory modeling. © 2010 The Wildlife Society."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953809303&partnerID=40&md5=d67b50426e9ce1f685b56bee974de374","Climate change presents a significant planning challenge for water management agencies in the western United States. Changing precipitation and temperature patterns will disrupt their supply and extensive distribution systems over the coming decades, but the precise timing and extent of these impacts remain deeply uncertain, complicating decisions on needed investments in infrastructure and other system improvements. Adaptive strategies represent an obvious solution in principle, but are often difficult to develop and implement in practice. This paper describes work helping the Inland Empire Utilities Agency (IEUA) explicitly develop adaptive policies to respond to climate change and integrating these policies into the organizations' long-range planning processes. The analysis employs Robust Decision Making (RDM), a quantitative decision- analytic approach for supporting decisions under conditions of deep uncertainty. RDM studies use simulation models to assess the performance of agency plans over thousands of plausible futures, use statistical ""scenario discovery"" algorithms to concisely summarize those futures where the plans fail to perform adequately, and use these resulting scenarios to help decisionmakers understand the vulnerabilities of their plans and assess the options for ameliorating these vulnerabilities. This paper demonstrates the particular value of RDM in helping decisionmakers to design and evaluate adaptive strategies. For IEUA, the RDM analysis suggests the agency's current plan could perform poorly and lead to high shortage and water provisioning costs under conditions of: (1) large declines in precipitation, (2) larger-than-expected impacts of climate change on the availability of imported supplies, and (3) reductions in percolation of precipitation into the region's groundwater basin. Including adaptivity in the current plan eliminates 72% of the high-cost outcomes. Accelerating efforts in expanding the size of one of the agency's groundwater banking programs and implementing its recycling program, while monitoring the region's supply and demand balance and making additional investments in efficiency and storm-water capture if shortages are projected provides one promising robust adaptive strategy - it eliminates more than 80% of the initially-identified high-cost outcomes. © 2010 Elsevier Inc."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956546408&partnerID=40&md5=4e48d70d22d0feb6177e5314dafdea60","In order to improve the financial performance of electric power generation companies, we show how additional on-peak forwards can be exploited to hedge against the risk exposure of nonpeak physical power, taking advantage of the competitive market outside the regulated service territory. Specifically, we model the evolution of the forward price as a geometric Brownian motion (GBM) process without drift and show how the optimal amount of additional on-peak forwards can be determined under the mean minus variance criterion via simulation. Furthermore, we show how the simulation approach and the analytical results may lead to a robust decision-making process. Copyright © 2010 Institute of Industrial Engineers."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954951922&partnerID=40&md5=f20d9ac5aabbe0888da6a18a8ecd9b41","Changes in forest growth have been found in European forests and worldwide. However most observations have been derived from samples of restricted size, whose representativeness at a regional forest scale is questionable. National forest inventories provide an interesting perspective for both regional scale assessment of these trends and the investigation of their variations over environmental gradients, but have been little used.The aim of our work was to carry out an exploratory modelling analysis of productivity changes, based on the French national forest inventory data. The objectives were: (i) to assess recent trends in forest productivity and to investigate a possible recent decline as found in previous studies; and (ii) to investigate trend variations relative to site fertility factors. We focused on pure and even-aged stands of common beech (Fagus sylvatica L.) in north-eastern France, already documented from previous studies based on retrospective data.The dataset consisted of 925 temporary plots inventoried between 1979 and 2007. We developed two regression models of stand basal area increment (BAI) against stand developmental stage (dominant height), site fertility (site index in the SI model, environmental indicators in the EI model) and stand density (relative density index). The effect of calendar date was tested in order to investigate possible historical trends. Site fertility-date interactions were also tested to investigate the site-dependence of trends.The fitted models showed a high goodness of fit (adj. R 2 over 0.69). We showed an increase in stand BAI of 27.8% between 1977 and 1987, (10.4% between 1979 and 1987). Stand BAI then decreased by approximately 5% between 1987 and 2004. We thus confirmed the hypothesis of a recent decline in common beech vitality in its temperate range. The chronologies clearly depicted the effect of severe drought events (1976 and 2003), pointing out the predominant role of water availability in the changes observed. No significant site-dependence of the trend was identified. © 2010 Elsevier B.V."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953476309&partnerID=40&md5=1a5ba85d94212e01cfbb6bb7a4131353","Background: Partial blockade of voltage-gated sodium channels is neuroprotective in experimental models of inflammatory demyelinating disease. In this phase 2 trial, we aimed to assess whether the sodium-channel blocker lamotrigine is also neuroprotective in patients with secondary progressive multiple sclerosis. Methods: Patients with secondary progressive multiple sclerosis who attended the National Hospital for Neurology and Neurosurgery or the Royal Free Hospital, London, UK, were eligible for inclusion in this double-blind, parallel-group trial. Patients were randomly assigned via a website by minimisation to receive lamotrigine (target dose 400 mg/day) or placebo for 2 years. Treating physicians, evaluating physicians, and patients were masked to treatment allocation. The primary outcome was the rate of change of partial (central) cerebral volume over 24 months. All patients who were randomly assigned were included in the primary analysis. This trial is registered with ClinicalTrials.gov, NCT00257855. Findings: 120 patients were randomly assigned to treatment (87 women and 33 men): 61 to lamotrigine and 59 to placebo. 108 patients were analysed for the primary endpoint: 52 in the lamotrigine group and 56 in the placebo group. The mean change in partial (central) cerebral volume per year was -3·18 mL (SD -1·25) in the lamotrigine group and -2·48 mL (-0·97) in the placebo group (difference -0·71 mL, 95% CI -2·56 to 1·15; p=0·40). However, in an exploratory modelling analysis, lamotrigine treatment seemed to be associated with greater partial (central) cerebral volume loss than was placebo in the first year (p=0·04), and volume increased partially after treatment stopped (p=0·04). Lamotrigine treatment reduced the deterioration of the timed 25-foot walk (p=0·02) but did not affect other secondary clinical outcome measures. Rash and dose-related deterioration of gait and balance were experienced more by patients in the lamotrigine group than the placebo group. Interpretation: The effect of lamotrigine on cerebral volume of patients with secondary progressive multiple sclerosis did not differ from that of placebo over 24 months, but lamotrigine seemed to cause early volume loss that reversed partially on discontinuation of treatment. Future trials of neuroprotection in multiple sclerosis should include investigation of complex early volume changes in different compartments of the CNS, effects unrelated to neurodegeneration, and targeting of earlier and more inflammatory disease. Funding: Multiple Sclerosis Society of Great Britain and Northern Ireland. © 2010 Elsevier Ltd. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950594372&partnerID=40&md5=7f052fb6569286ef29337e693f14fb0d","Nature conservation may be considered a post-normal science in that the loss of biodiversity and increasing environmental degradation require urgent action but are characterised by uncertainty at every level. An 'extended peer community' with varying skills, perceptions and values are involved in decision-making and implementation of conservation, and the uncertainty involved limits the effectiveness of practice. In this paper we briefly review the key ecological, philosophical and methodological uncertainties associated with conservation, and then highlight the uncertainties and gaps present within the structure and interactions of the conservation community, and which exist mainly between researchers and practitioners, in the context of nature conservation in the UK. We end by concluding that an openly post-normal science framework for conservation, which acknowledges this uncertainty but strives to minimise it, would be a useful progression for nature conservation, and recommend ways in which knowledge transfer between researchers and practitioners can be improved to support robust decision making and conservation enactment. © 2009 Elsevier GmbH. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950690162&partnerID=40&md5=77a81fd3682321558cd1c6774290d7b8","Assessment of future socio-ecological consequences of land-use policies is useful for supporting decisions about what and where to invest for the best overall environmental and developmental outcomes. However, the task faces a great challenge due to the inherent complexity of coupled human-landscape systems and the long-term perspective required for sustainability assessment. Multi-agent system models have been recognized to be well suited to express the co-evolutions of the human and landscape systems in response to policy interventions. This paper applies the Land Use Dynamics Simulator (LUDAS) framework presented by Le et al. [Ecological Informatics 3 (2008) 135] to a mountain watershed in central Vietnam for supporting the design of land-use policies that enhance environmental and socio-economical benefits in long term. With an exploratory modelling strategy for complex integrated systems, our purpose is to assess relative impacts of policy interventions by measuring the long-term landscape and community divergences (compared with a baseline) driven from the widest plausible range of options for a given policy. Model's tests include empirical verification and validation of sub-models, rational evaluation of coupled model's structure, and behaviour tests using sensitivity/uncertainty analyses. We design experiments of replicated simulations for relevant policy factors in the study region that include (i) forest protection zoning, (ii) agricultural extension and (iii) agrochemical subsidies. As expected, the stronger human-environment interactions the performance indicators involve, the more uncertain the indicators are. Similar to the findings globally summarised by Liu et al. [Science 317 (2007) 1513], time lags between the implementation of land-use policies and the appearance of socio-ecological consequences are observed in our case. Long-term legacies are found in the responses of the total cropping area, farm size and income distribution to changes in forest protection zoning, implying that impact assessment of nature conservation policies on rural livelihoods must be considered in multiple decades. Our comparative assessment of alternative future socio-ecological scenarios shows that it is challenging to attain better either household income or forest conservation by straightforward expanding the current agricultural extensions and subsidy schemes without improving the qualities of the services. The results also suggest that the policy intervention that strengthens the enforcement of forest protection in the critical areas of the watershed and simultaneously create incentives and opportunities for agricultural production in the less critical areas will likely promote forest restoration and community income in long run. We also discuss limitations of the simulation model and recommend future directions for model development. © 2010 Elsevier B.V. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949873882&partnerID=40&md5=02c6edfa8d2d5b511b50ea8d1320b219","It has been proposed that animals and humans might choose a speed-accuracy tradeoff that maximizes reward rate. For this utility function the simple drift-diffusion model of two-alternative forced-choice tasks predicts a parameter-free optimal performance curve that relates normalized decision times to error rates under varying task conditions. However, behavioral data indicate that only ≈ 30 % of subjects achieve optimality, and here we investigate the possibility that, in allowing for uncertainties, subjects might exercise robust strategies instead of optimal ones. We consider two strategies in which robustness is achieved by relinquishing performance: maximin and robust-satisficing. The former supposes maximization of guaranteed performance under a presumed level of uncertainty; the latter assumes that subjects require a critical performance level and maximize the level of uncertainty under which it can be guaranteed. These strategies respectively yield performance curves parameterized by a presumed uncertainty level and required performance. Maximin performance curves for uncertainties in response-to-stimulus interval match data for the lower-scoring 70 % of subjects well, and are more likely to explain it than robust-satisficing or alternative optimal performance curves that emphasize accuracy. For uncertainties in signal-to-noise ratio, neither maximin nor robust-satisficing performance curves adequately describe the data. We discuss implications for decisions under uncertainties, and suggest further behavioral assays. © 2010 Elsevier Inc. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950357192&partnerID=40&md5=d040772a72967a0b47e90e6a15af8bf7","Governments have come under increasing pressure to promote horizontal flows of information across agencies, but investment in cross-agency interoperable and standard systems have been minimally made since it seems to require government agencies to give up the autonomies in managing own systems and its outcomes may be subject to many external and interaction risks. By producing an agent-based model using 'Blanche' software, this study provides policy-makers with a simulation-based demonstration illustrating how government agencies can autonomously and interactively build, standardize, and operate interoperable IT systems in a decentralized environment. This simulation designs an illustrative body of 20 federal agencies and their missions. A multiplicative production function is adopted to model the interdependent effects of heterogeneous systems on joint mission capabilities, and six social network drivers (similarity, reciprocity, centrality, mission priority, interdependencies, and transitivity) are assumed to jointly determine inter-agency system utilization. This exercise simulates five policy alternatives derived from joint implementation of three policy levers (IT investment portfolio, standardization, and inter-agency operation). The simulation results show that modest investments in standard systems improve interoperability remarkably, but that a wide range of untargeted interoperability with lagging operational capabilities improves mission capability less remarkably. Nonetheless, exploratory modeling against the varying parameters for technology, interdependency, and social capital demonstrates that the wide range of untargeted interoperability responds better to uncertain future states and hence reduces the variances of joint mission capabilities. In sum, decentralized and adaptive investments in interoperable and standard systems can enhance joint mission capabilities substantially and robustly without requiring radical changes toward centralized IT management. © JASSS."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-81355125697&partnerID=40&md5=d66506525cef01b182e418e2e051d96e",[No abstract available]
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77249177511&partnerID=40&md5=181e2d99e8209812410197bdc4569c0e","Today, simulation has a broad range of applications from solving service problems to analyzing manufacturing problems such as warehousing and logistic systems. Simulation has become a popular methodology and selecting an appropriate simulation software package is one of the decisions that any industrial engineer may face at work. As a result, numerous types of simulation software packages have been developed for modeling simulation problems. The increasing variety of simulation software packages in the software market makes the selection of an appropriate simulation software package a critical decision. Selecting an inappropriate software package will be followed by many negative consequences. This paper will present a robust decision-making methodology based on Fuzzy Analytical Hierarchy Process (FAHP) for evaluating and selecting the appropriate simulation software package. The robust decision method aggregates the experts' judgments for the criteria weights and the suitability of simulation software alternatives. The FAHP is used to prioritize and evaluate existing alternatives based on the proposed criteria for choosing the proper simulation software. The proposed methodology is applied to selecting the appropriate simulation software as an experiment and results are provided. © 2009 Springer-Verlag London Limited."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77249103506&partnerID=40&md5=e98ea8c1ccb42ceb96b8742bda566e9c","This paper revisits a widely adopted approach to robust decision making developed by Hansen and Sargent (2003, 2008)-henceforth HS-and applies it to monetary policy design in the face of model uncertainty. We pay particular attention to two issues: first, we distinguish three possible forms of the implied game between malign nature and the policymaker in the HS procedure each leading to a different robust and approximating equilibria. Second, we impose the zero lower bound (ZLB) constraint on the nominal interest rate. We show that the ZLB constraint has serious consequences for a policymaker pursuing HS-type robustness, especially when accompanied by an inability to commit. © 2009 Elsevier B.V. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949267653&partnerID=40&md5=7a36dc4dd363d431233d846bd877a3ab","By now, the need for addressing uncertainty in the management of water resources is widely recognized, yet there is little expertise and experience how to effectively deal with uncertainty in practice. Uncertainties in water management practice so far are mostly dealt with intuitively or based on experience. That way decisions can be quickly taken but analytic processes of deliberate reasoning are bypassed. To meet the desire of practitioners for better guidance and tools how to deal with uncertainty more practice-oriented systematic approaches are needed. For that purpose we consider it important to understand how practitioners frame uncertainties. In this paper we present an approach where water managers developed criteria of relevance to understand and address uncertainties. The empirical research took place in the Doñana region of the Guadalquivir estuary in southern Spain making use of the method of card sorting. Through the card sorting exercise a broad range of criteria to make sense of and describe uncertainties was produced by different subgroups, which were then merged into a shared list of criteria. That way framing differences were made explicit and communication on uncertainty and on framing differences was enhanced. In that, the present approach constitutes a first step to enabling reframing and overcoming framing differences, which are important features on the way to robust decision-making. Moreover, the elaborated criteria build a basis for the development of more structured approaches to deal with uncertainties in water management practice. © 2009 Elsevier Ltd. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-76349093369&partnerID=40&md5=11791cd5f07b61c6bec641c0cd247e16","The transfer of genetic material among bacteria in the environment can occur both in the planktonic and attached state. Given the propensity of organisms to exist in sessile microbial communities in oligotrophic subsurface conditions, and that such conditions typify the subsurface, this study focuses on exploratory modeling of horizontal gene transfer among surface-associated Escherichia coli in the subsurface. The mathematics so far used to describe the kinetics of conjugation in biofilms are developed largely from experimental observations of planktonic gene transfer, and are absent of lags or plasmid stability that appear experimentally. We develop a model and experimental system to quantify bacterial filtration and gene transfer in the attached state, on granular porous media. We include attachment kinetics described in Nelson et al. (2007) using the filtration theory approach of Nelson and Ginn (2001, 2005) with motility of E. coli described according to Biondi et al. (1998). © 2009 Elsevier B.V. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885494892&partnerID=40&md5=b451df4e46acfe35f0796b3227adfa74",[No abstract available]
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888289822&partnerID=40&md5=0c905d1c2102048b20cc9db159495009","This chapter addresses three of the central challenges in the literature on informed consent to medical treatment. First, how does the nature of the physician-patient relationship, which establishes the context in which medical decisions are made, influence norms of informed consent for medical care? Second, how do the characteristics of the particular decision at hand affect expectations for informed consent? Finally, how can we bridge the persistent gap between the theory and practice of informed consent, and in particular between the robust decision-making rights accorded to patients and the often limited extent to which patients claim decision-making responsibility for themselves? It is argued that progress in resolving these three questions is possible if we take seriously two features of medical relationships and decisions that to date have been neglected in conceptual discussions about informed consent: the complex fiduciary nature of the physician-patient relationship and the notion that some medical decisions involve choices among, or at least substantially impact upon, important ends, whereas others are best understood as choices among means to a settled end. © 2010 by Oxford University Press, Inc. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-73549103383&partnerID=40&md5=20651e9c4895dae5cbe303b078297cb3",[No abstract available]
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905657026&partnerID=40&md5=8dd7ce4f031805deacedd741d13ac6ca","We demonstrate a method of validating the utility of simpler, more agile models for supporting tactical robust decision making. The key is a focus on the decision space rather than the situation space in decision making under deep uncertainty. Whereas the situation space is characterized by facts about the operational environment, the decision space is characterized by a comparison of the options for action. To visualize the range of options available, we can use computer models to generate the distribution of plausible consequences for each decision option. If we can avoid needless detail in these models, we can save computational time and enable more tactical decision-making, which will in turn contribute to more efficient Information Technology systems. We show how simpler low fidelity, low precision models can be proved to be sufficient to support the decision maker. This is a pioneering application of exploratory modeling to address the human-computer integration requirements of tactical robust decision making."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-72149099073&partnerID=40&md5=113648699b02bbb73e89635feb54084f","Scenarios provide a commonly used and intuitively appealing means to communicate and characterize uncertainty in many decision support applications, but can fall short of their potential especially when used in broad public debates among participants with diverse interests and values. This paper describes a new approach to participatory, computer-assisted scenario development that we call scenario discovery, which aims to address these challenges. The approach defines scenarios as a set of plausible future states of the world that represent vulnerabilities of proposed policies, that is, cases where a policy fails to meet its performance goals. Scenario discovery characterizes such sets by helping users to apply statistical or data-mining algorithms to databases of simulation-model-generated results in order to identify easy-to-interpret combinations of uncertain model input parameters that are highly predictive of these policy-relevant cases. The approach has already proved successful in several high impact policy studies. This paper systematically describes the scenario discovery concept and its implementation, presents statistical tests to evaluate the resulting scenarios, and demonstrates the approach on an example policy problem involving the efficacy of a proposed U.S. renewable energy standard. The paper also describes how scenario discovery appears to address several outstanding challenges faced when applying traditional scenario approaches in contentious public debates. © 2009 Elsevier Inc. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-74849114261&partnerID=40&md5=e4361ec890570edbc89753e9be83e276","The diagnostic capability of using positive and negative predictive power in dynamic and robust decision making paradigms is explored. Several mathematical formulations are discussed to employ this concept to assist in decision making when human-machine systems may have characteristics that change with time. ©2009 IEEE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955696502&partnerID=40&md5=44b83bf08105cd5b324d8c65cd132300","Throughout the last two decades many attempts took place in order policy makers and researchers to be able to measure the energy security of supply of a particular country, region and corridor. This chapter is providing an overview presentation of the Energy Security Risk Assessment System (E.S.R.A.S.) which comprises the Module of Robust Decision Making (RDM) and the Module of Energy Security Indices Calculation (ESIC). Module 1 & 2 are briefly presented throughout section 2 and the application of Module 2 in nine case study countries is discussed at section 3. Finally, in the last section are the conclusions, which summarize the main points, arisen in this chapter. © 2010, IGI Global."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953601592&partnerID=40&md5=f8b25ce316e2f469dfe2e4f0c92538b2","In our societies, sociotechnical expertise has long drawn its legitimacy only from the conception of the ""savant"" (a form of vertical and unilateral communication as expressed by the ""standard"" model), before expanding to a mode where ""savants"" and ""lay- people"" are in equal dialogue (a form of horizontal and reciprocal communication, as expressed in the model of ""co-construction""). This article explores the conditions for a satisfactory function of the co-construction model of sociotechnical expertise in our current societies. I will first show a parallel between the succession of types of legitimate discourse in sociotechnical matters in our societies and the evolution of the type of discourse considered as politically legitimate in ancient Greece. I then analyze how the parrêsía and the co-construction of sociotechnical expertise are realized within the framework of ""hybrid forums"" in the university. Instead of the pseudo-modular system that currently instantiates expertise in the university, I recommend a configuration of modules, where the redundancy is in the epistemological postures taken by the actors, whether in or out of academia, rather than a configuration of disciplinary modules, solely consisting of academic actors whose epistemological postures are often quite varied. This configuration would have the advantage of enabling both, modular and architectural innovations. The interactions between such modules would result in a learning process in the form of reciprocal translations, giving rise to multiple cross-fertilizations. This learning process might lead to a sort of cognitive grammar common to all actors, laying the foundation for collectively negotiated, and therefore robust, decision making."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77749271055&partnerID=40&md5=3e279f8777fcf5c7acc26c3bb63ba97f","Enabling ordinary people to create high-quality 3D models is a long-standing problem in computer graphics. In this work, we draw from the literature on design and human cognition to better understand the design processes of novice and casual modelers, whose goals and motivations are often distinct from those of professional artists. The result is a method for creating exploratory modeling tools, which are appropriate for casual users who may lack rigidly-specified goals or operational knowledge of modeling techniques. Our method is based on parametric design spaces, which are often high dimensional and contain wide quality variations. Our system estimates the distribution of good models in a space by tracking the modeling activity of a distributed community of users. These estimates drive intuitive modeling tools, creating a self-reinforcing system that becomes easier to use as more people participate. We present empirical evidence that the tools developed with our method allow rapid creation of complex, high-quality 3D models by users with no specialized modeling skills or experience. We report analyses of usage patterns garnered throughout the year-long deployment of one such tool, and demonstrate the generality of the method by applying it to several design spaces. © 2009 ACM."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449858780&partnerID=40&md5=27e6b2c9d6793d431257be2f21bc7612","This paper presents a novel time-frequency based pattern recognition technique for an intelligent cricket decision making system using Snickoanalysis. A snickometer is used to detect and analyze the weak audio signals coming from bat and ball contact for better decision making system in cricket match. However, the audio signals detected using snickometer needs to be used along with slow motion video for the offline analysis to make a decision. This method has limitations in analyzing the multiple sounds leading to wrong decision. To overcome the limitations of snickometer analysis and to improve the accuracy of decision making system in cricket a novel time-frequency based pattern recognition technique was proposed for an effective cricket decision making system using snickosignals. Experiments were conducted to simulate and record snickometer signals using PC micro-phone with the help of RAVEN LITE software. A simple time-frequency based pattern recognition technique was developed and tested. The performance of the proposed technique is found be satisfactory and suitable for the development of robust decision making system using Snickoanalysis. © 2009 IEEE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349956418&partnerID=40&md5=2920d57f26f7d47e3f1c79a38fcbe026","Application of a companywide environmental management system (EMS) and guidelines is a best practice for environmental performance assurance in the oil and gas industry because it helps to institutionalize environmental protection efforts. While implementation of these systems and guidelines is a very positive step toward promoting sustainability and environmental stewardship, it sometimes fall short of fulfilling this goal with regards to designing and developing the most sustainable new projects possible. A key to this shortcoming is the alternatives analysis element of the Environmental Impact Assessment (EIA) process. The EIA process is typically a company's first in-depth look at environmental components of a project and proper application can make the difference in defming the sustainability of development projects. EIA is most valuable when used not only to help identify, predict, evaluate and mitigate potential impacts of proposed developments upstream in the decision-making process, but to support a robust decision-making process about the basic design and scope of the project. Environmental and social impacts of projects tend to be readily predictable, but unfortunately not readily quantifiable. Therefore, decision making tools used in alternatives analysis may not adequately reflect the external environmental and social costs associated with a given project. For this reason, economic and technological considerations are often unintentionally prioritized in the decision making process and become the true driving force of early project decision making. Environmental and social concerns are less often addressed up front, and are more often addressed in the EIA's Environmental Management Plan (EMP) where they are managed over the life of the project and sometimes beyond. While EMPs are vital to minimizing and mitigating environmental and social impacts, there are risks to allowing environmental and social concerns to be addressed only after most technical and design planning has been largely finalized. This paper will demonstrate the importance of addressing environmental and social considerations through ETA alternatives analysis early in the project decision-making process, review cases in which this has or has not been accomplished and review tools that can be used by decision makers to support alternatives analysis in EIA. The tools explored, sustainability appraisal, true cost accounting, scenario planning and modeling software, take into consideration the true costs of environmental and social impacts. Their application in EIA alternatives analysis can help assure more sustainable projects. Copyright 2009. Society of Petroleum Engineers."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449931818&partnerID=40&md5=4cfde941feddceaaef16a7b16f4e4be4","A more robust decision making procedure must be established to best prepare for unknown severity and frequency of hurricanes. © 2009 American Chemical Society."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349631880&partnerID=40&md5=6bfe45780efffe47f405295b1bfa1902","Road safety is a policy priority due to the high casualties and costs associated with road accidents. Since speed is a major cause of road accidents, in-vehicle speed limiters or Intelligent Speed Adaptation (ISA), seems a promising solution. ISA implementation, however, is hindered by large uncertainties, for example about the impacts of ISA, the way users might respond to ISA, and the relationship between speed and accidents. Traditional Multi-Criteria Analysis (MCA) has limitations in handling these uncertainties. We present an MCA approach based on exploratory modeling, which uses computational experiments to explore the multiple outcomes of ISA policies (safety, emissions, throughput, and cost) across a range of future demand scenarios, functional relationships for performance criteria, and user responses to ISA. As an illustration, by testing the impacts of different ISA penetration levels on two driver groups, we show that when compliance with ISA is expected to be low, a policy aimed only at novice drivers outperforms other ISA policies on safety improvement."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349204503&partnerID=40&md5=fadad4c4b2c6cb6acb98d641fa6c3b3c","Purpose: The purpose of this paper is to propose a manufacturing product-screening methodology that will require minimal resource expenditures as well as succinct improvement tools based on multi-response prioritisation. Design/methodology/approach: A six-step methodology is overviewed that relies on the sampling efficiency of fractional factorial designs introduced and recommended by Dr G. Taguchi. Moreover, the multi-response optimisation approach based on the super-ranking concept is expanded to the more pragmatic situation where prioritising of the implicated responses is imperative. Theoretical developments address the on-going research issue of saturated and nreplicated fractional-factorial designs. The methodology promotes the ""user- friendly"" incorporation of assigned preference weights on the studied responses. Test efficiency is improved by concise rank ordering. This technique is accomplished by adopting the powerful rank-sum inference method of Wilcoxon-Mann-Whitney. Findings: Two real-life case studies complement the proposed technique. The first discusses a production problem on manufacturing disposable shavers. Injection moulding data for factors such as handle weight, two associated critical handle dimensions and a single mechanical property undergo preferential multi-response improvement based on working specification standards. This case shows that regardless of fluctuations incurred by four different sources of response prioritisation, only injection speed endures high-statistical significance for all four cases out of the seven considered production factors. Similarly, the technique identifies a single active factor in a foil manufacturing optimisation of three traits among seven examined effects. Originality/value: This investigation suggests a technique that targets the needs of manufacturing managers and engineers for ""quick-and- robust"" decision making in preferential product improvement. This is achieved by conjoining orthogonal arrays with a well-established non-parametric comparison test. A version of the super-ranking concept is adapted for the weighted multi-response optimisation case. © Emerald Group Publishing Limited."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349729861&partnerID=40&md5=1107de78ea2b53827592719f7b5e82c9","This article provides a concise overview of methods for analyzing policy choices that have been used in the study of long-term environmental challenges. We open with an overview of the broad classes of methods used for long-term policy analysis, and subsequent sections will describe in depth three particular methods. They are: statistical models, such as employed in the debate on the environmental Kuznets curve, which infer past patterns from data and project them into the future; robust decision-making, a decision analytic framework that supports choices under deep uncertainty, and relates near-term policy interventions to different clusters of long-term environmental futures; and adaptive control and agent-based modeling, which provide an approach to simulation modeling that focuses on cooperation and conflict among multiple actors and their choice of strategies. While all three approaches can be used for various applications, this article focuses on the challenge of a potential transition to a low-carbon future to illustrate the strengths, weaknesses, and synergies among the respective methods. In the final section, we offer guidance for choosing among methods. © 2009 by the Massachusetts Institute of Technology."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-67349218863&partnerID=40&md5=7f26f2059ac0d873dbc4fa08ceb70356","The diagnosis of diabetes, based on measured fasting plasma glucose level, depends on choosing a threshold level for which the probability of failing to detect disease (missed diagnosis), as well as the probability of falsely diagnosing disease (false alarm), are both small. The Bayesian risk provides a tool for aggregating and evaluating the risks of missed diagnosis and false alarm. However, the underlying probability distributions are uncertain, which makes the choice of the decision threshold difficult. We discuss an hypothesis for choosing the threshold that can robustly achieve acceptable risk. Our analysis is based on info-gap decision theory, which is a non-probabilistic methodology for modelling and managing uncertainty. Our hypothesis is that the non-probabilistic method of info-gap robust decision making is able to select decision thresholds according to their probability of success. This hypothesis is motivated by the relationship between info-gap robustness and the probability of success, which has been observed in other disciplines (biology and economics). If true, it provides a valuable clinical tool, enabling the clinician to make reliable diagnostic decisions in the absence of extensive probabilistic information. Specifically, the hypothesis asserts that the physician is able to choose a diagnostic threshold that maximizes the probability of acceptably small Bayesian risk, without requiring accurate knowledge of the underlying probability distributions. The actual value of the Bayesian risk remains uncertain. © 2009 Elsevier Ltd. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-68249114454&partnerID=40&md5=20ac427e768505c0751669cfb8082db5","In order to solve the problem of exploring scenario space faced by robust decision-making, an exploratory analysis experimental design was introduced for supporting robust decision-making. First of all, a scenario space generation framework, PUCM framework for short was established, and its construction was analyzed. After that, the scenario generation and scenario analysis were integrated into one framework, named scenario exploratory framework, which is a formalization analysis approach for scenario generation and analysis. Finally, a hierarchical experimental design technology was proposed, which could integrate the analysis ability of human and the computing capacity of computers, to deal with the issues in the experiments based on exploratory analysis, and introduce its logic process to guide the interactive experiments for identifying robust strategies."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-61449152039&partnerID=40&md5=76c5b816b2971c2cc6b9d6a7fc6c7f6b","Recently in inventory management instead of maximizing expected profit or minimizing expected cost risk-averse objective functions have been used for determining the optimal order quantity. We use the well-known newsvendor model to determine the optimal order quantity for an objective function with two risk parameters, which can describe risk-neutral, risk-averse as well as risk-taking behaviour of the inventory manager. This approach can also be applied to situations in which the demand distribution cannot be specified uniquely. We consider robust optimization procedures-maximin and minimax regret-to determine optimal order quantities if the set of potential demand variables can be partially ordered by stochastic dominance rules. © 2008 Elsevier B.V. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890618828&partnerID=40&md5=0e4e485f647f07ecf1aea1ca42bd5433","Recognizing that robust decision making is vital in risk management, this book provides concepts and algorithms for computing the best decision in view of the worst-case scenario. The main tool used is minimax, which ensures robust policies with guaranteed optimal performance that will improve further if the worst case is not realized. The applications considered are drawn from finance, but the design and algorithms presented are equally applicable to problems of economic policy, engineering design, and other areas of decision making. Critically, worst-case design addresses not only Armageddon-type uncertainty. Indeed, the determination of the worst case becomes nontrivial when faced with numerous--possibly infinite--and reasonably likely rival scenarios. Optimality does not depend on any single scenario but on all the scenarios under consideration. Worst-case optimal decisions provide guaranteed optimal performance for systems operating within the specified scenario range indicating the uncertainty. The noninferiority of minimax solutions--which also offer the possibility of multiple maxima--ensures this optimality. Worst-case design is not intended to necessarily replace expected value optimization when the underlying uncertainty is stochastic. However, wise decision making requires the justification of policies based on expected value optimization in view of the worst-case scenario. Conversely, the cost of the assured performance provided by robust worst-case decision making needs to be evaluated relative to optimal expected values. Written for postgraduate students and researchers engaged in optimization, engineering design, economics, and finance, this book will also be invaluable to practitioners in risk management. © 2002 by Princeton University Press. All Rights Reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84993708274&partnerID=40&md5=50704514287efcf7d147fb26b50c9f92","We provide a review and analysis of much of the published literature on decision making that is relevant to the design of immersive environments. This review draws from the basic and applied literature in order to provide insight for the design of such synthetic environments. Included in this review are articles and books cited in other works, and articles and books obtained from an Internet search. Issues discussed are (a) an overview of immersive decision environments; (b) dual-process decision making; (c) training robust intuitive decision making; (d) combining analytical and intuitive processing in immersive environments; and (e) concluding remarks. For the development of robust decision making in immersive environments, intuitive reasoning should be emphasized by creating an immersive situation and by providing for the development of automatic processing through implicit learning, with the latter reinforced by explicit thought processes. Considerations of the literature on decision making will provide insight for future design solutions for immersive decision environments. © 2009, Human Factors and Ergonomics Society. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905592116&partnerID=40&md5=a20ea784ac0ead2f56c17a9db0a62449","We are investigating decision aids that present potential courses of action available to emergency responders. To determine whether these aids improve decision quality, however, we first developed test scenarios that were challenging in well-understood ways to ensure testing under the full breadth of representative decision-making situations. We devised a three-step method of developing scenarios: define the decision space, determine the cost components of each decision's potential consequences based on the principles of Robust Decision Making, then choose conflicting pairs of cost components (e.g., a small fire, implying low property damage, in a densely inhabited area, which implies high personal injury). In a validation of this approach, experiment participants made decisions faster in non-ambiguous cases versus cases that included this principled introduction of ambiguity. Our Principled Ambiguity Method of scenario design is also appropriate for other domains as long as they can be analyzed in terms of costs of decision alternatives."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-58149231456&partnerID=40&md5=19b6a900d6efb87433a5458bda924e7e","Stochastic programming (SP) brings together models of optimum resource allocation and models of randomness and thereby creates a robust decision-making framework. The models of randomness with their finite, discrete realizations are known as scenario generators. In this report, we consider alternative approaches to scenario generation in a generic form which can be used to formulate (a) two-stage (static) and (b) multi-stage dynamic SP models. We also investigate the modelling structure and software issues of integrating a scenario generator with an optimization model to construct SP recourse problems. We consider how the expected value and SP decision model results can be evaluated within a descriptive modelling framework of simulation. Illustrative examples and computational results are given in support of our investigation."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-59149091411&partnerID=40&md5=0c9d62ae95ba5c940ba71fc090acc38f","All realistic Multi Criteria Decision Making (MCDM) problems in water resources management face various kinds of uncertainty. In this study the evaluations of the alternatives with respect to the criteria will be assumed to be stochastic. Fuzzy linguistic quantifiers will be used to obtain the uncertain optimism degree of the Decision Maker (DM). A new approach for stochastic-fuzzy modeling of MCDM problems will be then introduced by merging the stochastic and fuzzy approaches into the Ordered Weighted Averaging (OWA) operator. The results of the new approach, entitled SFOWA, give the expected value and the variance of the combined goodness measure for each alternative, which are essential for robust decision making. In order to combine these two characteristics, a composite goodness measure will be defined. By using this measure the model will give more sensitive decisions to the stakeholders whose optimism degrees are different than that of the decision maker. The methodology will be illustrated by using a water resources management problem in the Central Tisza River in Hungary. Finally, SFOWA will be compared to other methods known from the literature to show its suitability for MCDM problems under uncertainty. © Springer-Verlag 2008."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-54049101718&partnerID=40&md5=48f2f568a177fcdf7c55168baf235df4","Enterprise resource planning (ERP) systems have gained major prominence by enabling companies to streamline their operations, leverage and integrate business data process. In order to implement an ERP project successfully, it is necessary to select an ERP system which can be aligned with the needs of the company. Thus, a robust decision making approach for ERP software selection requires both company needs and characteristics of the ERP system and their interactions to be taken into account. This paper develops a novel decision framework for ERP software selection based on quality function deployment (QFD), fuzzy linear regression and zero-one goal programming. The proposed framework enables both company demands and ERP system characteristics to be considered, and provides the means for incorporating not only the relationships between company demands and ERP system characteristics but also the interactions between ERP system characteristics through adopting the QFD principles. The presented methodology appears as a sound investment decision making tool for ERP systems as well as other information systems. The potential use of the proposed decision framework is illustrated through an application. © 2007 Elsevier Ltd. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-67349141790&partnerID=40&md5=8d36b4221781d3d8633383041c99f62d","An exploratory modeling approach to investigate spatial variation in the levels of regional endogenous employment growth and decline over the decade 1991-2001 is developed and applied to an analysis of the non-metropolitan regions (Local Government Areas) in each of the five mainland States of Australia. For the dependent variable, the summation of the regional shift component for change in total employment in major industry sectors1 over the decade 1991-2001, standardized by the size of the labor force at the beginning of the period, is used as a proxy measure of regional endogenous growth. A general OLS model incorporating a set of 27 independent variables (measuring aspects of industry structure, unemployment, occupational structure, population size and growth, human capital, income distribution, and proximity to the coast and the state metropolitan region) is run, followed by a backward iterative statistical procedure to reduce the complexity of the general model by eliminating statistically insignificant variables to arrive at a specific model for each State. © Springer-Verlag 2008."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-58049129060&partnerID=40&md5=2542d657c8dad77db3b7a6cd0c1c5fa9","Hierarchical and wider applications of robots, manipulators, and pick and place machines are facing challenges in industrial environments due to their insufficient intelligence for appropriately recognizing objects for grasping and handling purposes. Since robots do not posses self-consciousness, estimation of adequate grasping force for individual objects by robots or manipulators is another challenge for wider applications of robots and manipulators. This article suggests a mathematical model, recently developed, for computation of scattered energy of vibrations sensed by the stylus during an object slippage in robot grippers. The model includes in it dynamic parameters like trial grasping force, object falling velocity, and geometry of object surface irregularities. It is envisaged that using the said mathematical model, with the help of robust decision making capabilities of artificial neural network (NN), a robot memory could be able to estimate appropriate/optimal grasping force for an object considering its physiomechanical properties. On the basis of above mentioned mathematical model, this article demonstrates an experimental methodology of estimating adequate grasping forces of an object by robot grippers using Backpropagation (BP) neural networks. Four different algorithms have been explored to experiment the optimal grasping force estimation. © 2008 IEEE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-57149129963&partnerID=40&md5=ed39a356de4753a6c4326106bb6d61b5","Public acceptability is widely acknowledged as a key factor in the success of indirect potable water reuse (IPR) schemes. Social research has provided useful insights into the factors that influence public attitudes to IPR and guidelines for engaging the public. Recent IPR developments in Australia demonstrate that clear democratic processes for decision making are yet to be devised. The distinction between technology and society which underpins work in this field does not adequately reflect the nature of IPR and limits possibilities for more robust decision making processes. IPR is not simply a technology to be accepted or rejected by society. IPR is a complex socio-technology which cannot exist unless its technologies become embedded in social, institutional, infrastructural and ecological networks. Reconsidering IPR as a complex sociotechnology provides a new grounding for devising processes and institutions for decision making. Based on the formulations of Bruno Latour for bringing the sciences into democracy such processes will include four main tasks: 1) Perplexity - identifying propositions to be taken into account; 2) Consultation - evaluating the strength of the propositions; 3) Hierarchy - ordering the propositions into relative merit and importance; and 4) Institution - stabilising the outcomes through appropriate institutions. © IWA Publishing 2008."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861482420&partnerID=40&md5=aef9cff5c552898131306be60d475433","The interdependency between information requirements and lead-time reduction is a challenging issue usually addressed by collaborative and concurrent design processes. The combination of temporal issue, multi-disciplinary teams, task parallelization and fuzzy system boundaries introduces additional risks due to the need to provide and receive uncertain information. Engineers often face uncertainty. They adopt a tacit approach in assessing the quality and the maturity of information. An appreciation of both uncertainty and information maturity can influence task execution and the way tasks are solved. This paper discusses a framework that may aid robust decision-making and help in achieving smooth progress in the collaborative and concurrent engineering activities in the presence of uncertainty."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-58149117935&partnerID=40&md5=47ffb122491ed23dd7cf713763fa50e6","Commutative monoids of belief states have been defined by imposing one or more of the usual axioms and assigning a combination rule. Familiar operations such as normalization and the Voorbraak map are surjective homomorphisms. The latter map takes values in a monoid of Bayesian states. The pignistic map is not a monoid homomorphism. This can impact robust decision making for frames of cardinality at least 3. We adapt the concept of measure zero reflecting functions between probability spaces to define a category P0R having belief states as objects and plausibility zero reflecting functions as morphisms. This definition encapsulates a generalization of the notion of absolute continuity to the context of belief spaces. We show that the Voorbraak map induces a functor valued in P0R that is right adjoint to the embedding of Bayesian states. © 2008 Springer-Verlag Berlin Heidelberg."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-54349084124&partnerID=40&md5=9a674aec0d013466dc9e95ce7722700c","1. In conservation decision-making, we operate within the confines of limited funding. Furthermore, we often assume particular relationships between management impact and our investment in management. The structure of these relationships, however, is rarely known with certainty - there is model uncertainty. We investigate how these two fundamentally limiting factors in conservation management, money and knowledge, impact optimal decision-making. 2. We use information-gap decision theory to find strategies for maximizing the number of extant subpopulations of a threatened species that are most immune to failure due to model uncertainty. We thus find a robust framework for exploring optimal decision-making. 3. The performance of every strategy decreases as model uncertainty increases. 4. The strategy most robust to model uncertainty depends not only on what performance is perceived to be acceptable but also on available funding and the time horizon over which extinction is considered. 5. Synthesis and applications. We investigate the impact of model uncertainty on robust decision-making in conservation and how this is affected by available conservation funding. We show that subpopulation triage can be a natural consequence of robust decision-making. We highlight the need for managers to consider triage not as merely giving up, but as a tool for ensuring species persistence in light of the urgency of most conservation requirements, uncertainty and the poor state of conservation funding. We illustrate this theory by a specific application to allocation of funding to reduce poaching impact on the Sumatran tiger Panthera tigris sumatrae in Kerinci Seblat National Park. © 2008 The Authors."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-58149138808&partnerID=40&md5=a5c94d946e0d8eb12a5bccc1fd198f2c","This paper presents results related to the development of activities of applied exploratory modelling to Quantum Physics Teaching by using use of the learning object (LO) called the Quantum Duck. The LO is a metaphor to the photoelectric erect and it makes possible the calculation of the constant of Planck. A study was done through an experiment with High School students. As a result, it was noticed that the construction or manipulation of a model does not depend exclusively of the assimilation of the employed logic in the tool computacional, but of the understanding of the the physical phenomenon and their abilities in relating it to the aim of the developed activity."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-60749084411&partnerID=40&md5=46651bbcb92e9147e052b2de514a1b69","Uncertainty associated with input parameters and models in simulation has gained attentions in recent years. The sources of uncertainties include lack of data and lack of knowledge about physical systems. In this paper, we present a new reliable simulation mechanism to help improve simulation robustness when significant uncertainties exist. The new mechanism incorporates variabilities and uncertainties based on imprecise probabilities, where the statistical distribution parameters in the simulation are intervals instead of precise real numbers. The mechanism generates random interval variates to model the inputs. Interval arithmetic is applied to simulate a set of scenarios simultaneously in each simulation run. To ensure that the interval results bound those from the traditional real-valued simulation, a generic approach is also proposed to specify the number of replications in order to achieve the desired robustness. This new reliable simulation mechanism can be applied to address input uncertainties to support robust decision making.©2008 IEEE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952396873&partnerID=40&md5=f04514f24f5f6705841b14f80d01fbb1","Each year traffic in Europe claims more than 40,000 fatalities, of which one third are due to inappropriate speed. This makes increasing traffic safety and reducing speeding important policy objectives for public policymakers. A possible measure to help achieve these objectives is implementing Intelligent Speed Adaptation (ISA), an in-vehicle device that assists a driver in maintaining the appropriate speed. Past research in this field focuses on estimating ISA's safety benefits, as well as the impact of ISA on other criteria, such as emissions, throughput, and travel time. Coming up with a robust policy, however, also requires insights into the trade-offs the various stakeholders make with respect to these criteria. In this paper, we show how exploratory modeling in a multi-criteria multi-stakeholder context can support developing robust policies for implementing ISA."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858308718&partnerID=40&md5=ce991ec6d56983e64f2c0a3077e70349","The European Water Framework Directive (WFD) sets out an integrated perspective to water management in river catchments and river basin districts and is a key driver in the movement towards Integrated River Basin Management. Integrated river basin management must deliver objectives related to the WFD in the wider context of various other stakeholder interests, for example related to flooding, water resources, employment and cost. In managing such complex systems, a specific objective can be achieved through different management actions. Likewise, a specific management action can have implications for multiple objectives. Synergies or conflicts between specific objectives and between specific actions are likely to occur, and need careful consideration in order to increase the efficiency of planned management actions. However, such integrated decision making is a very difficult and highly complex task, which cannot easily be accomplished by either single or groups of planners. Integrated modelling tools to facilitate and enhance communication within a group of decision-makers and inform a more objective and evidence-based multi-criteria decision-making process are required. The scope for the development of such an integrated tool is being tested by the Catchment Science Centre (CSC) at The University of Sheffield. The CSC and the Environment Agency are jointly developing a tool termed the Macro-Ecological Model (MEM). The MEM is developed as a consistent framework for the integration of knowledge and information about environmental, social and economic processes and process-interactions that are affected by management actions and have impacts on multiple management objectives. The MEM enables knowledge from various different resources to be integrated, including empirical data, model results and even expert knowledge using a Bayesian Belief Network (BBN) approach. BBNs have the advantage of representing system understanding in an intuitive, graphical format. Furthermore, the approach provides the ability to explicitly account for uncertainties in model predictions. Therefore, the model framework provides a good tool for visualising system understanding and communicating uncertainties. Applied in a participatory process, it can support robust decision making in river basin management. The conceptual model framework is illustrated with examples from the prototyping study. The prototype model captures the process interactions affecting the management objectives ""Ecological Status"" (composed of both Biological Quality and Physico-chemical Quality) and ""Flood Risk"". It is planned to be later extended to incorporate further environmental, and also social and economic objectives."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-56049108588&partnerID=40&md5=d14f29e1889bb4d06000a383d04055ee","Nonparametric regression models are proposed in the framework of ecological inference for exploratory modeling of disease prevalence rates adjusted for variables, such as age, ethnicity/race, and socio-economic status. Ecological inference is needed when a response variable and covariate are not available at the subject level because only summary statistics are available for the reporting unit, for example, in the form of R x C tables. In this article, only the marginal counts are assumed available in the sample of R x C contingency tables for modeling the joint distribution of counts. A general form for the ecological regression model is proposed, whereby certain covariates are included as a varying coefficient regression model, whereas others are included as a functional linear model. The nonparametric regression curves are modeled as splines fit by penalized weighted least squares. A data-driven selection of the smoothing parameter is proposed using the pointwise maximum squared bias computed from averaging kernels (explained by O'Sullivan, 1986, Statistical Science 1, 502-517). Analytic expressions for bias and variance are provided that could be used to study the rates of convergence of the estimators. Instead, this article focuses on demonstrating the utility of the estimators in a study of disparity in health outcomes by ethnicity/race. © 2008, The International Biometric Society."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-55849095132&partnerID=40&md5=e7f5b4e45457270ddcbb90d7e85bcad5","The problems of modelling hydrological processes have long been recognised. We have significant uncertainty in the measurements of basic hydrological quantities (catchment rainfalls, discharges at a point, actual evapotranspiration from catchments); we have cost limitations on many types of measurements which means that information content is compromised (particularly for water quality and tracer information); we have measurements that are often only indirectly related to hydrological quantities of interest (particularly remote sensing measurements); we have a mismatch between the scale of measurement techniques and the scale at which we need to apply models for management purposes; and we have a mismatch between theory developed from small scale measurements and the scale at which that theory is applied. We can be hopeful that some of these constraints will be relaxed in the future, but it seems rather unlikely that they will cease to be constraints. The result is that the representation of hydrological processes and the predictions of hydrological models will remain uncertain. There are consequently two important issues that must be addressed in future hydrological science. The first is how to estimate that uncertainty (in both gauged and ungauged catchments) and constrain it using the most informative measurements. The second is how to present and use the uncertainty in management decisions. Solution of these issues will require a framework for spanning scales, for assessing the real information content of observations in testing models as hypotheses about system response, and for robust decision making under uncertainty. It is suggested that this framework will actually develop naturally as part of a learning process as ""models of everywhere"" become available. But this also focuses attention back on the information content of different types of data. This question is fundamental to the future of hydrological science. Copyright © 2008 IAHS Press."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-55349085476&partnerID=40&md5=60e0650dbda56e89b298dedc75659c4b","In the relatively new field of visual analytics there is a great need for automated approaches to both verify and discover the intentions and schemes of primary actors through time. Data mining and knowledge discovery play critical roles in facilitating the ability to extract meaningful information from large and complex textual-based (digital) collections. In this study, we develop a mathematical strategy based on nonnegative tensor factorization (NTF) to extract and sequence important activities and specific events from sources such as news articles. The ability to automatically reconstruct a plot or confirm involvement in a questionable activity is greatly facilitated by our approach. As a variant of the PARAFAC multidimensional data model, we apply our NTF algorithm to the terrorism-based scenarios of the VAST 2007 Contest data set to demonstrate how term-by-entity associations can be used for scenario/plot discovery and evaluation. © 2008 Springer-Verlag Berlin Heidelberg."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-49249098476&partnerID=40&md5=d854f8f571f3ef9c58d4c9253ec40a13","A wide range of modelling algorithms is used by ecologists, conservation practitioners, and others to predict species ranges from point locality data. Unfortunately, the amount of data available is limited for many taxa and regions, making it essential to quantify the sensitivity of these algorithms to sample size. This is the first study to address this need by rigorously evaluating a broad suite of algorithms with independent presence-absence data from multiple species and regions. We evaluated predictions from 12 algorithms for 46 species (from six different regions of the world) at three sample sizes (100, 30, and 10 records). We used data from natural history collections to run the models, and evaluated the quality of model predictions with area under the receiver operating characteristic curve (AUC). With decreasing sample size, model accuracy decreased and variability increased across species and between models. Novel modelling methods that incorporate both interactions between predictor variables and complex response shapes (i.e. GBM, MARS-INT, BRUTO) performed better than most methods at large sample sizes but not at the smallest sample sizes. Other algorithms were much less sensitive to sample size, including an algorithm based on maximum entropy (MAXENT) that had among the best predictive power across all sample sizes. Relative to other algorithms, a distance metric algorithm (DOMAIN) and a genetic algorithm (OM-GARP) had intermediate performance at the largest sample size and among the best performance at the lowest sample size. No algorithm predicted consistently well with small sample size (n < 30) and this should encourage highly conservative use of predictions based on small sample size and restrict their use to exploratory modelling. © 2008 The Authors."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-48649092005&partnerID=40&md5=754068b0d054a61fef28ba7c026b75dd","Future catchment planning requires a good understanding of the impacts of land use and management, especially with regard to nutrient pollution. A range of readily usable tools, including models, can play a critical role in underpinning robust decision-making. Modelling tools must articulate our process understanding, make links to a range of catchment characteristics and scales and have the capability to reflect future land-use management changes. Hence, the model application can play an important part in giving confidence to policy makers that positive outcomes will arise from any proposed land-use changes. Here, a minimum information requirement (MIR) modelling approach is presented that creates simple, parsimonious models based on more complex physically based models, which makes the model more appropriate to catchment-scale applications. This paper shows three separate MIR models that represent flow, nitrate losses and phosphorus losses. These models are integrated into a single catchment model (TOPCAT-NP), which has the advantage that certain model components (such as soil type and flow paths) are shared by all three MIR models. The integrated model can simulate a number of land-use activities that relate to typical land-use management practices. The modelling process also gives insight into the seasonal and event nature of nutrient losses exhibited at a range of catchment scales. Three case studies are presented to reflect the range of applicability of the model. The three studies show how different runoff and nutrient loss regimes in different soil/geological and global locations can be simulated using the same model. The first case study models intense agricultural land uses in Denmark (Gjern, 114 km2), the second is an intense agricultural area dominated by high superphosphate applications in Australia (Ellen Brook, 66 km2) and the third is a small research-scale catchment in the UK (Bollington Hall, 2 km2). Copyright © 2007 John Wiley &amp; Sons, Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-44349179867&partnerID=40&md5=91a6244af9d3b23467222998c23ac9d5","In this article, Information-Gap Decision Theory (IGDT), an approach to robust decision making under severe uncertainty, is applied to decisions about a remanufacturing process. IGDT is useful when only a nominal estimate is available for an uncertain quantity; the amount that estimate differs from the quantity's actual value is not known. The decision strategy in IGDT involves maximizing robustness to uncertainty of unknown size, while still guaranteeing no worse than some ""good enough"" critical level of performance, rather than optimal performance. The design scenario presented involves selecting the types of technologies and number of stations to be used in a remanufacturing process. The profitability of the process is affected by severe uncertainty in the demand for remanufactured parts. Because nothing is know about demand except an estimate based on a different product from a previous year, info-gap theory will be used to determine an appropriate tradeoff between performance and robustness to severe uncertainty. Which design is most preferred is seen to switch depending on choice of critical performance level. Implications of findings, as well as future research directions, are discussed. Copyright © 2007 by ASME."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-42449139639&partnerID=40&md5=e72c3cb1dd1900437fefc45e8206325d","This paper presents a three-stage optimization algorithm for solving two-stage deviation robust decision making problems under uncertainty. The structure of the first-stage problem is a mixed integer linear program and the structure of the second-stage problem is a linear program. Each uncertain model parameter can independently take its value from a real compact interval with unknown probability distribution. The algorithm coordinates three mathematical programming formulations to iteratively solve the overall problem. This paper provides the application of the algorithm on the robust facility location problem and a counterexample illustrating the insufficiency of the solution obtained by considering only a finite number of scenarios generated by the endpoints of all intervals. © 2007 Springer Science+Business Media, LLC."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-43249124624&partnerID=40&md5=bed852107b8b90f5296b3b6e238770e4","This paper examines apparent tensions between ""science-based,"" ""precautionary,"" and ""participatory"" approaches to decision making on risk. Partly by reference to insights currently emerging in evolutionary studies, the present paper looks for ways to reconcile some of the contradictions. First, I argue that technological evolution is a much more plural and open-ended process than is conventionally supposed. Risk politics is thus implicitly as much about social choice of technological pathways as narrow issues of safety. Second, it is shown how conventional ""science-based"" risk assessment techniques address only limited aspects of incomplete knowledge in complex, dynamic, evolutionary processes. Together, these understandings open the door to more sophisticated, comprehensive, rational, and robust decision-making processes. Despite their own limitations, it is found that precautionary and participatory approaches help to address these needs. A concrete framework is outlined through which the synergies can be more effectively harnessed. By this means, we can hope simultaneously to improve scientific rigor and democratic legitimacy in risk governance. © 2008 New York Academy of Sciences."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-38849088066&partnerID=40&md5=c49a401f10271b92cf16fd1237856b14","Over the last few decades it has become increasingly obvious that disturbance, whether natural or anthropogenic in origin, is ubiquitous in ecosystems. Disturbance-related processes are now considered to be important determinants of the composition, structure and function of ecological systems. However, because disturbance and succession processes occur across a wide range of spatio-temporal scales their empirical investigation is difficult. To counter these difficulties much use has been made of spatial modelling to explore the response of ecological systems to disturbance(s) occurring at spatial scales from the individual to the landscape and above, and temporal scales from minutes to centuries. Here we consider such models by contrasting two alternative motivations for their development and use: prediction and exploration, with a focus on forested ecosystems. We consider the two approaches to be complementary rather than competing. Predictive modelling aims to combine knowledge (understanding and data) with the goal of predicting system dynamics; conversely, exploratory models focus on developing understanding in systems where uncertainty is high. Examples of exploratory modelling include model-based explorations of generic issues of criticality in ecological systems, whereas predictive models tend to be more heavily data-driven (e.g. species distribution models). By considering predictive and exploratory modelling alongside each other, we aim to illustrate the range of methods used to model succession and disturbance dynamics and the challenges involved in the model-building and evaluation processes in this arena. © 2007 Rübel Foundation, ETH Zürich."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-39549088103&partnerID=40&md5=bf64683b623554ad8848829074521180","All realistic Multi-Criteria Decision Making (MCDM) problems face various kinds of uncertainty. Since the evaluations of alternatives with respect to the criteria are uncertain they will be assumed to have stochastic nature. To obtain the uncertain optimism degree of the decision maker fuzzy linguistic quantifiers will be used. Then a new approach for fuzzy-stochastic modeling of MCDM problems will be introduced by merging the stochastic and fuzzy approaches into the OWA operator. The results of the new approach, entitled FSOWA, give the expected value and the variance of the combined goodness measure for each alternative. Robust decision depends on the combined goodness measures of alternatives and also on the variations of these measures under uncertainty. In order to combine these two characteristics a composite goodness measure will be defined. The theoretical results will be illustrated in a watershed management problem. By using this measure will give more sensitive decisions to the stakeholders whose optimism degrees are different than that of the decision maker. FSOWA can be used for robust decision making on the competitive alternatives under uncertainty. © 2008 Springer Science+Business Media, LLC."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889765103&partnerID=40&md5=8e278af4bf98f1f43dcd353c366e8829",[No abstract available]
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893522915&partnerID=40&md5=1eb2f145378f2c56b1d487827f159253","Policymakers are facing public pressure to take measures to reduce road fatalities due to speeding. A potential measure for reducing speeding is implementing Intelligent Speed Adaptation (ISA), which supports or enforces a driver to maintain the appropriate speed limit. However, despite its safety benefits, ISA impacts on other measures, such as emissions, throughput, and travel time, also need to be considered. In this paper we show how exploratory modelling can support a multi-criteria analysis to develop good policies for the implementation of ISA. The insights may shed light on the uncertainties and enable a large scale implementation of ISA."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-36349026995&partnerID=40&md5=5c2618df98caac5844962f7d2bd11a3a","Exploratory modeling is an approach used when process and/or parameter uncertainties are such that modeling attempts at realistic prediction are not appropriate. Exploratory modeling makes use of computational experimentation to test how varying model scenarios drive model outcome. The goal of exploratory modeling is to better understand the system of interest through delineation of plausible boundaries, description of patterns within multidimensional model space, and to the extent possible, quantification of the likelihood of occurrence of different model outcomes. This study makes use of exploratory modeling in GIS to delineate boundaries of likely and plausible variability in past Oregon forests due to natural fire disturbance processes interacting with climate change, and compares those with current forests modified by harvest disturbance and with a hypothetical forest scenario. The implications of different forest landscapes for biodiversity are quantified, using a rule base constructed from empirical data describing forest age class, elevation, and heterogeneity requirements by species. Results show: 1) a wide range of natural forest structure plausibly existed in the past, 2) portions of the current forest are outside of the most liberal models of the natural range of forest variability, and 3) large changes in forest structure produce relatively small changes in biodiversity indicators. These findings suggest that disturbance processes (natural or human) that retain forested land cover but alter forest age class structure do not have a strong impact on biodiversity. As an exploratory exercise, the findings are not the end objective; rather, they serve as a basis for dialogue about which forest landscape factors are most important for assessment of biodiversity impacts. © 2007 Elsevier B.V. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-47349115736&partnerID=40&md5=588d906e44858873afe1ba2edc9ee01c","Reducing carbon emissions is needed to tackle climate change. However, despite the large uncertainties surrounding the issue, most policy design processes employ a predict-then-act approach. This paper applies a System-of-Systems perspective and exploratory modeling method to support the design of an adaptive policy. Using a case in the Dutch residential sector, we use the SoS perspective as a way to decompose the policy issue into interdependent relevant policy systems. This representation of the policy system provides a framework to test a large number of hypotheses about the evolution of the system's behavior by way of computational experiments, which form the core of the exploratory modeling method. As some of the system uncertainties are resolved, policies are adapted so that policy goals can still be met. In particular, in a situation in which the realized emission level misses the target, a new set of conditions can be identified to bring the level back to the intended target. ©2007 IEEE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-48349131760&partnerID=40&md5=63a16c0fb33b2b379065b33ad50bc951","In this paper a strategic planning framework is proposed for generation expansion planning of grid-connected micro-power systems. The proposed framework is composed of two different decision making techniques that results a robust planning approach compared to previous techniques to identify the feasible and efficient plans according to attributes and uncertain parameters considered in planning environment. The framework considers the robustness and risk exposure of different configurations according to uncertain parameters in the planning environment to select the plans which meet the objectives more efficiently with least risk exposure. The proposed methodology is implemented for generation expansion planning of a grid-connected micro-power system for a small region in Iran as a case study. © 2007 IEEE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-49749120613&partnerID=40&md5=b6742fdebef95374460d73920b3dc2ef","Decision making in complex environments in the face of uncertain and missing information is a daunting task. We describe a modeling and simulation based approach to providing planners, analysts, and decision makers with a better understanding of the effect of imperfect information on the reliability of decisions made in such situations. We use techniques adopted from Sensitivity Analysis to evaluate the sensitivity of particular decision-making procedures to the uncertainty associated with the information that is being used to make the decision. We use this analysis to support the development of more robust decision-making procedures and effective and efficient information-gathering plans. We demonstrate how these tools can be used in both on-line decision analysis and offline decision evaluation and development, and we describe how these tools can be used to support complex simulation systems such as the U.S. Army's Modeling Architecture for Technology and Research EXperimentation (MATREX). © 2007 IEEE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-55849131118&partnerID=40&md5=1ea7f079406b1b359f3737b9e1ce292e","One of the most important activities related to water resources management consists of finding suitable development strategies for a given river basin. In this paper an exploratory modelling technique will be used instead of the standard scenario approach to assess the robustness of a proposed strategy. Preliminary tests of this technique in the Körsch River basin in Germany suggested that this approach poses many advantages if it is compared with the scenario approach, namely: (1) one can ""stress test"" a strategy and assess its inherent risk of failure; and (2) one can find events or parameter values under which a good strategy may fail."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-48349122631&partnerID=40&md5=584534473c164698f6743e68ae3b8188","Intrusion alert correlation techniques correlate alerts into meaningful groups or attack scenarios for the ease to understand by human analysts. These correlation techniques have different strengths and limitations. However, all of them depend heavily on the underlying network intrusion detection systems (NIDSs) and perform poorly when the NIDSs miss critical attacks. In this paper, a system was proposed to represents a set of alerts as subattacks. Then correlates these subattacks and generates abstracted correlation graphs (CGs) which reflect attack scenarios. It also represents attack scenarios by classes of alerts instead of alerts themselves to reduce the rules required and to detect new variations of attacks. The experiments were conducted using Snort as NIDS with different datasets which contain multistep attacks. The resulted CGs imply that our method can correlate related alerts, uncover the attack strategies, and can detect new variations of attacks. © 2007 IEEE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-47249103464&partnerID=40&md5=d5d85560400ad0cad8df63e91430798d","The proceedings contain 119 papers. The topics discussed include: human systems integration- a system of systems engineering challenge; a two-stage formation flying strategy to reduce the mission time; system of systems management: a network management approach; methodology to design the capacity of a microgrid; an integrated multi-robot test bed to support incremental simulation-based design; system-of-systems perspective and exploratory modeling to support the design of adaptive policy for reducing carbon emission; bilateral haptic teleoperation of an articulated track mobile robot; agent solutions for navy shipboard power systems; complexity as a cause of failure in information technology project management; precise formation of multi-robot systems; and prevention,detection and recovery for cyber-attacks using a multilevel agent architecture."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867795378&partnerID=40&md5=31186826bf8f8d7562f98a81c121640e","Insurgents have effectively employed asymmetric tactics, such as suicide vehicle borne improvised explosive devices (SVBIED), as viable threats in urban environments against counterinsurgent forces conducting Stability, Security, Transition, and Reconstruction (SSTR) Operations. The operational environment, to include political, military, social, and physical factors, in which they implement these tactics is not as readily constrainable as it is in full combat operations. These factors add to the complexity and challenges of detecting and defeating this threat. This paper discusses results and the insights we have gained from experiments examining traffic control point strategy characteristics and associated SVBIED path choice and mission outcome and application of these insights in designing experiments to investigate area coverage strategies in light of insurgent capabilities and behaviors. Agent based modeling and simulation environments are used in this work for exploratory modeling across a wide range of parameters. The research extends previous work by incorporating denser and more complex urban settings, traffic, and strategy factors that can affect the decisioning of the threat. Moreover, in previous efforts, the traffic control points (TCPs) were arrayed around a single target in a point type of defense. In this set of experiments we move to multiple possible targets and an area defense. Our goal is to generate insights that will allow counterinsurgent forces to implement strategies robust against a range of SVBIED behaviors."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-56849104175&partnerID=40&md5=4eee66c91b623f45004c08716f77e666","Knowledge intensive sectors, such as the pharmaceutical, have typically to face the problem of dealing with heterogeneous and vast amounts of information. In these scenarios discovery, integration and easy access to the knowledge are the most important factors. The use of semantics to classify meaningfully the information and to bridge the gap between the different representations that different stakeholders have is widely accepted [1]. The problem arises when the ontologies used to model the domain become too large and unmanageable. The current status of the technology does not allow to easily working with this type of ontologies. In this paper we propose the use of networked ontologies to solve these problems for the particular case scenario of the pharmaceutical sector in Spain. Instead of using a single ontology, the idea is to break the model in several meaningful pieces and bind them together using a networked ontology model for representing and managing relations between multiple ontologies. The Semantic Nomenclature is a case study that is currently under development in the NeOn1 EC funded FP6 project [2]. Our approach is that of the integration of the large quantity of information about pharmaceutical products provided by a wide range of actors in the Spanish pharmaceutical sector using networked ontologies. Cover the profound lack of systematization for creating, maintaining and updating drug-related information, the creation of a common reference ontology about drugs, the provision of easy mechanisms for discovery, model and mapping of new sources of information in a collaborative way, and the ability to reason on the context of user and ontologies in order to facilitate the mapping and retrieving processes, are among the main goal of this case study that will be further developed in the present paper."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-37749012090&partnerID=40&md5=694e12258aa7cb9db1afebe7cb106631","Due to increased competition, especially small and medium-sized enterprises (SMEs) are forced to concentrate on their core competencies. Focusing on core competencies on the one hand and the demand on system solutions on the other hand is a challenge to the product development process. To meet these challenges a robust decision making in product development is an essential factor for the success of these companies. For example, in the deregulated markets SMEs have to cope with the altered economic setting and with new demands on development, products and production structures. These circumstances appear especially in European railway industry. This paper shows scenarios for a sustainable product development. It discusses how scenario-based decisions can contribute to sustainable products, especially for SMEs in deregulated markets. For this reason three scenarios of railway freight bogie concept created by railway industry experts during a scenario workshop are introduced. They show how competitors can collaborate during the product development process. The results are based on an appliance research project in the collaboration with the practitioners, e.g. components producers, providers and consulters of the railway industry. © Carl Hanser Verlag."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-35548973441&partnerID=40&md5=72c1af23378a3b27c63f437f78f99e52","In this paper, we present a robust quantitative decision-making methodology for structural damage detection using piezoelectric transducers and the Lamb wave approach with the consideration of environmental and operational variances. The features of wave propagation are extracted from multiple signals using an improved adaptive harmonic wavelet transform (AHWT), which are then denoised and highlighted by applying the principal component analysis (PCA) in the wavelet domain. After self-checking and updating the baseline dataset, the Hotelling's T2 analysis provides a statistical indication of damage under a certain given confidence level. The detectability, sensitivity and robustness of the methodology are investigated using experimental as well as numerical studies. As the basis for the detection practice, the propagation of Lamb waves in an aluminum beam is systematically studied in this paper."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548356969&partnerID=40&md5=dadd3cc04690696a8bda5c3e5f9aab52","Evidence based medicine (EBM) is a potent patient centered clinical learning strategy. EBM facilitates efficient robust decision making on an individual patient basis. It provides the framework and tools to rapidly and effectively find answers to specific clinical questions and educate others. At the core of EBM is clinical judgement. This is the ability to combine clinical experience and expertise with the best available evidence in the literature to answer a specific pediatric anesthesia question. © 2007 The Authors."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-35448997206&partnerID=40&md5=6ccd083990b00b7fa2e19dc7a3d1e682","Many commentators have suggested the need for new decision analysis approaches to better manage systems with deeply uncertain, poorly characterized risks. Most notably, policy challenges such as abrupt climate change involve potential nonlinear or threshold responses where both the triggering level and subsequent system response are poorly understood. This study uses a simple computer simulation model to compare several alternative frameworks for decision making under uncertainty - optimal expected utility, the precautionary principle, and three different approaches to robust decision making - for addressing the challenge of adding pollution to a lake without triggering unwanted and potentially irreversible eutrophication. The three robust decision approaches - trading some optimal performance for less sensitivity to assumptions, satisficing over a wide range of futures, and keeping options open - are found to identify similar strategies as the most robust choice. This study also suggests that these robust decision approaches offer a quantitative, decision analytic framework that captures the spirit of the precautionary principle while addressing some of its shortcomings. Finally, this study finds that robust strategies may be preferable to optimum strategies when the uncertainty is sufficiently deep and the set of alternative policy options is sufficiently rich. © 2007 Society for Risk Analysis."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248571358&partnerID=40&md5=c1c1cf3b26533c3f0b64491254dbb87e","Accurate software metrics-based maintainability prediction can not only enable developers to better identify the determinants of software quality and thus help them improve design or coding, it can also provide managers with useful information to help them plan the use of valuable resources. In this paper, we employ a novel exploratory modeling technique, multiple adaptive regression splines (MARS), to build software maintainability prediction models using the metric data collected from two different object-oriented systems. The prediction accuracy of the MARS models are evaluated and compared using multivariate linear regression models, artificial neural network models, regression tree models, and support vector models. The results suggest that for one system MARS can predict maintainability more accurately than the other four typical modeling techniques, and that for the other system MARS is as accurate as the best modeling technique. © 2006 Elsevier Inc. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247172058&partnerID=40&md5=23c08cf5573c91464c8561761078ecc4","Bacterial adhesion and motility are studied at the pore scale by focusing on two interrelated aspects of transport: wall attachment/detachment (reversible sorption) and the role of convection and pore geometry on adhesion. Motility is also examined through use of Brownian dynamics. Bacteria motility and reversible attachment/detachment are incorporated with a numerical laminar flow solver. Since individual bacteria are modeled, the results apply to low concentrations/coverage. Pore geometries consistent with a microflow cell of variable cross sectional area are used. This exploratory modeling work precedes an ongoing microflow cell experimental study and more detailed Lévy particle models. Adsorption reactions occurring over different time scales are modeled as multimodal distributions with power law tails. Computations show the relative magnitude of bacterial motility to advection controls the average number of collisions against solid walls. Variable cross section in pore geometry changes hydrodynamic conditions for deposition (e.g., variable shear stress). In regions of reduced cross sectional area, the ratio of bacteria motility to average velocity is smaller and results in less collisions and reduced retardation. Additionally, reduced cross sectional area increases both wall shear stress and vorticity which should be considered in adhesive models. While the shear forces acting on a particle deposited at the wall work on a spatial scale of the microbe's size, adhesive forces may be confined to tens of nanometers. Multimodal adhesion causes the first passage time distributions to have long tails. © 2006 Elsevier Ltd. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248674011&partnerID=40&md5=f7c8788db6eff00988ff153258d94e23","Weapon system of system evaluation belongs to high-level decision-making, and it is necessary to consider a lot of uncertainty environments and factors. Exploratory analysis is an agility analysis method combine multi- knowledge and multi-methodology. For deal with uncertainty problem of weapon system of system evaluation, a framework was brought forward. The exploratory analysis framework includes gain various knowledge, exploratory analysis modeling, uncertainty management and experiment analysis, finally sustains a robust decision-making. Exploratory analysis modeling is the most important part of the framework, and it includes an analysis modeling based on extend influence diagram and motivated meta-models; finally the framework has applied for air force campaign exploratory analysis."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846838684&partnerID=40&md5=1df6e3e91c165d86dafecc1aba9029ed","Scenarios play a prominent role in policy debates over climate change, but questions continue about how best to use them. We describe a new analytic method, based on robust decision making, for suggesting narrative scenarios that emerge naturally from a decision analytic framework. We identify key scenarios as those most important to the choices facing decision makers and find such cases with statistical analysis of datasets created by multiple runs of computer simulation models. The resulting scenarios can communicate quantitative judgments about uncertainty as well as support a well-defined decision process without many drawbacks of current approaches. We describe an application to long-term water planning in California. © 2006 Elsevier Ltd. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750470047&partnerID=40&md5=226eac3a68d75219c2d778cdc0eb8aa2","A new method for speaker identification that selectively uses feature vectors for robust decision-making is described. Experimental results, with short speech segments ranging from 0.25 to 2 s, showed that our method consistently outperforms other approaches yielding relative improvements of 20-51% and 15-30% over baseline GMM and the LDA-GMM systems, respectively. © 2006 Elsevier B.V. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845863445&partnerID=40&md5=c7d149dae469bcabd94582e290a844c4","Stochastic programming brings together models of optimum resource allocation and models of randomness to create a robust decision-making framework. The models of randomness with their finite, discrete realisations are called scenario generators. In this paper, we investigate the role of such a tool within the context of a combined information and decision support system. We explain how two well-developed modelling paradigms, decision models and simulation models can be combined to create ""business analytics"" which is based on ex-ante decision and ex-post evaluation. We also examine how these models can be integrated with data marts of analytic organisational data and decision data. Recent developments in on-line analytical processing (OLAP) tools and multidimensional data viewing are taken into consideration. We finally introduce illustrative examples of optimisation, simulation models and results analysis to explain our multifaceted view of modelling. In this paper, our main objective is to explain to the information systems (IS) community how advanced models and their software realisations can be integrated with advanced IS and DSS tools. © 2006 Elsevier B.V. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751530889&partnerID=40&md5=b77d2779aa63e7c17aadae31b1e60e7c","The existence and function of tidally dominated and predominantly allochthonous marshes are ultimately contingent upon the operation of hydrodynamic and sedimentary processes within constraints imposed by the available accommodation space and sediment supply. This paper re-interprets published data relating to contemporary vertical marsh growth and sea-level rise in the context of the conceptual model relating elevation, sedimentation, sea-level rise, sediment supply and tidal range. This analysis is supported by numerical mass balance modelling of the equivalent parameter space and of the sensitivity of marsh hydroperiod and sedimentation to sea-level and sediment supply forcing. The effect of autocompaction on the translation of sedimentation into elevation change is also considered. Parameter space modelling provides a framework for the interpretation of field data and affords indicative insights into marsh resilience to change. It is argued that the assessment of marsh response to external environmental forcing should be based not on empirical comparisons of sedimentation versus sea-level rise but on the estimation of sediment supply, and the efficiency with which this is depleted by deposition, as metrics of marsh resilience. This implies a shift towards more intensive process studies aimed at elucidating more fully the linkages between tidal marshes and adjacent estuarine and coastal systems. Model results also indicate significant variability in marsh sedimentation associated with 18.6 yr tidal modulation and meteorological processes at short-term scales. Such variability further complicates the interpretation of sedimentation or elevation change data obtained from monitoring programmes of short duration. Longer-term monitoring is of value, however, as a means of identifying important mechanisms of climate and sediment supply forcing that may contribute to the formation and maintenance of marsh sedimentary sequences. © 2006 Elsevier B.V. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878806975&partnerID=40&md5=5fd1ab9d48a71ff0d1b8d39b4aa2820e","With the addition of advanced technologies and automation within systems, the tasks that operators are asked to complete have grown more complex. This has led to greater opportunities for malfunctions, critical incidents, and catastrophes. A major contributor to these problems is the human operator and the technology not being explicitly designed as a single Joint Cognitive System (JCS). The operator is typically only blithely considered during system design and is typically expected to adapt to inputs from the other subsystems. Thinking of the operator and the automation/technology from a holistic perspective, as a single JCS, instead of separate subsystems, allows for more effective, agile, and robust decision-making. The operator must have a complete understanding of the automated processes and the work domain, leading to more informed decision-making. Thinking of the entities as co-agents, rather than two isolated components that must interface, allows for more effective problem solving and decision-making."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751331386&partnerID=40&md5=67de8bf162c124f0e938fbbb58bfcf1c","Information-Gap Decision Theory (IGDT), an approach to robust decision making under severe uncertainty, is considered in the context of a simple life cycle engineering example. IGDT offers a path to a decision in the class of problems where only a nominal estimate is available for some uncertain life cycle variable that affects performance, and where there is some unknown amount of discrepancy between that estimate and the variable's actual value. Instead of seeking maximized performance, the decision rule inherent to IGDT prefers designs with maximum immunity (info-gap robustness) to the size that the unknown discrepancy could take. This robustness aspiration is subject to a constraint of achieving better than some minimal requirement for performance. In this paper, an automotive oil filter selection design example, which involves several types of severe uncertainty, is formulated and solved using an IDGT approach. Particular attention is paid to the complexities of assessing preference for robustness to multiple severe uncertainties simultaneously. The strengths and limitations of the approach are discussed mainly in the context of environmentally benign design and manufacture. Copyright © 2006 by ASME."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750453755&partnerID=40&md5=c41bfe0af67620ce3e4603903349923a","Before implementing policy measures that may contribute to a sustainable development, one has to determine in one or other away in advance if these measures will have their intended effects. As sustainable development necessitates long-term perspectives, a robust vision of possible future states of the world is vital. This paper discusses two different approaches to gain insights into possible future states of the system of interest. One is the classical scenario approach, and the other is the empirical modelling approach. This paper considers possible future air travel demand as a case study, as in that system long-term visions of up to 50 years are necessary. This paper discusses a need for more sustainable development in the aviation sector and argues that changes in that system require long horizons for action. It explains the need for developing ideas about the possible future air travel demand for the period up to 2050. This paper presents a brief review of the literature on scenarios and exploratory modelling. It discusses both the scenario and empirical modelling approach to come up with plausible future states of the system and gives a convergent forecast of future growth in the year 2050. Copyright © 2006 Inderscience Enterprises Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750726083&partnerID=40&md5=0c5ca20f1bdcea5900b3e559a41c3a10","Civil engineering infrastructure systems are vulnerable to the effects of natural hazards such as flooding, landslides, windstorm and coastal erosion. Risk analysis provides a rational approach to analysing the threat these phenomena pose and identifying efficient options for system management.This paper presents a general formulation of the risk-analysis problem for an engineering system subject to environmental loads. However, most of the variables that determine system behaviour may be subject to long-term change, for example due to climate change or structural deterioration.The uncertainties in appraisal of infrastructure over extended timescales can be considerable, so a framework is presented for systematic analysis of uncertainties and robust decision making."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747367514&partnerID=40&md5=452a47ddfc96741959ed8d6b85c113ec","This paper describes a prototype model for exploring counterterrorism issues related to the recruiting effectiveness of organizations such as al Qaeda. The prototype demonstrates how a model can be built using qualitative input variables appropriate to representation of social-science knowledge, and how a multiresolution design can allow a user to think and operate at several levels - such as first conducting low-resolution exploratory analysis and then zooming into several layers of detail. The prototype also motivates and introduces a variety of nonlinear mathematical methods for representing how certain influences combine. This has value for, e.g., representing collapse phenomena underlying some theories of victory, and for explanations of historical results. The methodology is believed to be suitable for more extensive system modeling of terrorism and counterterrorism."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747361015&partnerID=40&md5=c7bb4d8a521272915ed02d1e6a63bbda","With increasing industry interest in high pressure roll grinding (HPGR) technology, there is a strong incentive for improved understanding of the nature of grinding pressure that exists in the interior of a compressed particle bed. This corresponds to the crushing region of the HPGR. The relationship between applied pressure (stress) to the particle bed and induced pressure (stress) within particles and at contact points between particles is of particular interest. A detailed parametric investigation is beyond the scope of this exploratory paper. However, this exploratory investigation does suggest some interesting behaviour. The compressed particle bed within an 80 mm diameter piston has been modelled using Particle Flow Code for three dimensions. PFC3D is a discrete element code. The total number of simulated particles was 1225 and 2450 for two beds of different thickness. Particle diameters were uniformly distributed between 4 and 4.5 mm. The results of the simulations show that stress intensity within the simulated particle beds and within the observed particles increased with increase of the applied stress. The intensity of the average vertical stress in the selected particles tended to be comparable with the intensity of the pressure applied to the surface of particle bed and was only occasionally higher. However, the stress at contact points between particles could be several times higher. In a real crusher, such high stress amplification at contacts will quickly decrease due to local crushing and a resultant increase the size of the contact area. Therefore, its significance is likely to be relatively small in an industrial context. The modelling results also suggest that failure within the particle bed will progress from the crushing surface towards the depth of the bed. © 2006 Elsevier Ltd. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746623003&partnerID=40&md5=6b07fdd90a7d3b9ca2e5edabfd498a11","To unravel the complex in vivo regulatory interdependences of biochemical networks, experiments with the living organism are absolutely necessary. Stimulus response experiments (SREs) have become increasingly popular in recent years. The response of metabolite concentrations from all major parts of the central metabolism is monitored over time by modern analytical methods, producing several thousand data points. SREs are applied to determine enzyme kinetic parameters and to find unknown enzyme regulatory mechanisms. Owing to the complex regulatory structure of metabolic networks and the amount of measured data, the evaluation of an SRE has to be extensively supported by modelling. If the enzyme regulatory mechanisms are part of the investigation, a large number of models with different enzyme kinetics have to be tested for their ability to reproduce the observed behaviour. In this contribution, a systematic model-building process for data-driven exploratory modelling is introduced with the aim of discovering essential features of the biological system. The process is based on data pre-processing, correlation-based hypothesis generation, automatic model family generation, large-scale model selection and statistical analysis of the best-fitting models followed by an extraction of common features. It is illustrated by the example of the aromatic amino acid synthesis pathway in Escherichia coli. 2006 © The Institution of Engineering and Technology."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33645811102&partnerID=40&md5=0e9a96e0c4c21dc5dee4d4fd830e76fe","Robustness is a key criterion for evaluating alternative decisions under conditions of deep uncertainty. However, no systematic, general approach exists for finding robust strategies using the broad range of models and data often available to decision makers. This study demonstrates robust decision making (RDM), an analytic method that helps design robust strategies through an iterative process that first suggests candidate robust strategies, identifies clusters of future states of the world to which they are vulnerable, and then evaluates the trade-offs in hedging against these vulnerabilities. This approach can help decision makers design robust strategies while also systematically generating clusters of key futures interpretable as narrative scenarios. Our study demonstrates the approach by identifying robust, adaptive, near-term pollution-control strategies to help ensure economic growth and environmental quality throughout the 21st century. © 2006 INFORMS."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70749143209&partnerID=40&md5=bf0aeb3aadbb373fbdfe9e1c2109b755","This paper is an introduction to the simulation and modeling framework and tool SmallDEVS. It is a new modeling and simulation framework for Smalltalk. SmallDEVS is different from other tools of its category, because of its openness and reflective features. It supports class-based as well as prototype-based object-oriented model construction. Its meta-object protocol allows the models to be constructed from scratch and inspected and edited during run-time. Interactive modeling and simulation is supported by a graphical user interface which has been highly influenced by the user interface of Self."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-31344449858&partnerID=40&md5=1af26a263cea09f945b940b8c78a5e3d","The paper reviews selected literature on the theory and practice of constructive technology assessment (CTA), which represents a promising approach for managing technology through society. CTA emphasises the involvement and interaction of diverse participants to facilitate 'upstream' (or anticipatory) learning about possible impacts of technology and socially robust decision-making. The paper seeks to identify limitations of CTA, as these relate to the broadening of debate about nascent, controversial technology. In particular, it considers the relevance of CTA to the achievement of more democratic decision-making about technology. In addition, the paper directs attention towards differences in participants' discursive capacities and rhetorical skills that may affect the role and contribution of non-expert citizens in technology assessment. The paper draws upon the debate between Habermas and Foucault to suggest promising avenues for future research based on technology assessment conceptualised as discourse. It concludes that the theory and practice of CTA may be improved by addressing explicitly possible structural limitations on the broadening of debate, whilst invoking a notion of technology assessment as discourse to point up cultural, subjective or cognitive limitations on agency. © 2005 Taylor & Francis."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33244457274&partnerID=40&md5=130a2f365b93e690e69e65123faaa002","Starting from a need and a set of functional requirements (FRs), a designer is often perplexed to assess the potential of a given concept to fit these requirements. He is even more perplexed when several concepts are candidates. This paper proposes a definition of a concept in a practical way as a parameterized model linking a set of design variables (DVs) to a set of performance variables (PVs). This set of PVs is supposed to be the same for any concept candidate to fulfill a need. This is why our model propose to ""plug"" a card of FRs into candidate concepts in order to lead concurrent reasonings on competing concepts until one or several of them appear to be of poor interest. The plugging mechanism is implemented by constraint programming techniques (evolved interval arithmetics) that immediately contract the performance and design variable domains to provide an outer approximation of the solution (or design) space. Two sets of comparison operators between solution spaces are proposed: operators for comparing the relative potential of two concepts submitted to the same FRs, and operators for comparing two successive stages of solution spaces of a given concept. These last operators provide the way to tackle the robustness of design decision making under uncertainty. All the mentioned features: plugging mechanism, contraction of domains and design space representation, comparison operators and robustness considerations have been experimented on an example of a pair of candidate concepts of truss structures. Copyright © 2005 by ASME."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960732287&partnerID=40&md5=be0fed1391dbf613c1683a3da8514d39","The automated design scheme called exploratory modelling for controller optimization (EMCO) is re-examined to obtain simple robust controllers for complex plant dynamics. The allowable gain error (AGE) function is revised to quantify a frequency dependent bound of model gain errors which allow for both robust servo performance and the required stability robustness margin to be maintained. AGE lends itself to iterative modelling and controller redesign and the resulting scheme is outlined. It is proven that control has synergistic interaction with system identification of the plant model. Copyright © 2005 IFAC."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646553910&partnerID=40&md5=27514b4be47b0a04f4ba77828eddb03b","The Nuclear Decommissioning Authority (NDA) has recently been established to oversee and manage the decommissioning and clean up of the UK's civil nuclear legacy, and has taken ownership of the civil nuclear assets and liabilities that had previously been in public ownership. The nuclear sites that were previously in the ownership of BNFL and UKAEA have recently transferred to the NDA, and the operation of these sites will be contracted to Site Licence Companies (SLCs). The NDA is expected to focus on accelerated clean up and it will therefore be important to identify appropriate site end points. The challenge for the SLCs is to demonstrate to the NDA, as the owner of liabilities, amongst other stakeholders, that the implementation of remediation solutions and disposal of waste remain appropriate. A key issue will be the identification of appropriate site end points. Issues that may require consideration within the development of appropriate site end points include below ground contamination, decommissioning rubble, potential new disposal facilities, options for waste management, partial remediation, delicensing, indefinite control, interim end points, eventual closure, and extent of decommissioning. The site end points will need to meet requirements related to such issues. Nexia Solutions Ltd (part of the BNFL group) is undertaking a programme of work to develop an integrated framework to inform the decision making process for site end point management on NDA owned nuclear licensed sites in the UK. This framework enables an understanding of the current condition of a site, an assessment of its evolution with time, and the potential impacts or consequences of alternative site end points. Key to this programme is the development of an integrated approach collating the tools, characterisation and best practice themes to aid robust decision making. The NDA will be expecting SLCs to operate in an open and transparent manner, thus the development of a common framework to support decision making by SLCs across NDA sites is key. Furthermore, NDA decision making will be subject to public consultation. A further objective is to ensure the transfer of learning across NDA sites in order to maximise savings resulting from R&D and best practice. This framework programme for site end point management is designed to provide underlying common principles, guidelines and technology solutions that can then be applied - as appropriate - at different levels of detail to the different levels of complexity of sites. This provision of guidance to the SLCs is to facilitate strategic thinking and decision making in liabilities management and to help to ensure safe, cost effective and proportionate management. The programme will enable the assessment of potential options and thus facilitate the identification of appropriate end points for specific sites, to enhance site clean up and - where appropriate - to accelerate schedules, reduce lifetime costs, reduce key uncertainties, and reduce hazards and risks (radiological and non-radiological). SLCs will be able to utilise the integrated framework to enable regulatory compliance, adherence to best practice guidance, and a consistent risk-based decision making approach for the achievement of appropriate site end points. Copyright © 2005 by ASME."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-26844522859&partnerID=40&md5=ea16b685b8aaa3e5125a5dc9a861d037","At Windber Research Institute we have started research programs that use artificial neural networks (ANNs) in the study of breast cancer in order to identify heterogeneous data predictors of patient disease stages. As an initial effort, we have chosen matrix metalloproteinases (MMPs) as potential biomarker predictors. MMPs have been implicated in the early and late stage development of breast cancer. However, it is unclear whether these proteins hold predictive power for breast disease diagnosis, and we are not aware of any exploratory modeling efforts that address the question. Here we report the development of ANN models employing plasma levels of these proteins for breast disease predictions. © Springer-Verlag Berlin Heidelberg 2005."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-27944476851&partnerID=40&md5=3f5062e7541f2e1672cab12298405e46","Objective: To evaluate the effect of hepatitis C virus (HCV)/HIV co-infection on neuropsychological performance and neurological status in HIV/HCV treatment-naive HIV-1-infected individuals we conducted a cross-sectional study using baseline data from an HIV therapy trial. Methods: HCV status was determined by the presence of anti-HCV antibodies. Neuropsychological function was evaluated by Trailmaking tests, and the Digit Symbol Task. Depression was assessed using the Center for Epidemiologic Studies - Depression Scale. Sleep quality was evaluated by the Pittsburgh Sleep Quality Index and anxiety by the State-Trait Anxiety Inventory for Adults. A questionnaire was designed grading the severity of a variety of symptoms. Results: Of 264 patients with HCV status data, 30 were HCV positive and 234 were HCV negative. Both groups were comparable except that HCV-positive individuals had a higher prevalence of intravenous drug use and lower educational level. The HCV-positive group had a significantly lower neuropsychological performance overall. Multivariate modeling supported an association between HCV infection status with test performance in the Digit Symbol Task and mood parameters even when controlling for potentially confounding variables. Marginal differences were noted with respect to symptom questionnaire scores and global sleep. No differences were noted with respect to anxiety. Conclusion: The findings suggest that HCV/HIV co-infection has an adverse impact on neuropsychological function. HCV may also be associated with depressed mood, particularly somatic depressive symptoms. Although confounding contributors to neuropsychological performance are difficult to exclude, exploratory modeling supports the association between HCV infection status and some impairment of neuropsychological performance and depressed mood. © 2005 Lippincott Williams & Wilkins."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-24944498390&partnerID=40&md5=51d509a3beac1715f00dcbf66ebba9ee","The Intermodal Surface Transportation Efficiency Act (ISTEA) of 1991 mandated the consideration of safety in the regional transportation planning process. As part of National Cooperative Highway Research Program Project 8-44, ""Incorporating Safety into the Transportation Planning Process,"" we conducted a telephone survey to assess safety-related activities and expertise at Governors Highway Safety Associations (GHSAs), and GHSA relationships with metropolitan planning organizations (MPOs) and state departments of transportation (DOTs). The survey results were combined with statewide crash data to enable exploratory modeling of the relationship between GHSA policies and programs and statewide safety. The modeling objective was to illuminate current hurdles to ISTEA implementation, so that appropriate institutional, analytical, and personnel improvements can be made. The study revealed that coordination of transportation safety across DOTs, MPOs, GHSAs, and departments of public safety is generally beneficial to the implementation of safety. In addition, better coordination is characterized by more positive and constructive attitudes toward incorporating safety into planning."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-23044493930&partnerID=40&md5=8cd3767266c3e5f4039df8e7b8f34f65","In conservation biology it is necessary to make management decisions for endangered and threatened species under severe uncertainty. Failure to acknowledge and treat uncertainty can lead to poor decisions. To illustrate the importance of considering uncertainty, we reanalyze a decision problem for the Sumatran rhino, Dicerorhinus sumatrensis, using information-gap theory to propagate uncertainties and to rank management options. Rather than requiring information about the extent of parameter uncertainty at the outset, information-gap theory addresses the question of how much uncertainty can be tolerated before our decision would change. It assesses the robustness of decisions in the face of severe uncertainty. We show that different management decisions may result when uncertainty in utilities and probabilities are considered in decision-making problems. We highlight the importance of a full assessment of uncertainty in conservation management decisions to avoid, as much as possible, undesirable outcomes. © 2005 by the Ecological Society of America."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33644634187&partnerID=40&md5=60d761e9c3e7ab64cec5158fd2bcacc2","We model dependencies between m multivariate continuous-valued information sources by a combination of (i) a generalized canonical correlations analysis (gCCA) to reduce dimensionality while preserving dependencies in m -1 of them, and (ii) summarizing dependencies with the remaining one by associative clustering. This new combination of methods avoids multiway associative clustering which would require a multiway contingency table and hence suffer from curse of dimensionality of the table. The method is applied to summarizing properties of yeast stress by searching for dependencies (commonalities) between expression of genes of baker's yeast Saccharomyces cerevisiae in various stressful treatments, and summarizing stress regulation by finally adding data about transcription factor binding sites. © World Scientific Publishing Company."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-19944410204&partnerID=40&md5=a0342311e83417920002aa268aef5062","A. B. Murray and C. Paola (1994, Nature, vol. 371, pp. 54-57; 1997, Earth Surface Processes and Landforms, vol. 22, pp. 1001-1025 proposed a cellular model for braided river dynamics as an exploratory device for investgating the conditions necessary for the occurrence of braiding. The model reproduces a number of the general morphological and dynamic features of braided rivers in a simplified form. Here we test the representation of braided channel morphodynamics in the Murray-Paola model against the known characteristics (mainly from a sequence of high resolution digital elevation models) of a physical model of a braided stream. The overall aim is to further the goals of the exploratory modelling approach by first investigating the capabilities and limitations of the existing model and then by proposing modifications and alternative approaches to modelling of the essential features of braiding. The model confirms the general inferences of Murray and Paola (1997) about model performance. However, the modelled evolution shows little resemblance to the real evolution of the small-scale laboratory river, although this depends to some extent on the coarseness of the grid used in the model relative to the scale of the topography. The model does not reproduce the bar-scale topography and dynamics even when the grid scale and amplitude of topography are adapted to be equivalent to the original Murray-Paola results. Strong dependence of the modelled processes on local bed slopes and the tendency for the model to adopt its own intrinsic scale, rather than adapt to the scale of the pre-existing topography, appear to be the main causes of the differences between numerical model results and the physical model morphology and dynamics. The model performance can be improved by modification of the model equations to more closely represent the water surface but as an exploratory approach hierarchical modelling promises greater success in overcoming the identified shortcomings. Copyright © 2005 John Wiley & Sons, Ltd."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-17044362640&partnerID=40&md5=3cb48d06cade4229c17094d29599c608","Different approaches to deal with the long-term environmental problems to make the future secure are discussed. Analysts are adopting techniques such as scenario planning that involve exploring different possible futures rather than gambling on a single prediction. Policies are made robust on the basis of future scenario determined using advanced computational technologies. The robust decision making strategy is considered to be a more sustainable approach to effect a safe environment in the future."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-20344385278&partnerID=40&md5=c609205826845bc033b3207271c223c1","In this paper, the problem of choosing from a set of design alternatives based upon multiple, conflicting and uncertain criteria is investigated. Multiple attributes can arise when different disciplines contribute to the design of a product or process. The Overlap Measure Method developed in this paper models two sources of uncertainties - imprecise attribute values provided to the decision maker from the various disciplines and inabilities of the decision-maker to specify an exact desirable attribute level. Additionally, this method also provides the necessary steps to mitigate the effects of these sources of uncertainties using the Overlap Measure metric. The Overlap Measure Method can be applied to any theoretically sound multiattribute decision making method and in this paper is applied to the Hypothetical Equivalents and Inequivalents Method. The Overlap Measure Method also includes a sub-routine that ensures that the winning alternative is insensitive to changes in the relative importances of the different disciplines. Copyright © 2004 by the American Institute of Aeronautics and Astronautics, Inc. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-5444275807&partnerID=40&md5=b4f82dd98e6e1173267c05b98527307e","We analyze the empirical predictions of ambiguity aversion in intertemporal heterogenous agents economies. We examine equilibria for two tractable wealth-homothetic settings of ambiguity aversion in continuous time. Each setting is motivated by a different robust control optimization problem. We show that ambiguity aversion affects optimal portfolios in a way that is similar to an increase in risk aversion. A distinct property of our second setting of ambiguity aversion is that this increase is state dependent, highly pronounced at moderate portfolio exposures and reduces equity-market participation. In general equilibrium, ambiguity aversion raises the equity premium and lowers interest rates. A distinct feature of our second setting of ambiguity aversion is that the equity premium part due to ambiguity aversion dominates when volatility is low."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33645585834&partnerID=40&md5=fbd20d5ae3cf10ef62471274dddbe04f","Version space is used in inductive concept learning to represent the hypothesis space where the goal concept is expressed as a conjunction of attribute values. The size of the version space increases exponentially with the number of attributes. We present an efficient method for representing the version space with DNA molecules and demonstrate its effectiveness by experimental results. Primitive operations to maintain a version space are derived and their DNA implementations are described. We also propose a novel method for robust decision-making that exploits the huge number of DNA molecules representing the version space. © Springer-Verlag Berlin Heidelberg 2003."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-1442303268&partnerID=40&md5=bf9d3ac6570189289a55708bd3f04730","Driven by the rapid development of artificial intelligence applications and embedded processing paradigms in computing, intelligent agent has been an emerging software paradigm and widely developing subject of research during recent years in the context of emerging internet technologies. In this proposal, a programme for the investigation of a novel man machine interface integrated with an efficient and effective information management and decision support environment, designed for a reliable and robust decision-making process, is proposed. The aim of this proposal is to develop effective intelligent software agents and their supervisory control organisation, for specific applications in data management and control in complex environments, i.e. power plant/system control rooms. Finally, an object-oriented graphical interface will allow operators to interact efficiently and effectively with the intelligent agents embedded in the intelligent information management and decision support environment, which will be implemented using the ZEUS® platform."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-7044285810&partnerID=40&md5=64f6c81446832f0f88150d50509ab09f","This paper reports on initial investigations of an agent architecture that embodies philosophical and social layers. A key feature of the architecture is that agent behavior is constrained by sets of agent societal laws similar to Asimov's laws of robotics. In accordance with embedded philosophical principles, agents use decision theory in their negotiations to evaluate the expected utility of proposed actions and use of resources. This enables more robust decision-making and task execution. To evaluate the robustness, our investigations have included the effect of misinformation among cooperative agents in worth-oriented domains, and active countermeasures for dealing with the misinformation. We demonstrate that propagating misinformation is against the principles of ethical agents. Moreover, such agents are obligated to report on misbehavior, which minimizes its effects and furthers the progress of the agents and their society towards their goals. We also show how dedicating some agents to specialized tasks can improve the performance of a society. © Springer-Verlag Berlin Heidelberg 2003."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-1542349291&partnerID=40&md5=e2f597773c49beffef4fc70a8274a0b3","A new technique of iterative modelling and controller timing is presented and its applicability demonstrated on a model of an active headrest which attenuates the ambient noise for the comfort of a passenger. The underlying scheme relies on internal model control (IMC) which is shown to have self-excitation for identification for control. Application of IMC creates synergy of identification and control and models tuned for control can bring enhanced and more reliable performance. The allowed gain error (AGE) is used which is compared with model uncertainty to form a criterion of modelling for control. This modelling criterion quantifies the maximum violation of robust stability and performance robustness along the frequency axes, as such it is a criterion of modelling for control. The new scheme is called exploratory modelling for controller optimization (EMCO) and its objective is to obtain simple robust controllers. The EMCO scheme can be used both on and off-line."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889587125&partnerID=40&md5=c1aea1f6db89949022459acfd4db4453","A large and growing variety of tools can support all kinds of UML modeling aspects: from model creation to advanced round-trip engineering of UML models and code. However, such tools aim at supporting specific life-cycle phases, but they often do not meet basic requirements arising in heterogeneous environments, UML education, early life-cycle phases, or agile processes: hassle-free tool deployment, support for fast model sketching, and flexible graphic export features. We present the freely available modeling tool UMLet we designed to specifically address these basic issues. It is a flyweight Java application that can easily be deployed in various development environments; it features an intuitive and pop-up-free user interface, while still providing output to common high-quality publishing formats. Thus, the tool UMLet provides an effective way to teach UML and to create and share UML sketches, especially in agile environments and during early life-cycle phases. Its user interface supports intuitive and exploratory modeling, its architecture makes distribution and deployment cost-efficient in heterogeneous environments. © 2003 IEEE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248645397&partnerID=40&md5=28de56633d5ad8bb32f4e6847cc8b105","Exploratory data-driven multivariate analysis provides a means of investigating underlying structure in complex data. To explore the stability of multivariate data modeling, we have applied a common method of multivariate modeling (factor analysis) to the Genetic Analysis Workshop 13 (GAW13) Framingham Heart Study data. Given the longitudinal nature of the data, multivariate models were generated independently for a number of different time points (corresponding to cross-sectional clinic visits for the two cohorts), and compared. In addition, each multivariate model was used to generate factor scores, which were then used as a quantitative trait in variance component-based linkage analysis to investigate the stability of linkage signals over time. We found surprisingly good correlation between factor models (i.e., predicted factor structures), maximum LOD scores, and locations of maximum LOD scores (0.81< rho <0.94 for factor scores; rho >0.99 for peak locations; and 0.67< rho <0.93 for peak LOD scores). Furthermore, the regions implicated by linkage analysis with these factor scores have also been observed in other studies, further validating our exploratory modeling."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0142057059&partnerID=40&md5=1d50e2dcb46e123f6cb4b1b839f9feb3","Recent advances in the description of aqueous dissolution rates for explosive compounds enhance the ability to describe these compounds as a contaminant source term and to model the behavior of these compounds in a field environment. The objective of this study is to make predictions concerning the persistence of 2,4,6-trinitrotoluene (TNT) and octahydro-1,3,5,7-tetranitro-1,3,5,7-tetrazocine (HMX) in solid form both as individual explosive compounds and components of octol, and the resultant concentrations of explosives in water as a result of dissolution using three exploratory modeling approaches. The selection of dissolution model and rate greatly affect not only the predicted persistence of explosive compound sources but also their resulting concentrations in solution. This study identifies the wide range in possible predictions using existing information and these modeling approaches to highlight the need for further research to ensure that risk assessment, remediation and predicted fate and transport are appropriately presented and interpreted. © 2003 Elsevier Science B.V. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0642369922&partnerID=40&md5=2659ddd4ede2e32b410ad44b6e395fc7","Objective: This study investigated the pre-(T1) and post-treatment (T2) relationship between anterior (canine to canine inclusive) inter-arch tooth size ratio and various dental and skeletal variables. Design: Retrospective longitudinal clinical study. Setting: Swedish Health Board Clinic 2000. Subjects: Random selection of T1 and T2 orthodontic records of 137 Swedish patients (56 male and 81 female). The sample included non-extraction (77), and four premolar extraction (60) cases across a range of dental and skeletal malrelationships. Main outcome measures: Dental cast and lateral cephalogram measurements were recorded. Exploratory modelling investigated whether a significant relationship existed between the anterior inter-arch tooth size ratio and these measurements. Results: Data was normally distributed with no statistically significant differences between males and females (P = 0.88) and extraction and non-extraction (P = 0.52) treatment modalities with respect to the anterior ratio. T1 bivariate regression analysis failed to show a relationship (p &lt; 0.05) between variables. T2 bivariate analysis showed a statistically significant relation between three variables and anterior tooth size ratio. Multiple regression analysis led to a final model where maxillary inter-canine width (P = 0.002) and upper arch crowding (0.001) were statistically significantly related to the anterior inter-arch ratio. The coefficient of determination was however uniformly low (R 2 &lt;0.2) for all variables. Conclusion: The anterior inter-arch tooth size ratio was not associated with any common pre- or post-treatment variables in the population studied, therefore measurement of an anterior tooth size ratio pre-treatment was not clinically beneficial for determining anterior dental relations posttreatment."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0041562479&partnerID=40&md5=79f162e9152c5265d57ab7ea2eed3fc5","Parallel coordinate plots have been shown to be useful in exploratory data analysis, especially when they are implemented in interactive software. They are also very useful in exploratory modelling analysis, that is in the evaluation and comparison of many models simultaneously, where they can be applied in at least three quite different ways. The advantages of this innovative graphical tool will be illustrated using the software CASSATT and a data set previously analysed using Bayesian Model Averaging. © 2003 Elsevier B.V. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037225336&partnerID=40&md5=87e689594fffe0e290f8dddf988502e9","The multidimensional approach to the study of anxiety (Martens, Vealey, & Burton, 1990a) considers subcomponents of anxiety, specifically cognitive anxiety, somatic anxiety, and self-confidence. Much of the research based on this theory has utilized the Competitive State Anxiety Inventory (CSAI-2) (Martens, Burton, Vealey, Bump, & Smith, 1990b). Findings have been inconsistent, with some research suggesting that the three subcomponents have separate relationships with performance and other studies failing to find any relationship between the anxiety subcomponents and performance. This meta-analysis examined the effect of state anxiety as measured by the CSAI-2 (i.e., cognitive anxiety, somatic anxiety, and self-confidence) on athletic performance. Studies were coded for characteristics that could potentially moderate the effects of anxiety on performance (i.e., features of design, subjects, sport). Interdependency between the three subscales was examined using multivariate meta-analytic techniques (Becker & Schram, 1994). Relationships among cognitive anxiety, somatic anxiety, self-confidence, and performance appeared weak. Exploratory modeling showed that self-confidence displayed the strongest and most consistent relationship with performance."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-1442334636&partnerID=40&md5=5debce38670e4ee23bc1d5eff749fb7c","Robust decision making under uncertainty is considered to be of fundamental importance in numerous disciplines and application areas. In dynamic chemical processes in particular there are parameters which are usually uncertain, but may have a large impact on equipment decisions, plant operability, and economic analysis. Thus the consideration of the stochastic property of the uncertainties in the optimization approach is necessary for robust process design and operation. As a part of it, efficient chance constrained programming has become an important field of research in process systems engineering. A new approach is presented and applied for stochastic optimization problems of batch distillation with a detailed dynamic process model."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0142074226&partnerID=40&md5=18b4d09496ae5aada17060c26779909f","In astrophysical (inverse) regression problems it is an important task to decide whether a given parametric model describes the observational data sufficiently well or whether non-parametric modelling becomes necessary. However, in contrast to common practice this cannot be decided solely by comparing the quality of fit owing to possible overfitting by the non-parametric method. Therefore, in this paper we present a resampling algorithm that allows one to decide whether deviations between a parametric and a non-parametric model are systematic or caused by noise. The algorithm is based on a statistical comparison of the corresponding residuals, under the assumption of the parametric model and under violation of this assumption. This yields a graphical tool for a robust decision making of parametric versus non-parametric modelling. Moreover, our approach can be used for the selection of the most appropriate model among several possibilities (model selection). The methods are illustrated by the problem of recovering the luminosity density in the Milky Way (MW) from near-infrared (NIR) surface brightness data of the DIRBE experiment on-board the COBE satellite. Among the parametric models investigated one with a four-armed spiral structure performs best. In this model the Sagittarius-Carina arm and its counter-arm are significantly weaker than the other pair of arms. Furthermore, we find statistical evidence for an improvement over a range of parametric models with different spiral structure morphologies using a non-parametric model by Bissantz & Gerhard."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944745417&partnerID=40&md5=880847b533bb7ea2403985a2ad804eea","Risk assessment provides the basis for risk management decisions. Traditional risk assessment, based on the precise specification of loss probability distributions, is prone to failure due to the uncertainties involved. We suggest here the incorporation of exploratory modeling and the development of multiple plausible scenarios as a solution to modeling risk under extreme uncertainty. This solution is consistent with fuzzy and interval-based approaches to the assessment of model uncertainties. © 2003 IEEE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037226273&partnerID=40&md5=d6e101f830baedfa12c3b2dd78154f5a","Canada's fish habitat management is guided by the principle of ""no net loss of the productive capacity of fish habitat"" (NNL). Many development proposals are assessed using habitat information alone, rather than fish data. Because fish-habitat linkages are often obscured by uncertainty, uncertainty must be factored into NNL assessments. Using a quantitative framework for assessing NNL and lake habitats as a context, the implications of uncertainty for decision making are examined. The overall behaviour of a net change equation given uncertainty is explored using Monte Carlo simulation. Case studies from Great Lakes development projects are examined using interval analysis. The results indicate that uncertainty, even when large, can be incorporated into assessments. This has important implications for the habitat management based on NNL. First, schemas to specify relative levels of uncertainty using simple habitat classifications can support robust decision making. Second, attaining NNL requires greater emphasis on minimizing habitat loss and creating new areas to compensate for losses elsewhere and less on detailing small incremental changes in modified habitats where the fish response is difficult to demonstrate. Third, the moderate to high levels of uncertainty in fish-habitat linkages require that created compensation is at least twice the losses to reasonably ensure NNL."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036433572&partnerID=40&md5=00bec13e843d7e449c10df48d85309c0","There has been significant recent interest in Agent Based Modeling in many social sciences including economics, sociology, anthropology, political science, and game theory. This article describes three problems that need to be addressed in order for such models to become effective tools for formulating new social theory and informing policy debates and suggests approaches to meeting them. These issues are computational epistemology, research methodology, and software technology. These innovations augment Agent Based Modeling to create an effective new tool base to help better understand complex social systems."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036836553&partnerID=40&md5=199432b84c96ad2cb992c48e816cefa4","Complex nonlinear dynamic systems are ubiquitous in the landscapes and phenomena studied by earth sciences in general and by geomorphology in particular. Concepts of chaos, fractals and self-organization, originating from research in nonlinear dynamics, have proven to be powerful approaches to understanding and modeling the evolution and characteristics of a wide variety of landscapes and bedforms. This paper presents a brief survey of the fundamental ideas and terminology underlying these types of investigations, covering such concepts as strange attractors, fractal dimensions and self-organized criticality. Their application in many areas of geomorphological research is subsequently reviewed, in river network modeling and in surface analysis amongst others, followed by more detailed descriptions of the use of chaos theory, fractals and self-organization in coastal geomorphology in particular. These include self-organized behavior of beach morphology, the fractal nature of ocean surface gravity waves, the self-organization of beach cusps and simulation models of ripples and dune patterns. This paper further presents a substantial extension of existing dune landscape simulation models by incorporating vegetation in the algorithm, enabling more realistic investigations into the self-organization of coastal dune systems. Interactions between vegetation and the sand transport process in the model-such as the modification of erosion and deposition rules and the growth response of vegetation to burial and erosion-introduce additional nonlinear feedback mechanisms that affect the course of self-organization of the simulated landscape. Exploratory modeling efforts show tantalizing results of how vegetation dynamics have a decisive impact on the emerging morphology and-conversely-how the developing landscape affects vegetation patterns. Extended interpretation of the modeling results in terms of attractors is hampered, however, by want of suitable state variables for characterizing vegetated landscapes, with respect to both morphology and vegetation patterns. © 2002 Elsevier Science B.V. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037056084&partnerID=40&md5=d67edc2e50e4c74d018048a61a65b1ed","Considering the high required capital outlay and moderate risk of a flexible manufacturing system (FMS) investment, economic justification techniques are insufficient by themselves since they cannot cope with the benefits such as flexibility and enhanced quality offered by advanced manufacturing technologies. A robust decision-making procedure for evaluating FMS requires the consideration of both economic and strategic investment measures. A distance-based fuzzy multicriteria decision-making (MCDM) framework based on the concepts of ideal and anti-ideal solutions is presented for the selection of an FMS from a set of mutually exclusive alternatives. The proposed method provides the means for integrating the economic figure of merit with the strategic performance variables. The multicriteria decision approach presented here enables us to incorporate data in the forms of linguistic variables, triangular fuzzy numbers and crisp numbers into the evaluation process of FMS alternatives. Linguistic variables are also used to indicate the criteria's importance weights assigned by the decision-makers. A comprehensive example illustrates the application of the multicriteria decision analysis."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0242550408&partnerID=40&md5=28dec1a900125ca2d481756da5e6958f","Much work in AI deals with the selection of proper actions in a given (known or unknown) environment. However, the way to select a proper action when facing other agents is quite unclear. Most work in AI adopts classical game-theoretic equilibrium analysis to predict agent behavior in such settings. This approach however does not provide us with any guarantee for the agent. In this paper we introduce competitive safety analysis. This approach bridges the gap between the desired normative AI approach, where a strategy should be selected in order to guarantee a desired payoff, and equilibrium analysis. We show that a safety level strategy is able to guarantee the value obtained in a Nash equilibrium, in several classical computer science settings. Then, we discuss the concept of competitive safety strategies, and illustrate its use in a decentralized load balancing setting, typical to network problems. In particular, we show that when we have many agents, it is possible to guarantee an expected payoff which is a factor of 8/9 of the payoff obtained in a Nash equilibrium. Our discussion of competitive safety analysis for decentralized load balancing is further developed to deal with many communication links and arbitrary speeds. Finally, we discuss the extension of the above concepts to Bayesian games, and illustrate their use in a basic auctions setup."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-5444254846&partnerID=40&md5=d77f5b2086604961dda7ba436d82308a","The paper presents a robust version of a simple two-assets Merton (1969, Review of Economics and Statistics 51, 247-57) model where the optimal choices and the implied shadow market prices of risk for a representative robust decision maker (RDM) can be easily described. With the exception of the log-utility case, precautionary behaviour is induced in the optimal consumption-investment rules through a substitution of investment in risky assets with both current consumption and riskless saving. For the log-utility case, precautionary behaviour arises only through a substitution between risky and riskless assets. On the financial side, the decomposition of the market price of risk in a standard consumption based component and a further price for model uncertainty risk (which is positively related to the robustness parameter) is independent of the underlying risk aversion parameter. © 2002 Elsevier Science B.V. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036493899&partnerID=40&md5=250e9fe2dc69b84567024711912f3265","A prototype autonomous/adaptive interplanetary navigation system employing neural networks and genetic algorithms is introduced. This system consists of a near real-time autonomous monitoring component and an offline adaptive modeling component. Unexpected dynamic or measurement events trigger 1) alerts to navigators and 2) recommendations for modifying the appropriate model parameters to improve filtering. The autonomous component analyzes tracking measurement residuals to first detect and then characterize the unexpected event. Once an environment change has been detected and characterized, the adaptive modeling component then modifies the necessary model parameters to bring the tracking filter back into optimal operation. The autonomous monitor employs a hierarchical mixture-of-experts model where the experts are extended Kalman filters organized into banks regulated by two levels of single-layer neural networks. The autonomous monitor is the focus of this presentation and demonstrates the ability to detect successfully the occurrence and to differentiate between the characteristics of unexpected discrete velocity changes and continuous dynamic changes. These environment changes are represented by an unmodeled impulsive maneuver and by solar radiation pressure surface mismodeling, respectively. The robust decision-making capability of this approach is further demonstrated by successfully characterizing three successive environment changes. All experiments were performed on recorded Mars Pathfinder two-way Doppler data from the period of 4 February 1997 to 17 April 1997."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-2942528909&partnerID=40&md5=e7f91116546916accc3bce2fc316d667","In a paper published in JASSS, Chris Goldspink discusses the methodological implications of complex systems approaches to the modeling of social systems. Like others before him Goldspink advocates the use of bottom-up computer simulations (BUCSs) for examining social phenomena. It is argued therein that computer simulation offers a partial solution to the methodological crisis apparently observed in the social sciences. Though I agree with many of Goldspinkís remarks I personally feel that BUCS has been oversold as a tool for modeling and managing organizational complexity at the expense of other equally legitimate (from a complex systems stance) approaches. I have no doubt that BUCS offer a new and exciting lens on organizational complexity, but we must explicitly recognize that this nonlinear approach suffers from some of the same limitations as its linear predecessors. The aim of this short note is to discuss some of the limitations in more detail and suggest that complexity thinking offers a simulation paradigm that is broader than the new reductionism of BUCS. This alternative interpretation of complexity thinking forces us to reconsider the relationship between our models and ""reality"" as well as the role of simulation in decision making."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035703184&partnerID=40&md5=d4231efbe1d23b0e3238d32f14ea93b1","Deregulation, with concurrent pressure on electricity utilities, has fundamentally changed the once-""closed"" radioactive waste management system controlled by the so-called ""nuclear establishment."" Advocacy coalitions may change - who knows in which direction - but policy learning may also take place. This article presents a framework to evaluate the management options for a specified concept of ""sustainability."" When weighing the different objectives in view of the long-lasting potential danger of radiotoxic substances, the overall goal of a sound waste management system is to demonstrate safety. The first-priority objective of a disposal system, therefore, is its stability so that it can comply with the protection goal, that is, the long-term protection of humans and the environment from ionizing radiation. The complementary objective is flexibility, defined here as intervention potential. Because trade-offs within the ""sustainability triangle"" of ecology, economy, and society are unavoidable, the concept of ""integral robustness"" - both technical and societal - is introduced into radioactive waste management. A system is robust if it is not sensitive to significant parameter changes. In the present case, it has to have a conservative, passively stable design with built-in control and intervention mechanisms. With regard to technical implementation, a concept called ""monitored long-term geological disposal"" is presented. Such an ""extended"" final disposal concept emphasizes technical robustness, recognizes evaluation demands (for a potential break-off of a project), and enhances process-based transparency. This open approach admittedly sets high challenges with regard to technicalities as well as the institutional setting and the management process. It requires ""mutual learning"" by and from all stakeholders to achieve a truly sustainable radioactive waste management system."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035540726&partnerID=40&md5=0d8a5ea1e9855efe97d912e8f127d772","This article explores the notion that the evaluation process for services may include some variables not typically associated with the evaluation of physical goods. Evidence exists that enhanced perceptions of control may contribute to the pleasantness of service experiences. Although three forms of perceived control - behavioral, decisional, and cognitive - have been utilized as experimental manipulations of individuals' control, established measures exist for the first two forms only. Through two studies, this research develops a measure for perceived cognitive control and reports exploratory modeling with the construct within the disconfirmation paradigm and extensive tests of the measure's validation. The two-dimensional scale evidences unidimensionality, strong internal consistency, and convergent and discriminant validity. Assessment of relationships between the perceived cognitive control scale and numerous, related constructs supports criterion-related validity and nomological validity. © 2001 John Wiley & Sons, Inc."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035294735&partnerID=40&md5=915ab5542642d04c337ac9b7ebf4b97d","This paper develops the importance of decision-making in the engineering design. It makes the argument that the decisions made, and the information contained in the decision-making effort, are key to managing the design process. Based on this importance, the concept of robust decision-making is developed and 12 steps that are necessary to make robust decisions are itemized. Activities that support each step will also be listed."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034755701&partnerID=40&md5=e608f4d8f8d725579263c5a6d9c49537","Medical imaging continues to permeate the practice of medicine, but automated yet accurate segmentation and labeling of anatomical structures continues to be a major obstacle to computerized medical image analysis (MIA). Deformable models, with its profound roots in estimation theory, optimization, and physics-based dynamical systems, represent a powerful approach to the general problem of medical image segmentation. This Thesis presents a number of novel contributions to the field of deformable modeling, and includes theory as well as application. In the first part of the Thesis, a modified Active Contour Model (ACM), utilizing adaptive inflation reversal and damping, is applied to segmenting oral lesions in color images. In the second part, the amalgamation of Active Shape Models (ASM) and ACM into a technique, that harnesses the powers of both, is applied to locating the left ventricular boundary in echocardiographic images. The third part of the Thesis discusses the development of two methodological extensions for spatio-temporal image analysis: Optical flow-based contour deformations, applied to contrast agent tracking in echocardiographic image sequences, and deformable spatio-temporal shape models for extending 2D ASM to 2D+time. The fourth part describes the use of a new Hierarchical Regional Principal Component Analysis, and presents two methods for interactive and learned, localized and multiscale, controlled shape deformation: medial-based shape profiles and physics-based shape deformations. In the final part of the Thesis, we develop Deformable Organisms: a robust decision-making framework for MIA that combines bottom-up, data-driven deformable models with top-down, knowledge-driven processes in a layered fashion inspired by Artificial Life modeling concepts. We present different segmentation and labeling examples of various anatomical structures from medical images and conclude that deformable organisms represent a promising new paradigm for MIA."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035210439&partnerID=40&md5=87dede780263a6af4cb65b6d68421452","It has been widely recognized that agility enables manufacturing systems to respond to dynamic and unpredictable changes in today's competitive environment. Develops a quantitative analysis framework and a simulation methodology to explore the value of agility in financial terms. Addresses the issues pertaining to the assessment of how an agile system performs in an environment of unanticipated changes, the comparison between two or more systems with different designs and hence different agility levels and the justification of investments in agility. Proposes an exploratory framework for a structured analysis of the various segments of the manufacturing system in which agility at different levels is built-in through different pathways and links it to a set of aggregate performance measures. Then develops a simulation model that captures dynamic and unanticipated changes in the operating environment and facilitates performance appraisal and investment justification decisions using a quantitative financial metric."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034672918&partnerID=40&md5=c7696908dd8811145bf882cb7b4c7e93","Investment evaluation and selection of industrial robots is a complex multi-criteria decision-making task. This paper considers the possibility of applying to the robot selection problem a known mathematical method based on Dimensional Analysis (DA) theory. It is shown how it is possible to obtain an easy, efficient and robust decision-making support system which overcomes all attribute dimension problems. In particular, the robustness of the DA approach when compared with judgements made by a group of experts concerning different factors and weights, is evaluated. DA low sensitivity is compared with a Weighted-point decision method found in the literature."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894460008&partnerID=40&md5=5773b8264d83427157a00316accf1d7c","An autonomous/adaptive interplanetary navigation architecture employing neural networks and genetic algorithms is being developed that will alert navigators when a shift from optimal to suboptimal filtering occurs and assist in modifying the filter parameters to resume optimal tracking. This architecture consists of a near real-time autonomous monitoring component and an offline adaptive component. The autonomous component analyzes operational filter residuals to detect the transition to suboptimal filtering and to identify the nature of the mismodeling. Once identified, the adaptive component then modifies the necessary model parameters to bring the filter back into optimal operation. The autonomous identification of mismodeling employs a hierarchical mixture-of-experts model where the experts are extended Kalman filters. The filters in the hierarchy are organized into banks and regulated by two levels of single layer neural networks called gating networks. The architecture of the overall navigation approach is introduced and the operation of the autonomous monitoring component is demonstrated. Two experiments will show the autonomous navigation component can successfully identify discrete model changes such as impulsive maneuvers and continuous model changes such as solar radiation mismodeling. A third experiment will demonstrate the robust decision making capability of the hierarchical mixture-of-experts by successfully identifying three successive model changes. All experiments are performed on Mars Pathfinder two way Doppler data for the period from February 4, 1997 to April 17, 1997. © 2000 by The Center for Space Research. Published by the American Institute of Aeronautics and Astronautics, Inc."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033714923&partnerID=40&md5=b2544c937695d38a6b74a70097a89af5","Considering the high required capital outlay and moderate risk of a flexible manufacturing system (FMS) investment, economic justification techniques are insufficient by themselves since they cannot cope with the benefits such as flexibility and enhanced quality offered by advanced manufacturing technologies. A robust decision making procedure for selection of flexible manufacturing systems requires the consideration of both economic and strategic investment measures. In this paper, a fuzzy multi-criteria decision making (MCDM) framework based on the concepts of ideal and negative-ideal solutions is presented for the selection of an FMS from a set of mutually exclusive alternatives. The proposed method provides the means for incorporating the economic figure of merit as well as the strategic performance variables. Initially, the selection criteria and their importance weights are determined. Linguistic variables are used to indicate the importance weight of each criterion. Then, the decision matrix containing the criteria values for the FMS alternatives is normalized to obtain unit-free elements. Afterwards, the weighted normalized decision matrix is obtained by taking the importance weight of each criterion into consideration. The ideal solution and the negative-ideal solution are determined by ranking the weighted normalized values for each criterion. Next, the distance between each FMS alternative, and the ideal and negative-ideal solutions are computed. Finally, the ranking order of the FMS alternatives is obtained based on their relative proximity to the ideal solution."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951991702&partnerID=40&md5=40f0df93b69596ff80c7d925bdda2f52","Considering the high required capital outlay and moderate risk of a flexible manufacturing system (FMS) investment, economic justification techniques are insufficient by themselves since they cannot cope with the benefits such as flexibility and enhanced quality offered by advanced manufacturing technologies. A robust decision making procedure for selection of flexible manufacturing systems requires the consideration of both economic and strategic investment measures. In this paper, a fuzzy multi-criteria decision making (MCDM) framework based on the concepts of ideal and negative-ideal solutions is presented for the selection of an FMS from a set of mutually exclusive alternatives. The proposed method provides the means for incorporating the economic figure of merit as well as the strategic performance variables. Initially, the selection criteria and their importance weights are determined. Linguistic variables are used to indicate the importance weight of each criterion. Then, the decision matrix containing the criteria values for the FMS alternatives is normalized to obtain unit-free elements. Afterwards, the weighted normalized decision matrix is obtained by taking the importance weight of each criterion into consideration. The ideal solution and the negative-ideal solution are determined by ranking the weighted normalized values for each criterion. Next, the distance between each FMS alternative, and the ideal and negative-ideal solutions are computed. Finally, the ranking order of the FMS alternatives is obtained based on their relative proximity to the ideal solution. © 2000 IEEE."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032708602&partnerID=40&md5=d9508d22c2a33fd2c37c0cb0cddcb266","The historical application of mathematics in the natural sciences and in radiotherapy is compared. The various forms or mathematical models and their limitations are discussed. The Linear Quadratic (LQ) model can be modified to include (i) radiobiological parameter changes that occur during fractionated radiotherapy, (ii) situations such as focal forms of radiotherapy, (iii) normal tissue responses, and (iv) to allow for the process of optimization. The inclusion of a variable cell loss factor in the LQ model repopulation term produces a more flexible clonogenic doubling time, which can simulate the phenomenon of 'accelerated repopulation'. Differential calculus can be applied to the LQ model after elimination of the fraction number integers. The optimum dose per fraction (maximum cell kill relative to a given normal tissue fractionation sensitivity) is then estimated from the clonogen doubling times and the radiosensitivity parameters (or α/β ratios). Economic treatment optimization is described. Tumour volume studies during or following teletherapy are used to optimize brachytherapy. The radiation responses of both individual tumours and tumour populations (by random sampling 'MonteCarlo' techniques from statistical ranges of radiobiological and physical parameters) can be estimated. Computerized preclinical trials can be used to guide choice of dose fractionation scheduling in clinical trials. The potential impact of gene and other biological therapies on the results of radical radiotherapy are testable. New and experimentally testable hypotheses are generated from limited clinical data by exploratory modelling exercises."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038933159&partnerID=40&md5=0542d7c2b6f667915e398e5ef01cb261","An observational study of the nova-like cataclysmic binary V347 Pup (LB 1800) is presented. An analysis of optical and UV spectroscopy is made with the aim of defining the physical properties of the binary system and of the accretion disk. The study of the line profile behavior and the determination of the primary radial velocity are pursued using a variety of methods. We also present the detection of secondary spectral signatures that were used to derive the object's radial velocity curve. A tentative companion spectral classification and spectroscopic parallax are also given. A Doppler tomography study of Balmer and He II lines lead us to an estimate of the average surface brightness distribution of these lines in the accretion disk. Exploratory modeling of the accretion disk in V347 Pup and comparison with UV observations is carried on using the system parameters constrained by the radial velocity study."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038705517&partnerID=40&md5=961b8757030ee0fc2782ce5d8c969491","Simplifying the structure of core arrays from N-way PCA or Tucker3 models is desirable to allow for easy interpretation of the factor estimates. In the present paper, first a general algorithm for maximizing a differentiable goal function depending on a set of orthogonal matrices is formulated and then specified to the problem of estimating orthonormal transformation matrices for rotating core arrays to simpler structure. The generality of the chosen approach allows to cope with all possible transformation criteria by just changing one command in the implementation. In particular, the classical body- and slice-wise diagonalization of core arrays as well as the recently proposed maximization of the variance of squared entries are covered. The stability of the algorithm is addressed by a simulation study using 120 three-way core arrays of dimension (4,4,4). Each core array instantiates a class of 50 equivalent cores by random orthonormal transformations. Theoretically, each core within a given class has the same optimum with respect to the chosen criterion, and the ability of the algorithm to provide that result has been investigated. The algorithm proves to work with a high degree of stability and consistency in optimizing the three discussed goal functions. In addition, theoretical convergence results of the algorithm are provided. In particular, monotonic convergence of functional values and convergence of iterates towards a stationary solution are proven. To illustrate the effect of maximizing the variance-of-squares and the functionality of the algorithm, the proposed method is applied to a three-way data array from fluorometric analysis of fractions obtained from low-pressure chromatographic separation of a preliminary sugar product, thick juice. A significant gain in simplicity is achieved, and in particular optimizing variance-of-squares provides a simple core structure for the data under investigation. The proposed algorithms for maximizing variance-of-squares, body diagonality and slice-wise diagonality have been implemented in MATLAB and are available by contact to the authors. © 1999 Elsevier Science B.V."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032606215&partnerID=40&md5=48c5bfde07681ce19d2c52e78636e1ef","This paper gives a brief review of prospects in the area of cognitive computer graphics and robust decision-making methods for processing optical signals. A combination of these approaches is proposed that provides a qualitatively new effect of stimulation and actualization of the intuitive and empirical knowledge of the users of remote probing systems. A qualimetric estimate is proposed for the information product of the integration of the subjective knowledge of the operator and the objective data of the image processing. Using a specific practical example, the possibility of combining these approaches is demonstrated."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032330584&partnerID=40&md5=07c1ac426d15ef190422fb7c5566a6fb","Current Australian commitments to competency-based teacher reforms have wide acceptance. The key to this has been the affirmative stance adopted by the peak education unions and by 'progressive' educationalists, the nominal guarantors of recent professional advances in teaching. Since Australian approaches to 'competencies' are part of a more general agenda of industrialization of education, what is clearly indicated are radical changes in the way that nominally independent/critical language of education i.s currently being interpreted and applied: this is now being used to support an overall restructuring in which substantive (professional) forms of teacher authority are being uncompromisingly targeted, The transformations identified in this paper as having special strategic significance involve the themes of 'school as community' and 'critical practice'. These have been detached from their traditional (robustly anti-industrial) reference points and redeployed to authorize a more active (because supposedly conflict-free) regulatory roll- by newly empowered managers and to advance the claim that 'the practical', on its own, can now be revalorized to allow real scope for creative and contextualized interventions by teachers. A strictly managerialist/technicist perspective upon schooling is thus represented as occupying the highest moral and cognitive ground. Correspondingly ruled out is the case for any real measure of educational pluralism or leamer-centredness, no matter the suciocultural complexities, thereby licensing the extension to the school site itself of direct (educationally unmediated) political controls over education. As its counterpoint to the new agenda, this paper stresses the need for a robust decision-making role by educators at all levels of policy formation, and for learner-centred interventions which are much more than technical, whatever the processes of revalorization supposedly at work."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890539975&partnerID=40&md5=9668645e5c4a985ef8465b5ee20fc17d","The proceedings contain 32 papers. The topics discussed include: HySpirit - a probabilistic inference engine for hypermedia retrieval in large databases; towards optimal indexing for segment databases; fusion queries over Internet databases; equal time for data on the Internet with WebSemantics; multivariate and multidimensional OLAP; incremental generalization for mining in a data warehousing environment; modeling large scale OLAP scenarios; discovery-driven exploration of OLAP data cubes; a logical approach to multidimensional databases; efficient dynamic programming algorithms for ordering expensive joins and selections; an evaluation of alternative disk scheduling techniques in support of variable bit rate continuous media; and parallel processing of multiple aggregate queries on shared-nothing multiprocessors."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947778701&partnerID=40&md5=db2a018f60676848d99a8a021895390d","It is often important to search high dimensional spaces looking for patterns more complicated than maxima or minima of cost functions. For example, we can provide users with visualizations of two of three dimensional slices through such a space, but need to automate finding interesting pictures from among the very large number of possible slices. This problem, which can be called “subspace pursuit”, has proven very important for recent work in using exploratory modeling to understand complex systems. Conceptually akin to projection pursuit methods popularized in statistics, subspace pursuit is a problem area that has potential importance for a wide range of applications. In this paper, I present an evolutionary approach to making such searches. © Springer-Verlag Berlin Heidelberg 1998."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031634457&partnerID=40&md5=cf325bd336e85615245c75a46ff4e7c4","Most model based methodologies for engineering or policy design depend upon possession of a model that is predictively accurate. However, many systems of practical importance are now recognized as being both highly complex and composed of multiple interacting adaptive agents. For these systems, the construction of a model that accurately predicts the details of system behavior is essentially impossible, and any model for these systems is certain to be `wrong' in at least some details. If modeling and simulation is to be used to reason from what is known about such systems, we must understand how to think usefully with `wrong' models. This paper describes an approach based on exploratory modeling and adaptive strategies that can be used to make decisions about systems that are so complex their detailed behavior cannot be predicted. This approach seeks to find strategies that perform reasonably well over broad ranges of plausible futures - instead of devising strategies that are optimal for some particular best estimate model."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947713406&partnerID=40&md5=ff8723d53c83bc9da0f9a53e9694b8ee","The proceedings contain 79 papers. The special focus in this conference is on Economics, Emergence, Complex Systems, Issues and Innovations in Evolutionary Computation. The topics include: A framework for evolutionary processes in semantic memory; preferential partner selection in evolutionary labor markets; subspace pursuit for exploratory modeling; evolutionary computing in multi-agent environments; evalution of a simple host-parasite genetic algorithm; testing three paradigms for evolving groups of cooperative, simple agents; acquisition of general adaptive features by evolution; hybrid interior-lagrangian penalty based evolutionary optimization; GA-optimal fitness functions; scaling up evolutionary programming algorithms; short notes on the schema theorem and the building block hypothesis in genetic algorithms; a superior evolutionary algorithm for 3-SAT; evolvable hardware control for dynamic reconfigurable and adaptive computing; evolutionary programming strategies with self-adaptation applied to the design of rotorcraft using parallel processing; optimization of thinned phased arrays using evolutionary programming; evolutionary domain covering of an inference system for function approximation; learning to re-engineer semantic networks using cultural algorithms; integration of slicing methods into a cultural algorithm in order to assist in large-scale engineering systems design; genetic search for object identification; fuzzy cultural algorithms with evolutionary programming; culturing evolution strategies to support the exploration of novel environments by an intelligent robotic agent; skeuomorphs and cultural algorithms; sphere operators and their applicability for constrained parameter optimization problems; numeric mutation as an improvement to symbolic regression in genetic programming and variable-dimensional optimization with evolutionary algorithms using fixed-length representations."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031284234&partnerID=40&md5=edbd7925c49102a2f89b40855303cc15","Soil survey information combined with exploratory simulation modeling was used to define indicators for sustainable land management. In one soil series in the Netherlands (the genoform), three different phenoforms were formed as a result of different management practices. Locations were identified using a soil map and interviews with farmers. Organic matter, bulk densities, and porosities were significantly different for the three phenoforms: biodynamic management (Bio), conventional management (Cony), and permanent grassland (Perm). By applying a dynamic simulation model for water movement, crop growth and N dynamics, the three phenoforms were analyzed in terms of sustainability indicators by defining four scenarios based on productivity and N leaching to the groundwater. (i) potential production, (ii) water-limited production, (iii) current management, and (iv) the environmental scenario. The latter was divided into EnvA: never exceeding the N-leaching threshold of 11.3 mg L-1; EnvB: exceedance occurring in one out of 30 yr; and EnvC: exceedance occurring in three out of 30 yr. Biodynamic management obtained the lowest yield under current management, while yields for Penn were highest. EnvA could not be reached for Penn as a result of high mineralization rates. Obtainable yields for scenarios EnvA, EnvB, and EnvC differed substantially, illustrating the importance of selecting 'acceptable' risks in environmental regulation. The presented methodology demonstrates the importance of pedological input in sustainability studies."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-5844224413&partnerID=40&md5=a88863124faac3c536ab63b193e8c9d8","Delineation of areas of contribution to wells tapping a karst carbonate aquifer system can be extremely difficult using conventional approaches designed for isotropic and homogeneous aquifers, because ground-water flow tends to be through solution-enhanced conduits. Nonradial flow along preferential zones can result in inaccurate estimates of flow paths and traveltimes. Because of the large variability in factors affecting contributing areas and an imperfect understanding of how these factors can vary, the estimation of contributing areas is an approximation at best. To better understand the effects of aquifer anisotropy and heterogeneity on areas of contribution, an exploratory modeling approach was used. MODFLOW, a numerical flow model, and MODPATH, a particle tracking program, were used to generate time-related areas of contribution for six hypothetical carbonate aquifer system types. The six types were conceptualized to approximate different types of aquifer anisotropy and heterogeneity. These include: (1) an isotropic and homogeneous single-layer system; (2) an anisotropic in a horizontal plane single-layer system; (3) a discrete vertically fractured single-layer system; (4) a multi-layered system; (5) a doubly porous single-layer system; and (6) a vertically and horizontally interconnected heterogeneous system. The simulated aquifer anisotropy was 5:1 (Kxx/Kyy) determined from TENSOR2D results. The simulated discrete vertical fracture network represents locations inferred from mapped photolineaments. The simulated enhanced flow zones were determined from borehole video and geophysical logs. Areas of contribution were simulated for two prototype regions. The two prototypes were selected to be representative of the hydrologic diversity within the study area and were designated the Central Swamp and Lake Terrace regions. Localized conditions in pumping, production well distribution, and aquifer transmissivity affect the size, shape, and orientation of areas of contribution to public supply wells. The simulated areas of contribution are 60 percent larger in the Central Swamp region where pumpage is more than double and transmissivity is about half that of the Lake Terrace region. Although these factors are important, this study focused on the effects from hydrogeologic factors common to karst carbonate aquifer systems. This study indicates that the distribution and type of aquifer anisotropy and heterogeneity will affect the size, shape, and orientation of areas of contribution in a karst carbonate aquifer system. The size of the 50-year time-related areas of contribution ranged from 8.2 to 39.1 square miles in the Central Swamp region and from 4.0 to 18.3 square miles in the Lake Terrace region. Simulations showed that the size of areas of contribution is primarily affected by simulated withdrawal rates, effective porosity of the carbonate rock, and transmissivity. The shape and orientation of the simulated areas of contribution primarily result from aquifer anisotropy, well distribution, flow along solution-enhanced zones, and short-circuiting of flow through fracture networks. Comparisons also were made between protection zones delineated using analytical models and areas of contribution delineated using numerical models. The size of the 5-year time-related protection zone in the Central Swamp region using an analytical model was almost twice as large as the numerically simulated area of contribution, and more than eight times larger than the numerically simulated area of contribution in the Lake Terrace region. The differences in size are primarily the result of how the flow field is approximated. The analytical method assumes only lateral flow to wells but numerical methods allow particles to move laterally and vertically. Additionally, multiple-well-interference effects resulting from the close proximity of several pumping wells cause individual capture zones to converge or diverge, depending on the difference in pumping rates and orientation among the wells. Such an interpretation is not available from analytical methods. The simulated distributions of aquifer anisotropy and heterogeneity, in this study, were highly conceptualized, but were based on plausible occurrences of anisotropy and heterogeneity inherent in carbonate aquifer systems."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030181961&partnerID=40&md5=b05a3df49a2e7f49ddb4942d0734189f","Exploratory modelling technique has been used to develop a predictive model for the power consumption in sheeting dough as a function of reduction ratio (1 to 1.9), gap (0.055 to 0.44 cm), moisture (60 to 70% and 45 to 55%), salt (1.0 to 2.0% and 3.5 to 4.5%) and fat (2 to 6% and 1 to 2%) for the two types of flour, viz., whole wheat flour (WWF) and resultant atta (R-atta) respectively. Box-Behnken experimental design was used to plan experiments for moisture, salt and fat, requiring only 15 experiments, which were repeated at five discrete combinations of gap and reduction ratios,requiring in all 75 experiments. Polynomials of second order were fitted to the experimental data on power consumption (Watts) for sheeting dough prepared from the two flour types, which gave satisfactory fits with multiple correlations of 0.93 and 0.91, respectively. Each model required nine terms only as against a total of theoretical 15 in each (= n + nC2, n being the number of variables or factors). The selected significant terms differed with the flour type to some extent. The models have been interpreted through a few selected 3-D graphs. Such as approach is simple in nature and yet effective for the data-set at hand, which can be favourably exploited in optimizing doughs for Indian traditional foods to minimize power consumption."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030454197&partnerID=40&md5=f01c8936905ae6e091d6950cc28a26a2","To identify possible critical minimum flows, habitat availability and habitat selection by young Atlantic salmon and brown trout on selected stream reaches were studied in a spatially and temporally heterogeneous Norwegian west coast river. Based on direct underwater observations of 1768 individuals across transects on each reach, unimodal habitat suitability curves for water depth, water velocity and substrate were produced for both species. There was considerable spatial niche overlap between the two species, suggesting niche competition. Rather than narrow optima, they selected habitats within broad ranges of values of the measured microhabitat variables. Atlantic salmon used the broader range. Spatial variation in habitat use was attributed to different habitat availabilities, whereas temporal variations were partly explained by varying water flows and temperature. Variation in hydrophysical conditions, i.e. habitat availability, was quantified by measuring in the same transects at different water flows. A new hydraulic simulation model, the RIMOS (River MOdelling System), was developed to allow for hydraulic simulation of habitat availability at a spatial and temporal scale relevant to the fish. By comparing spatial variations in habitat use and availability, habitat preference curves were calculated and applied in the model. Modelling of the availability of suitable habitats from extreme summer minimum flows to 30 year high flows identified critical minimum flow levels, below which suitable habitat was drastically reduced. These values depended on the habitat variable, fish species, temperature and spatial heterogeneity in the habitat. The negative effects of reduced water flow on fish habitat suitabilities were most pronounced for water velocities and for Atlantic salmon. Thus through hydraulic modelling combined with fish habitat preference data, it was possible to predict a likely change in species composition. Habitat-hydraulic modelling also indicated that resilience towards reduced water flows depended on the in situ stream structure and was much greater in pool-like stream reaches than in riffle/rapids areas. Modelling also allowed for time series analysis of fish habitat suitability to identify temporal bottlenecks in available spatial niches. Furthermore, some exploratory modelling was carried out to identify landscape phenomena in fish habitat use. Habitats were considerably more fragmented in riffle/rapids habitats than in pools, where three-dimensional connectivity tended to be complete over a wider range of flows. It is concluded that fish habitat selection data combined with hydraulic modelling at a scale relevant to the fish can be a useful tool in stream management. However, there is no such a thing as 'the' suitable minimum flow; the effect of reduced flows will vary with stream structure, the hydro-physical variables in question and the fish species. More studies are needed to elucidate possible spatial and in particular temporal variations in fish habitat selection. Care must be taken in aggregating habitat suitability data into single-valued functions."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029214497&partnerID=40&md5=6d17c5bf7527d7b58e88f244b6bf106a","This paper develops a robust and distributed decision-making procedure for mathematically modeling and computationally supporting simultaneous decision-making by members of an engineering team. The procedure (1) treats variations in the design posed by other members of the design team as conceptual noise; (2) incorporates such noise factors into conceptually robust decision-making; (3) provides preference information to other team members on the variables they control; and (4) determines whether to execute the conceptually robust decision or to wait for further design certainty. While Chang et al. (1994) extended Taguchi's approach to such simultaneous decision-making, this paper uses a continuous formulation and discusses the foundations of the procedure in greater detail. The method is demonstrated by a simple distributed design process for a DC motor, and the results are compared with those obtained for the same problem using sequential decision strategies in Krishnan et al. (1991). © 1995 Springer-Verlag London Limited."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028713776&partnerID=40&md5=69838df732ea365d0b98ad5ef614c190","A search is described for the most suitable tools for evaluating rigid pavement performance in Australia and, as a likely consequential need, for refining rigid pavement design techniques through analytical modelling. Firstly, the literature is reviewed on the use of finite element analysis for structurally modelling concrete pavement. Most of the English language literature on computer models is from the USA, where several specialised finite element models have been developed. Secondly, the use of computer modelling is reviewed for evaluating the performance of rigid pavements based on deflection testing. Thirdly, the paper reports on exploratory modelling of rigid (i.e. concrete) pavement using the elastic layer method; the finite difference method; and the finite element method. These analyses were not strictly comparable, but afforded insights to some different options for modelling concrete pavements. It is concluded that three dimensional modelling of concrete pavements is feasible on a personal computer; locally supported software is available; and such modelling will be a useful structural analysis tool for evaluating pavement performance. Such analysis should lead to the refinement of design procedures, and perhaps the 'calibration' of an elastic layer model for the design of rigid pavements."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000993966&partnerID=40&md5=0ce4f7ebafcb7e5fe88ff2bdf3d7b850","Despite recent growth in interest among researchers into structural trait theories of personality, and in particular the 'Big Five', few studies have tested competing models empirically. Confirmatory Factor Analysis (CFA) as part of the LISREL linear structural modelling programme, in conjunction with a degree of exploratory modelling, was used to elicit the underlying factor structure of one particular measure, the Occupational Personality Questionnaire (OPQ) FMX5-Student version. Results indicated that, a modified version of the 'Big Five' model was the most parsimonious fit for this data set. Implications for the use of personality inventories in selection processes, the use of a mixture CFA and exploratory factor analysis in this kind of research, and the nature of the 'Big Five' are discussed. © 1994."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027725739&partnerID=40&md5=6082d633921622a257d7b5ae204db821","Socio-economic systems may be influenced by climatic change in ways ranging from minor or very local to drastic and nation-wide. To allow governments and policy-makers to play an active role in managing these socio-economic systems effectively, they should be provided with tools that will permit them to explore impacts in their full contexts. As part of a larger decision environment, a two-level mathematical modelling framework, geared to study the effects of climatic change on the level of the individual island or mainland state is proposed. The long-range mechanisms of change are modelled in a classic, non-equilibrium spatial interaction model. This model then feeds regional growth coefficients into a low-level cellular model that deals with the short-range location and interaction mechanisms. The prototype presented is a first, mostly coneptual, step towards a system for use in real-world applications. -from Authors"
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027458166&partnerID=40&md5=60d9d96cc0c0e34dfd5648f115a00e48","Research on soil biota under natural conditions in the field is complicated because large fluctuations in populations of certain species may occur as a result or natural or man-induced fluctuations in soil physical and chemical conditions. As a result, many experiments have therefore been made in the laboratory under controlled conditions. Though quite valuable, such data may be difficult to extrapolate to field conditions. To better relate biotic data to physical and chemical conditions in the field, expensive long-term monitoring programs can be initiated to collect both types of data. Physical, and chemical processes in soils can also be characterized by simulation techniques. Simulation requires one-time measurement of some basic and characteristic soil parameters, such as hydraulic conductivity and moisture retention, which are used in a model that calculates water contents in the soil at any given depth and time as a function of precipitation, evaporation and water table levels. Simulation modelling has been applied successfully to calculate water regimes in the soil, for periods of many years. The technique would appear to be particularly appropriate for use in a dynamic physical characterization of different types of soil structure, formed by various soil biota. Results are presented of a study in a sandy loam soil in the Netherlands with biogenic structures in grassland and a more compact physicogenic structure in arable land. Measured hydraulic conductivity and moisture retention data, and their variability were used in a simulation model to calculate important land qualities for a thirty year.period. Land qualities were expressed in stochastic terms by focusing on the probability that certain water and associated air contents and trafficabilities would occur at any given data. Results also reflected the effects of variability of input data. Exploratory modelling was used in this study to estimate effects of an increase of the hydraulic conductivity (which might result from an increase of biological activity), on water and air contents in the soil during the year. © 1993."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027553134&partnerID=40&md5=e05297c33d5d3efff5037739d2fcff76","Expert systems are one of the few areas of artificial intelligence which have successfully made the transition from research and development to practical application. The key to fielding a successful expert system is finding the right problem to solve. AI costs, including all the development and testing, are so high that the problems must be very important to justify the effort. This paper develops a systematic way of trying to predict the future. It provides robust decision-making criteria, which can be used to predict the success or failure of proposed expert systems. The methods focus on eliminating obviously unsuitable problems and performing risk assessments and cost evaluations of the program. These assessments include evaluation of need, problem complexity, value, user experience, and the processing speed required. If an application proves feasible, the information generated during the decision phase can be then used to speed the development process."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025211570&partnerID=40&md5=dfb254613c10479b4385227c0b7d3c22","The authors first summarize the strengths and weaknesses of knowledge acquisition (KA) techniques within the context of exploratory object-oriented models (OOM). These techniques typically capture what is regarded as 'known' or 'true' or 'important,' providing a necessary but insufficient foundation of information for exploratory modeling. The authors then describe creativity tools developed to enrich the knowledge bases associated with exploratory models. Appropriate definitions of creativity are described. The authors argue that the fundamental philosophy of creativity techniques is of value in overcoming KA weaknesses. Three specific creativity tools are described that were jointly developed by the Artificial Intelligence Applications Lab and the Center for Research on Creativity and Innovation at the University of Colorado, Colorado Springs. The utility of each tool for enriching the OOM is described."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995205946&partnerID=40&md5=256b528d04a9ecc4d10c90cc4bf4a392","Using data collected from a major supermarket chain, a regression mode! is estimated to describe buyers' judgments of the profitability of new products. Results indicate how different variables influence these judgments. Implications of new product introductions for private firm strategies and systemwide performance are discussed. Copyright © 1988 Wiley Periodicals, Inc., A Wiley Company"
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023761556&partnerID=40&md5=b86682906091941af92ba45778fd5788","This paper considers the selection of appropriate functional forms for discrete choice models of housing market behaviour. It is argued that uncritical adoption of linear or log-linear forms may be inappropriate, and that Box-Tukey transformations are unnecessarily cumbersome and mechanistic devices in the search for more general functional forms. An alternative graphical assessment procedure based on partial residuals is first presented and is then applied to an empirical case study of tenure choice in London, England. Results are compared with a Box-Tukey analysis, and suggest that the graphical procedure constitutes an important exploratory modelling tool. Graphical functional form evaluation is thus seen to add important insights to analysis of the policy-dominated United Kingdom housing market. © 1988, Sage Publications. All rights reserved."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001283516&partnerID=40&md5=a05207ead899809a3e1ef5ed9827a8c6","Molecular dynamics simulation employing 1000 atoms has been used to examine rapid chemical reactions in fluid sulfur at high temperature. A combination of two-atom and three-atom potentials has been used to represent interactions, with specific forms chosen to improve our earlier exploratory modeling for sulfur. The rate of loss of S 8 rings from media initially composed entirely of these molecules has been measured at 870, 1050, and 1310°C. Spontaneous formation of large linear polymers has been observed. Initial stages of the reaction sequences exhibit a peculiar dominance of species containing even numbers of atoms, which appears to stem from bond alternations produced by the model along diradical chains. The artificial system with only three-atom potentials operative is proposed as a suitable ""reference substance"" for implementing a perturbation theory of liquid sulfur at equilibrium, and molecular dynamics has been used to obtain the corresponding pair correlation functions both before and after quenching to potential energy minima. © 1987 American Chemical Society."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0342501427&partnerID=40&md5=f3fbf0193560ccb53707eda29018a463","Exploratory modelling revealed associations of individual perceptions, social factors, and physical components of air pollution with depressive symptomatology. Residents of the Los Angeles Metropolitan Area who have experienced a recent, undesirable life event and who perceive poor air quality in their neighborhood have greater symptoms of depression. These effects control for socioeconomic status and prior psychological status. In addition we show that perceived air quality is a function of both toxic components of ambient air as well as individual psychosocial experiences. © 1984 Human Sciences Press."
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0016819450&partnerID=40&md5=aac3a6b8810cba98fd31c2c318e7ef45","A conversational program is described which is being used for exploratory modelling of digestive tract electrical activity. The structure of the mathematical model is based on the Hodgkin-Huxley equations for nerve axons and the Noble equations for Purkinje fibres of the heart. The parameters of the models are changed conversationally to investigate the effects on frequency, amplitude and waveshape. In this way it is intended to extend the models to include nervous control. The program is also being used to correlate the behaviour of H-H type equations with the simpler dynamics of Van der Pol's equation which is the basis of an oscillator-array model for the stomach and small intestines. © 1975."
